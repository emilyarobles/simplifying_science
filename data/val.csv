question,pmid,input_text,target_text,Adaptation_Version,Question_Type
2,15316838,"Urinary incontinence is the inability to willingly control bladder voiding.  Stress urinary incontinence (SUI) is the most frequently occurring type of incontinence in women.  No widely accepted or approved drug therapy is yet available for the treatment of stress urinary incontinence.  Numerous studies have implicated the neurotransmitters, serotonin and norepinephrine in the central neural control of the lower urinary tract function.  The pudendal somatic motor nucleus of the spinal cord is densely innervated by 5HT and NE terminals.  Pharmacological studies confirm central modulation of the lower urinary tract activity by 5HT and NE receptor agonists and antagonists.  Duloxetine is a combined serotonin/norepinephrine reuptake inhibitor currently under clinical investigation for the treatment of women with stress urinary incontinence.  Duloxetine exerts balanced in vivo reuptake inhibition of 5HT and NE and exhibits no appreciable binding affinity for receptors of neurotransmitters.  The action of duloxetine in the treatment of stress urinary incontinence is associated with reuptake inhibition of serotonin and norepinephrine at the presynaptic neuron in Onuf's nucleus of the sacral spinal cord.  In cats, whose bladder had initially been irritated with acetic acid, a dose-dependent improvement of the bladder capacity (5-fold) and periurethral EMG activity (8-fold) of the striated sphincter muscles was found.  In a double blind, randomized, placebo-controlled, clinical trial in women with stress urinary incontinence, there was a significant reduction in urinary incontinence episodes under duloxetine treatment.  In summary, the pharmacological effect of duloxetine to increase the activity of the striated urethral sphincter together with clinical results indicate that duloxetine has an interesting therapeutic potential in patients with stress urinary incontinence.","Urinary incontinence is the loss of bladder control. Bladder control loss from stress is the most common type of urinary incontinence in women. No approved drug therapy is available. Chemical messengers, such as serotonin and norepinephrine, in the brain and spinal cord may control function of the bladder area. A portion of the spinal cord receives chemical signals from serotonin and norepinephrine. Drug studies show activity of serotonin and norepinephrine influences bladder area activity. Duloxetine blocks removal of serotonin/norepinephrine and is studied for treating women with bladder control loss from stress.  In cats with impaired bladders, the drug improved bladder capacity and control proportionally with dosage. In women with bladder control loss from stress, duloxetine reduced frequency of bladder control loss. In short, duloxetine increases bladder control and may help patients with bladder control loss from stress.",2,B
2,15316838,"Urinary incontinence is the inability to willingly control bladder voiding.  Stress urinary incontinence (SUI) is the most frequently occurring type of incontinence in women.  No widely accepted or approved drug therapy is yet available for the treatment of stress urinary incontinence.  Numerous studies have implicated the neurotransmitters, serotonin and norepinephrine in the central neural control of the lower urinary tract function.  The pudendal somatic motor nucleus of the spinal cord is densely innervated by 5HT and NE terminals.  Pharmacological studies confirm central modulation of the lower urinary tract activity by 5HT and NE receptor agonists and antagonists.  Duloxetine is a combined serotonin/norepinephrine reuptake inhibitor currently under clinical investigation for the treatment of women with stress urinary incontinence.  Duloxetine exerts balanced in vivo reuptake inhibition of 5HT and NE and exhibits no appreciable binding affinity for receptors of neurotransmitters.  The action of duloxetine in the treatment of stress urinary incontinence is associated with reuptake inhibition of serotonin and norepinephrine at the presynaptic neuron in Onuf's nucleus of the sacral spinal cord.  In cats, whose bladder had initially been irritated with acetic acid, a dose-dependent improvement of the bladder capacity (5-fold) and periurethral EMG activity (8-fold) of the striated sphincter muscles was found.  In a double blind, randomized, placebo-controlled, clinical trial in women with stress urinary incontinence, there was a significant reduction in urinary incontinence episodes under duloxetine treatment.  In summary, the pharmacological effect of duloxetine to increase the activity of the striated urethral sphincter together with clinical results indicate that duloxetine has an interesting therapeutic potential in patients with stress urinary incontinence.","Urinary incontinence is the inability to control your bladder. Stress urinary incontinence (SUI) is the most common type of bladder control issue in women. There are no drugs that are available to treat stress urinary incontinence. Many studies have looked at two chemical messengers, serotonin and norepinephrine, for controlling bladder function. The chemical messengers serotonin (5HT) and norepinephrine (NE) go to one particular part of the spinal cord. Drug studies find that drugs which help or block the chemical messengers 5HT and NE affect bladder control. Duloxetine is a drug that increases the amount of 5HT and NE available for use and is being looked at to see if it can treat women with bladder control problems. Duloxetine increases the amount of 5HT and NE available for use while not blocking the ability of the nerves to use the chemical messengers. Duloxetine works for the treatment of bladder problems by increasing the amount of the chemical messengers 5HT and NE for use in one special area of the spinal cord Researchers tested the drug in cats with irritated bladders and found that it improved bladder capacity and muscle control. Duloxetine reduced bladder control problems for women in clincal trials. To sum up, duloxetine may be a treatment option for bladder control problems because of its effect on the bladder and the results from clinical studies.",2,B
2,30249798,"In addition to treating depression, antidepressant drugs are also a first-line treatment for neuropathic pain, which is pain secondary to lesion or pathology of the nervous system. Despite the widespread use of these drugs, the mechanism underlying their therapeutic action in this pain context remains partly elusive. The present study combined data collected in male and female mice from a model of neuropathic pain and data from the clinical setting to understand how antidepressant drugs act. We show two distinct mechanisms by which the selective inhibitor of serotonin and noradrenaline reuptake duloxetine and the tricyclic antidepressant amitriptyline relieve neuropathic allodynia. One of these mechanisms is acute, central, and requires descending noradrenergic inhibitory controls and α2A adrenoceptors, as well as the mu and delta opioid receptors. The second mechanism is delayed, peripheral, and requires noradrenaline from peripheral sympathetic endings and β2 adrenoceptors, as well as the delta opioid receptors. We then conducted a transcriptomic analysis in dorsal root ganglia, which suggested that the peripheral component of duloxetine action involves the inhibition of neuroimmune mechanisms accompanying nerve injury, including the downregulation of the TNF-α-NF-κB signaling pathway. Accordingly, immunotherapies against either TNF-α or Toll-like receptor 2 (TLR2) provided allodynia relief. We also compared duloxetine plasma levels in the animal model and in patients and we observed that patients' drug concentrations were compatible with those measured in animals under chronic treatment involving the peripheral mechanism. Our study highlights a peripheral neuroimmune component of antidepressant drugs that is relevant to their delayed therapeutic action against neuropathic pain. SIGNIFICANCE STATEMENT In addition to treating depression, antidepressant drugs are also a first-line treatment for neuropathic pain, which is pain secondary to lesion or pathology of the nervous system. However, the mechanism by which antidepressant drugs can relieve neuropathic pain remained in part elusive. Indeed, preclinical studies led to contradictions concerning the anatomical and molecular substrates of this action. In the present work, we overcame these apparent contradictions by highlighting the existence of two independent mechanisms. One is rapid and centrally mediated by descending controls from the brain to the spinal cord and the other is delayed, peripheral, and relies on the anti-neuroimmune action of chronic antidepressant treatment.","Antidepressant drugs, aside from treating depression, are also an important treatment for pain that comes from sensory nerve disorders. However, we still don't know why antidepressants work for this type of pain. This study examined mice with this type of pain to learn more. We specifically examine the antidepressants duloxetine and amitriptyline. Duloxetine prevents the neurotransmitters serotonin and norepinephrine from being re-absorbed by cells, increasing the amount available. For relieving allodynia (where pain is felt instead of other sensations like touch), we found there are two different ways antidepressants can work. One way is short-term and involves the central nervous system. The other way is long-term and involves the neurotransmitter noradrenalin coming from the sympathetic nervous system (which controls the ""fight or flight"" response) and opioid receptors (which opioid drugs act on). We then studied the gene activation in clusters of nerve cells in the spinal cord. Results suggested that duloxetine acts on the peripheral nervous system by inhibiting the immune response following nerve injury. We also compared the amount of duloxetine in the blood of the mice versus people. We found that the level of the drug in people was similar to the level in the mice being treated long-term. This study shows that the immune system of the peripheral nervous system is important to how antidepressants provide long-term relief of pain that does not come from injury. Antidepressant drugs, aside from treating depression, are also an important treatment for pain that comes from sensory nerve disorders. However, we still don't know why antidepressants work for this type of pain. In fact, studies done before clinical trials seemed to create contradictions in how they acted. This study suggests that these apparent contradictions are actually because there are two different ways the drugs can work. One way is quick and involves the connection of the brain to the spinal cord. The other way is delayed and involves suppression of nerve-related immune response when taking antidepressants long-term.",2,B
2,30249798,"In addition to treating depression, antidepressant drugs are also a first-line treatment for neuropathic pain, which is pain secondary to lesion or pathology of the nervous system. Despite the widespread use of these drugs, the mechanism underlying their therapeutic action in this pain context remains partly elusive. The present study combined data collected in male and female mice from a model of neuropathic pain and data from the clinical setting to understand how antidepressant drugs act. We show two distinct mechanisms by which the selective inhibitor of serotonin and noradrenaline reuptake duloxetine and the tricyclic antidepressant amitriptyline relieve neuropathic allodynia. One of these mechanisms is acute, central, and requires descending noradrenergic inhibitory controls and α2A adrenoceptors, as well as the mu and delta opioid receptors. The second mechanism is delayed, peripheral, and requires noradrenaline from peripheral sympathetic endings and β2 adrenoceptors, as well as the delta opioid receptors. We then conducted a transcriptomic analysis in dorsal root ganglia, which suggested that the peripheral component of duloxetine action involves the inhibition of neuroimmune mechanisms accompanying nerve injury, including the downregulation of the TNF-α-NF-κB signaling pathway. Accordingly, immunotherapies against either TNF-α or Toll-like receptor 2 (TLR2) provided allodynia relief. We also compared duloxetine plasma levels in the animal model and in patients and we observed that patients' drug concentrations were compatible with those measured in animals under chronic treatment involving the peripheral mechanism. Our study highlights a peripheral neuroimmune component of antidepressant drugs that is relevant to their delayed therapeutic action against neuropathic pain. SIGNIFICANCE STATEMENT In addition to treating depression, antidepressant drugs are also a first-line treatment for neuropathic pain, which is pain secondary to lesion or pathology of the nervous system. However, the mechanism by which antidepressant drugs can relieve neuropathic pain remained in part elusive. Indeed, preclinical studies led to contradictions concerning the anatomical and molecular substrates of this action. In the present work, we overcame these apparent contradictions by highlighting the existence of two independent mechanisms. One is rapid and centrally mediated by descending controls from the brain to the spinal cord and the other is delayed, peripheral, and relies on the anti-neuroimmune action of chronic antidepressant treatment.","In addition to treating depression, antidepressants are used to treat chronic pain. Doctors do not yet understand how exactly these drugs treat chronic pain. This study combines the results of a study done on mice and a study done on humans to try to understand how the antidepressants work on pain. The study shows two different ways that the drugs duloxetine and amitriptyline relieve nerve pain. One way that duloxetine and amitriptyline work is by affecting the nerves quickly and strongly. The other way duloxetine and amitriptyline work is by affecting the nerves in a delayed way using different chemical messengers. The study suggests that duloxetine works by affecting the immune system around the nerve injury. This means that certain treatments that affect the immune system also relieved pain caused by touch. Researchers also found that duloxetine works similiarly in both humans and animals used for testing. This study shows a delayed way that antidepressant drugs use to treat chronic pain. In addition to treating depression, antidepressant drugs can also be used to treat chronic pain. However, researchers still don't understand how antidepressant drugs work to treat chronic pain. Previous animal studies did not give an answer to how antidepressants treat chronic pain. This study shows that there are two different ways antidepressants treat chronic pain. One way the antidepressants work is by affecting the brain and spinal cord quickly. The second way the antidepressants work is slower and affects the inmmune system around the nerves.",2,B
2,30838456,"This chapter covers antidepressants that fall into the class of serotonin (5-HT) and norepinephrine (NE) reuptake inhibitors. That is, they bind to the 5-HT and NE transporters with varying levels of potency and binding affinity ratios. Duloxetine is a more potent 5-HT and NE reuptake inhibitor with a more balanced profile of binding at about 10:1 for 5HT and NE transporter binding. It is also a moderate inhibitor of CYP2D6, so that modest dose reductions and careful monitoring will be needed when prescribing duloxetine in combination with drugs that are preferentially metabolized by CYP2D6. The most common side effects identified in clinical trials are nausea, dry mouth, dizziness, constipation, insomnia, asthenia, and hypertension, consistent with its mechanisms of action. Clinical trials to date have demonstrated rates of response and remission in patients with major depression that are comparable to other marketed antidepressants reviewed in this book. In addition to approval for MDD, duloxetine is approved for diabetic peripheral neuropathic pain, fibromyalgia, and musculoskeletal pain. All medications in the class can cause serotonin syndrome when combined with MAOIs.","This work covers antidepressants that block removal of the chemical messengers, serotonin (5-HT) and norepinephrine (NE). These antidepressants bind to 5-HT and NE transporters with varying effect. Duloxetine, an antidepressant, is a stronger, more balanced drug for blocking the removal of 5-HT and NE. Duloxetine suppresses the drug-metabolizing molecule, CYP2D6. Thus, careful dosage changes and monitoring are needed when used with other drugs digested by CYP2D6. The most common side effects are nausea, dry mouth, dizziness, constipation, insomnia, physical weakness, and high blood pressure. The drug works with similar success to other antidepressants. Besides depression, duloxetine also treats nerve damage from diabetes and full-body muscle pain. Similar medications that are used with another class of antidepressant medication can lead to high levels of serotonin.",2,B
2,30838456,"This chapter covers antidepressants that fall into the class of serotonin (5-HT) and norepinephrine (NE) reuptake inhibitors. That is, they bind to the 5-HT and NE transporters with varying levels of potency and binding affinity ratios. Duloxetine is a more potent 5-HT and NE reuptake inhibitor with a more balanced profile of binding at about 10:1 for 5HT and NE transporter binding. It is also a moderate inhibitor of CYP2D6, so that modest dose reductions and careful monitoring will be needed when prescribing duloxetine in combination with drugs that are preferentially metabolized by CYP2D6. The most common side effects identified in clinical trials are nausea, dry mouth, dizziness, constipation, insomnia, asthenia, and hypertension, consistent with its mechanisms of action. Clinical trials to date have demonstrated rates of response and remission in patients with major depression that are comparable to other marketed antidepressants reviewed in this book. In addition to approval for MDD, duloxetine is approved for diabetic peripheral neuropathic pain, fibromyalgia, and musculoskeletal pain. All medications in the class can cause serotonin syndrome when combined with MAOIs.","This chapter talks about the antidepressant drugs that are called serotonin (5-HT) and norepinephrine (NE) reuptake inhibitors. Duloxetine is a strong version of the 5-HT and NE reuptake inhibitor type. The way that duloxetine works means that doctors will have to watch you closely and may have to adjust your dose if you are taking certain types of other drugs. Duloxetine's side effects include upset stomach, dry mouth, dizzy spells, constipation, high blood pressure, and loss of sleep and energy. Studies show that duloxetine treats depression just as well as other drugs. In addition to treating depression, duloxetine may be given to you to treat pain such as pain caused by diabetes and other body pains. Drugs in the 5-HT and NE reuptake inhibitor class can make you very sick if taken with drugs that are in the MAOI class of antidepressants.",2,B
2,34397423,"Background: In antidepressant trials for pediatric patients with depression or anxiety disorders, the risk of suicidal events and other severe psychiatric adverse events such as aggression and agitation is increased with antidepressants relative to placebo. Objective: To examine whether largely mentally healthy adolescents treated for a non-psychiatric condition are also at increased risk of suicidality and other severe psychiatric disorders. Methods: This is a re-analysis of a placebo-controlled duloxetine trial for juvenile fibromyalgia based on the main journal article and additional data published in the online supplementary material and on ClinicalTrials.gov. Both serious adverse events related to psychiatric disorders and adverse events leading to treatment discontinuation were defined as severe treatment-emergent psychiatric adverse events. Results: We found that a significant portion of adolescents had treatment-emergent suicidal ideation and behaviour as well as other severe psychiatric adverse events with duloxetine, but no such events were recorded on placebo. The incidence of severe treatment-emergent psychiatric adverse events was statistically significantly higher with duloxetine as compared to placebo. Conclusions: Antidepressants may put adolescents at risk of suicidality and other severe psychiatric disorders even when the treatment indication is not depression or anxiety.","Studies of antidepressants in minors with depression or anxiety have shown a higher risk of suicide attempts and other severe mental issues, such as aggression and agitation, when taking antidepressants. Our objective was to see if healthy adolescents being treated for a non-mental condition also have a higher risk of suicide attempts and severe mental issues. We took a second look at the results from a published trial of duloxetine (Cymbalta) for treating fibromyalgia (a chronic pain disorder). We used results both from the published article and from supplemental results online and from ClinicalTrials.gov. We found that a significant portion of the adolescents on duloxetine had suicidal tendencies and other serious mental side effects. None of these problems were seen in the participants that weren't taking duloxetine. We concluded that antidepressants may put adolescents at risk of suicidal tendencies and other serious mental problems, even if they are not being treated for depression or anxiety.",2,B
2,34397423,"Background: In antidepressant trials for pediatric patients with depression or anxiety disorders, the risk of suicidal events and other severe psychiatric adverse events such as aggression and agitation is increased with antidepressants relative to placebo. Objective: To examine whether largely mentally healthy adolescents treated for a non-psychiatric condition are also at increased risk of suicidality and other severe psychiatric disorders. Methods: This is a re-analysis of a placebo-controlled duloxetine trial for juvenile fibromyalgia based on the main journal article and additional data published in the online supplementary material and on ClinicalTrials.gov. Both serious adverse events related to psychiatric disorders and adverse events leading to treatment discontinuation were defined as severe treatment-emergent psychiatric adverse events. Results: We found that a significant portion of adolescents had treatment-emergent suicidal ideation and behaviour as well as other severe psychiatric adverse events with duloxetine, but no such events were recorded on placebo. The incidence of severe treatment-emergent psychiatric adverse events was statistically significantly higher with duloxetine as compared to placebo. Conclusions: Antidepressants may put adolescents at risk of suicidality and other severe psychiatric disorders even when the treatment indication is not depression or anxiety.",Children with depression or anxiety have a higher risk of suicide and aggressive and anxious behavior when taking antidepressants. This study wants to find out if healthy teens have a higher risk of suicide and other bad thoughts when taking antidepressants for reasons other than depression. This study looks at data from another study where teens with chronic pain known as fibromyalgia were treated with an antidepressant called duloxetine. The study defined severe side effects as suicidal and other bad thoughts and anything that stopped the patient from taking the drug. The study found that a large number of teens in the study had suicidal thoughts and other side effects when treated with duloxetine. Teens that were not taking the drug did not have these side effects. Many more instances of severe side effects occurred for teens taking duloxetine as opposed to those not taking the drug. Teens may be at risk of suicide and other bad thoughts when taking the drug duloxetine even if the drug is not being used to treat depression or anxiety.,2,B
2,33347023,"Purpose: Several studies have previously reported the association between dry eye and depression along with the treatment of depression.  The aim of this study was to investigate the effects of different antidepressant drugs on tear parameters in patients with major depressive disorder. Methods: We recruited 132 patients who were using different antidepressants and 58 healthy controls.  Venlafaxine, duloxetine, escitalopram, and sertraline were used by 34, 28, 36, and 34 patients, respectively.  The participants filled out and completed the Beck Depression Scale.  We recorded Schirmer test, tear breakup time (TBUT) and corneal staining values of the participants.  The Ocular Surface Disease Index was completed by the participants.  In addition, we evaluated the tear meniscus parameters by using anterior segment optical coherence tomography.  Results: All conventional dry eye tests and tear meniscus parameters were significantly lesser in the depression group than in the control group (Schirmer test, 11.41 ± 6.73 mm and 22.53 ± 4.98 mm; TBUT, 5.29 ± 2.92 seconds and 13.38 ± 1.72; Corneal staining, tear meniscus area, 0.026 ± 0.012 mm2 and 0.11 ± 0.025 mm2; tear meniscus depth, 182.75 ± 78.79 μm and 257.48 ± 90.1 μm; tear meniscus height, 290.3 ± 133.63 μm and 459.78 ± 180.26 μm, in patients and controls, respectively).  The tear parameters of the duloxetine group were lowest among the drug groups and Schirmer test, and TBUT of the venlafaxine group was statistically significantly different from the duloxetine group (P = 0.028 and P = 0.017, respectively).  Ocular Surface Disease Index score of the depression group was significantly higher than the control group (31.12 ± 21.15 and 17.43 ± 11.75 in depression and control group, respectively.).  Conclusions: We found that the usage of selective serotonin reuptake inhibitors and serotonin noradrenaline reuptake inhibitors affects the ocular surface by a mechanism other than the anticholinergic system.  Besides serotonin blockage, the noradrenaline blockade of serotonin noradrenaline reuptake inhibitors may increase the dry eye findings on the ocular surface.","Dry eye, depression, and treatent of depression may be linked. This study investigates how different antidepressant drugs affects eye tears in patients with depression. We included 132 patients using different antidepressants and 58 healthy people. Different antidepressants were used by groups of size 34, 28, 36, and 34. Participants filled out a questionnaire to measure depression. We recorded different eye and tear measurements. Participants completed an eye measurement test. We also measured other parameters of the tear. The depression group had smaller dry eye and tear volume than the healthy group. The tear measurements of the antidepressant duloxetine group was lowest among the drug groups and notably different than the antidepressant venlafaxine group. Dry eye of the depression group was worse than that of the healthy group. We found that using antidepressants that block removal of the chemical messengers serotonin and noradrenaline affect the eye surface. Besides blocking removal of serotonin, blockage of noradrenaline by these antidepressants may increase dry eye.",2,B
2,33347023,"Purpose: Several studies have previously reported the association between dry eye and depression along with the treatment of depression.  The aim of this study was to investigate the effects of different antidepressant drugs on tear parameters in patients with major depressive disorder. Methods: We recruited 132 patients who were using different antidepressants and 58 healthy controls.  Venlafaxine, duloxetine, escitalopram, and sertraline were used by 34, 28, 36, and 34 patients, respectively.  The participants filled out and completed the Beck Depression Scale.  We recorded Schirmer test, tear breakup time (TBUT) and corneal staining values of the participants.  The Ocular Surface Disease Index was completed by the participants.  In addition, we evaluated the tear meniscus parameters by using anterior segment optical coherence tomography.  Results: All conventional dry eye tests and tear meniscus parameters were significantly lesser in the depression group than in the control group (Schirmer test, 11.41 ± 6.73 mm and 22.53 ± 4.98 mm; TBUT, 5.29 ± 2.92 seconds and 13.38 ± 1.72; Corneal staining, tear meniscus area, 0.026 ± 0.012 mm2 and 0.11 ± 0.025 mm2; tear meniscus depth, 182.75 ± 78.79 μm and 257.48 ± 90.1 μm; tear meniscus height, 290.3 ± 133.63 μm and 459.78 ± 180.26 μm, in patients and controls, respectively).  The tear parameters of the duloxetine group were lowest among the drug groups and Schirmer test, and TBUT of the venlafaxine group was statistically significantly different from the duloxetine group (P = 0.028 and P = 0.017, respectively).  Ocular Surface Disease Index score of the depression group was significantly higher than the control group (31.12 ± 21.15 and 17.43 ± 11.75 in depression and control group, respectively.).  Conclusions: We found that the usage of selective serotonin reuptake inhibitors and serotonin noradrenaline reuptake inhibitors affects the ocular surface by a mechanism other than the anticholinergic system.  Besides serotonin blockage, the noradrenaline blockade of serotonin noradrenaline reuptake inhibitors may increase the dry eye findings on the ocular surface.","Studies show that there is a link between depression treatment and dry eyes. This study looks at the tears of depressed people when given different types of drugs that treat depression. This study includes 132 people taking antidepressant drugs and 58 people not taking antidepressant drugs for comparison. Patients were taking the antidepressant drugs called venlafaxine, duloxetine, escitalopram, and sertraline. Patients filled our a form to measure their level of depression. The researchers looked at the eyes and tears of the patients. Patients filled out a form to measure the dryness of their eyes. Those people taking antidepressant drugs had drier eyes than those that were not taking antidepressant drugs. The people taking duloxetine had the driest eyes. The people taking antidepressant drugs scored much higher on a test to measure dry eyes. This study finds that certain classes of antidepressant drugs affects the eye ducts but does not because the drugs affect the nerves. Using antidepressants from the drug class called serotonin noradrenaline reuptake inhibitors may lead to dry eyes.",2,B
2,31747213,"Duloxetine is a medication used to manage major depressive disorder (MDD), generalized anxiety disorder (GAD), fibromyalgia, diabetic peripheral neuropathy, and chronic musculoskeletal pain. Off-label uses for duloxetine include chemotherapy-induced peripheral neuropathy and stress urinary incontinence. It is in the Serotonin and norepinephrine reuptake inhibitors (SNRIs) class of medications. This activity describes the indications, mechanism of action, and contraindications for duloxetine as a valuable agent in treating multiple health conditions. This activity will highlight the mechanism of action, adverse event profile, and other key factors (e.g., off-label uses, dosing, pharmacodynamics, pharmacokinetics, monitoring, relevant drug-drug interactions) pertinent for members of the interprofessional team in the treatment of patients with major depressive disorder (MDD), generalized anxiety disorder (GAD), fibromyalgia, diabetic peripheral neuropathy, chronic musculoskeletal pain, and related conditions.","Duloxetine is a medication used to treat depression, anxiety, fibromyalgia, diabete nerve damage, and chronic pains in the body. Duloxetine may also be used to treat nerve pain caused by chemotherapy and loss of bladder control caused by physical activity. Duloxetine is in the Serotonin and norepinephrine reuptake inhibitors (SNRIs) class of medications. Duloxetine is valuable because it treats many different health problems. This article describes the uses of the antidepressant, how it works, and medications that should not be used while taking it. This article will highlight things your doctors will need to know when prescribing duloxetine for treatment.",2,B
2,21366359,"Duloxetine, a potent reuptake inhibitor of serotonin (5-HT) and norepinephrine, is effective for the treatment of major depressive disorder, diabetic neuropathic pain, stress urinary incontinence, generalized anxiety disorder and fibromyalgia. Duloxetine achieves a maximum plasma concentration (C(max)) of approximately 47 ng/mL (40 mg twice-daily dosing) to 110 ng/mL (80 mg twice-daily dosing) approximately 6 hours after dosing. The elimination half-life of duloxetine is approximately 10-12 hours and the volume of distribution is approximately 1640 L. The goal of this paper is to provide a review of the literature on intrinsic and extrinsic factors that may impact the pharmacokinetics of duloxetine with a focus on concomitant medications and their clinical implications. Patient demographic characteristics found to influence the pharmacokinetics of duloxetine include sex, smoking status, age, ethnicity, cytochrome P450 (CYP) 2D6 genotype, hepatic function and renal function. Of these, only impaired hepatic function or severely impaired renal function warrant specific warnings or dose recommendations. Pharmacokinetic results from drug interaction studies show that activated charcoal decreases duloxetine exposure, and that CYP1A2 inhibition increases duloxetine exposure to a clinically significant degree. Specifically, following oral administration in the presence of fluvoxamine, the area under the plasma concentration-time curve and C(max) of duloxetine significantly increased by 460% (90% CI 359, 584) and 141% (90% CI 93, 200), respectively. In addition, smoking is associated with a 30% decrease in duloxetine concentration. The exposure of duloxetine with CYP2D6 inhibitors or in CYP2D6 poor metabolizers is increased to a lesser extent than that observed with CYP1A2 inhibition and does not require a dose adjustment. In addition, duloxetine increases the exposure of drugs that are metabolized by CYP2D6, but not CYP1A2. Pharmacodynamic study results indicate that duloxetine may enhance the effects of benzodiazepines, but not alcohol or warfarin. An increase in gastric pH produced by histamine H(2)-receptor antagonists or antacids did not impact the absorption of duloxetine. While duloxetine is generally well tolerated, it is important to be knowledgeable about the potential for pharmacokinetic interactions between duloxetine and drugs that inhibit CYP1A2 or drugs that are metabolized by CYP2D6 enzymes.","Duloxetine blocks two chemical messengers from working, called serotonin (5-HT) and norepinephrine. Duloxetine is given to treat things such as depression, diabetic nerve pain, leaky bladder, anxiety, and other chronic pain.  The goal of this paper is to look at other studies on duloxetine to see if the drug can be used with other drugs. Things that affect the way that duloxetine works are your sex, age, ethnicity, whether you smoke or not, the condition of your kidney and liver, and your genetics. A doctor will only have warnings against using duloxetine if your liver or kidneys are not in good condition. Studies show that activated charcoal, used to treat certain poisons, makes duloxetine less effective. Taking drugs that affect a certain enzyme with duloxetine could make you sick. Taking the drug fluvoxamine, used to treat depression and OCD, with duloxetine could make you sick. Smoking makes duloxetine less effective. Certain drugs and genetics could affect duloxetine, but not enough to affect how much dulxoetine you would take. There are certain drugs that should not be taken with duloxetine because they can make you sick when taking them normally. Studies show that duloxetine may make tranquilizers like valium and xanax stronger, but does not seem to affect alcohol or the blood thinner warfarin. Antacids and other stomach acid blockers do not affect duloxetine. It is important that your doctors know what drugs you are taking in order to make sure you do not get sick from taking duloxetine.",2,B
2,33542006,"Atraumatic trismus can be one of the presentations of medication-induced acute dystonia, particularly by antipsychotics and less commonly antidepressants. A case of an unusual emergency presentation of atraumatic trismus on initiation of duloxetine is reported. The patient was a 40-year-old woman experiencing sudden difficulty in mouth opening and speaking due to a stiffened jaw after taking 5 days of duloxetine prescribed for her fibromyalgia-related chest pain. Assessment of vital signs is prudent to ensure there is no laryngeal involvement. Other physical examinations and her recent investigations were unremarkable. She was treated for acute dystonia and intravenous procyclidine was given together with oral diazepam. Her symptoms improved immediately and her duloxetine was suggested to be stopped. To our knowledge, this is the first case of isolated trismus induced by duloxetine. Clinicians should be aware of this risk, especially considering the limitation of important physiological functions (such as swallowing, eating, etc) associated with this condition.","A stiff jaw is a possible side effect of taking certain medications, such as antidepressant drugs or drugs called antipsychotics. Here is an example of someone who visited the ER with a severely stiff jaw after taking the drug duloxetine. The patient was a 40-year-old woman who was having trouble with opening her mouth and speaking five days after starting the drug duloxetine. The woman's doctors found no other problems that might cause her jaw stiffness. She was treated for muscle spasms with an IV and pills. The woman improved right away and was told to stop taking duloxetine. This is the first time the authors have heard of jaw stiffness being caused by the drug duloxetine. Doctors should be aware of the risk of jaw stiffness since it could lead to very serious problems.",2,B
2,32874005,"We present a case of hypertensive urgency in a diabetic patient with painful diabetic neuropathy on duloxetine treatment. The patient's blood pressure was high after taking 1-day dose of duloxetine and the patient was diagnosed with hypertensive urgency. The patient was treated with labetalol, leading to reduction in blood pressure. The patient's medication was switched to telmisartan and metoprolol, which leads to resolution of increased blood pressure. This case report is a possible case of hypertensive urgency after the initiation of duloxetine managed with antihypertensives and resolves with the discontinuation of the duloxetine.","This paper is about a patient with pain caused by diabetes who went to the doctor with very serious high blood pressure. The patient's blood pressure was high after taking a 1-day dose of the antidepressant duloxetine (a drug that also treats diabetic nerve pain). The patient was diagnosed with a high blood pressure emergency. The patient was given labetalol, a drug that lowers blood pressure. The patient was switched to the drugs telmisartan and metoprolol, which treat high blood pressure. This paper reports on a case of high blood pressure caused by the drug duloxetine. It was treated with drugs that treat high blood pressure and went back to normal after the patient stopped taking duloxetine.",2,B
2,32546134,"Background: Antidepressants-induced movement disorders are rare and imperfectly known adverse drug reactions. The risk may differ between different antidepressants and antidepressants' classes. The objective of this study was to assess the putative association of each antidepressant and antidepressants' classes with movement disorders. Methods: Using VigiBase®, the WHO Pharmacovigilance database, disproportionality of movement disorders' reporting was assessed among adverse drug reactions related to any antidepressant, from January 1967 to February 2017, through a case/non-case design. The association between nine subtypes of movement disorders (akathisia, bruxism, dystonia, myoclonus, parkinsonism, restless legs syndrome, tardive dyskinesia, tics, tremor) and antidepressants was estimated through the calculation first of crude Reporting Odds Ratio (ROR), then adjusted ROR on four potential confounding factors: age, sex, drugs described as able to induce movement disorders, and drugs used to treat movement disorders. Results: Out of the 14,270,446 reports included in VigiBase®, 1,027,405 (7.2%) contained at least one antidepressant, among whom 29,253 (2.8%) reported movement disorders. The female/male sex ratio was 2.15 and the mean age 50.9 ± 18.0 years. We found a significant increased ROR for antidepressants in general for all subtypes of movement disorders, with the highest association with bruxism (ROR 10.37, 95% CI 9.62-11.17) and the lowest with tics (ROR 1.49, 95% CI 1.38-1.60). When comparing each of the classes of antidepressants with the others, a significant association was observed for all subtypes of movement disorders except restless legs syndrome with serotonin reuptake inhibitors (SRIs) only. Among antidepressants, mirtazapine, vortioxetine, amoxapine, phenelzine, tryptophan and fluvoxamine were associated with the highest level to movement disorders and citalopram, paroxetine, duloxetine and mirtazapine were the most frequently associated with movement disorders. An association was also found with eight other antidepressants. Conclusions: A potential harmful association was found between movement disorders and use of the antidepressants mirtazapine, vortioxetine, amoxapine, phenelzine, tryptophan, fluvoxamine, citalopram, paroxetine, duloxetine, bupropion, clomipramine, escitalopram, fluoxetine, mianserin, sertraline, venlafaxine and vilazodone. Clinicians should beware of these adverse effects and monitor early warning signs carefully. However, this observational study must be interpreted as an exploratory analysis, and these results should be refined by future epidemiological studies.","A rare side effect of taking antidepressant drugs is uncontrollable movement, also called movement disorders. There may be different side effects depending on what antidepressant drugs you take. This study tried to figure out which antidepressant drugs caused which movement disorder side effects. This study looked at reports of antidepressant use from 1967 to 2017 using a computer database. This study used statistics to see if the links between the different antidepressants and movement disorders. There were over 14 million reports in the database. One million of those contained a report of antidepressant use. Almost 30 thousand of the antidepressant reports contained a movement disorder side effect. The patients in the reports ranged from 32-68 years old and contained twice as many women as men. All movement disorders were linked to antidepressant use. Jaw clenching was seen the most often, and involuntary movement called tics were the least seen. The anitdepressant drug class called serotonin reuptake inhibitors (SRIs) were most often linked to movement disorders except for restless leg syndrome. The antidepressants linked to the strongest movement disorders, like Parkinson's or Huntington's disease, were mirtazapine, vortioxetine, amoxapine, phenelzine, tryptophan, and fluvoxamine. The antidepressants with the most movement disorder side effects were citalopram, paroxetine, duloxetine, and mirtazapine. Eight other antidepressants were linked to movement disorder side effects. This study found that movement disorder side effects were linked to the antidepressants mirtazapine, vortioxetine, amoxapine, phenelzine, tryptophan, fluvoxamine, citalopram, paroxetine, duloxetine, bupropion, clomipramine, escitalopram, fluoxetine, mianserin, sertraline, venlafaxine, and vilazodone. Doctors should be aware of side effects and watch patients on antidepresants carefully. This study is introductory and should be furthered explored with more studies.",2,B
7,22432746,"The European Society for Clinical Microbiology and Infectious Diseases established the Sore Throat Guideline Group to write an updated guideline to diagnose and treat patients with acute sore throat. In diagnosis, Centor clinical scoring system or rapid antigen test can be helpful in targeting antibiotic use. The Centor scoring system can help to identify those patients who have higher likelihood of group A streptococcal infection. In patients with high likelihood of streptococcal infections (e.g. 3-4 Centor criteria) physicians can consider the use of rapid antigen test (RAT). If RAT is performed, throat culture is not necessary after a negative RAT for the diagnosis of group A streptococci. To treat sore throat, either ibuprofen or paracetamol are recommended for relief of acute sore throat symptoms. Zinc gluconate is not recommended to be used in sore throat. There is inconsistent evidence of herbal treatments and acupuncture as treatments for sore throat. Antibiotics should not be used in patients with less severe presentation of sore throat, e.g. 0-2 Centor criteria to relieve symptoms. Modest benefits of antibiotics, which have been observed in patients with 3-4 Centor criteria, have to be weighed against side effects, the effect of antibiotics on microbiota, increased antibacterial resistance, medicalisation and costs. The prevention of suppurative complications is not a specific indication for antibiotic therapy in sore throat. If antibiotics are indicated, penicillin V, twice or three times daily for 10 days is recommended. At the present, there is no evidence enough that indicates shorter treatment length.","A European scientific organization made a Sore Throat Guideline Group to write a new guideline to diagnose and treat people with short-term sore throat. A common sore throat scoring survey or a rapid strep test involving a quick throat swab to find bacterial fragments can be useful in deciding which antibiotic to use. The sore throat scoring survey can help identify people who are more likely to have group A strep, caused by group A strep bacteria. Doctors might use a rapid strep test in people who are highly likely to have strep throat based on the sore throat scoring survey. If the rapid strep test shows no strep infection, a throat swab to find, grow, and test bacteria in the throat that make you sick is not needed. Advil or Tylenol can help short-term sore throat symptoms. Zinc gluconate should not be used in sore throat. It is unclear whether herbal treatments and acupuncture can help sore throat. People with less serious sore throat (low throat scoring survey score, for example) should not use antibiotics to help sore throat. Moderate benefits of antibiotics, seen in people with higher throat survey scores, have to be compared to side effects, effects on small organisms that live in or on the human body, the ability of bacteria to defeat the antibiotics designed to kill them, people more likely to seek medical care for future illness and costs. Preventing pus formation is not a reason for using antibiotics to help sore throat. If using antibiotics, taking penicillin V two to three times a day for 10 days is suggested. Currently, taking antibiotics for fewer than 10 days is not recommended.",1,C
7,22432746,"The European Society for Clinical Microbiology and Infectious Diseases established the Sore Throat Guideline Group to write an updated guideline to diagnose and treat patients with acute sore throat. In diagnosis, Centor clinical scoring system or rapid antigen test can be helpful in targeting antibiotic use. The Centor scoring system can help to identify those patients who have higher likelihood of group A streptococcal infection. In patients with high likelihood of streptococcal infections (e.g. 3-4 Centor criteria) physicians can consider the use of rapid antigen test (RAT). If RAT is performed, throat culture is not necessary after a negative RAT for the diagnosis of group A streptococci. To treat sore throat, either ibuprofen or paracetamol are recommended for relief of acute sore throat symptoms. Zinc gluconate is not recommended to be used in sore throat. There is inconsistent evidence of herbal treatments and acupuncture as treatments for sore throat. Antibiotics should not be used in patients with less severe presentation of sore throat, e.g. 0-2 Centor criteria to relieve symptoms. Modest benefits of antibiotics, which have been observed in patients with 3-4 Centor criteria, have to be weighed against side effects, the effect of antibiotics on microbiota, increased antibacterial resistance, medicalisation and costs. The prevention of suppurative complications is not a specific indication for antibiotic therapy in sore throat. If antibiotics are indicated, penicillin V, twice or three times daily for 10 days is recommended. At the present, there is no evidence enough that indicates shorter treatment length.","A European society created a group to update guidelines to identify and treat patients with sore throat. Centor clinical scoring system or rapid antigen, or foreign protein, testing can help target antibiotics (antibacterial medication). The Centor scoring system can help identify those with higher risk of group A streptococcal or strep bacterial infection. In patientis with high risk of streptoccal bacterial infections, physicians may use rapid antigen, or foreign protein, testing (RAT). If rapid antigen testing is used, testing isolated throat cells are not needed for identifying group A strep bacterial infection if no antigens are detected. Either ibuprofen or paracetamol, common pain relievers, can help relieve immediate sore throat symptoms. Zinc gluconate, a dietary supplement, is not recommeded with a sore throat. There is inconsistent evidence that herbal treatments or acupuncture treats sore throat. Patients with less severe sore throats should not use antibiotics to relieve symptoms. Limited benefits of antibiotics, seen in patients with severe sore throat, have to be weighed against antibiotic side effects, its effects on bacteria, medicalisation and costs. Preventing pus is not a sign for antibacterial medication in sore throat. If using antibiotics, penicillin V, two or three times daily for 10 days is recommended. Currently, there is not enough evidence for shorter treatment length.",2,C
7,21675907,"In patients with symptoms and signs suggestive of streptococcal pharyngitis a specific diagnosis should be determined by performing a throat culture or a rapid antigen-detection test with a throat culture if the rapid antigen-detection test is negative, at least in children. Penicillin is the preferred treatment, and a first-generation cephalosporin is an acceptable alternative unless there is a history of immediate hypersensitivity to a beta-lactam antibiotic.","Doctors should give patients who are believed to have strep throat a throat culture (a test using a throat swab to find, grow, and test bacteria in the throat that make you sick) or a rapid strep test (a test using a throat swab to find fragments of bacteria in the throat that make you sick) followed by a throat culture if the rapid strep test finds no strep-related bacteria, at least in children. Penicillin is prescribed most commonly. A first-generation cephalosporin, another kind of antibiotic, is another option if no allergies exist.",1,C
7,27386721,"Group A beta-hemolytic streptococcal (GABHS) infection causes 15% to 30% of sore throats in children and 5% to 15% in adults, and is more common in the late winter and early spring. The strongest independent predictors of GABHS pharyngitis are patient age of five to 15 years, absence of cough, tender anterior cervical adenopathy, tonsillar exudates, and fever. To diagnose GABHS pharyngitis, a rapid antigen detection test should be ordered in patients with a modified Centor or FeverPAIN score of 2 or 3. First-line treatment for GABHS pharyngitis includes a 10-day course of penicillin or amoxicillin. Patients allergic to penicillin can be treated with firstgeneration cephalosporins, clindamycin, or macrolide antibiotics. Nonsteroidal anti-inflammatory drugs are more effective than acetaminophen and placebo for treatment of fever and pain associated with GABHS pharyngitis; medicated throat lozenges used every two hours are also effective. Corticosteroids provide only a small reduction in the duration of symptoms and should not be used routinely.","Group A strep infection, caused by group A strep bacteria, causes 15% to 30% of sore throats in children and 5% to 15% in adults, and is more common in the late winter and early spring. The most common risk factors of group A strep throat are people under 5 to 15 years old, no cough, tender swollen lymph nodes in the front of the neck, white or yellow spots on the tonsils, and fever. To determine if it is Group A strep throat, a rapid strep test, a test using a throat swab to find bacterial fragments in the throat that make you sick, should be used in people with a medium to high score on common sore throat scoring surveys. Taking antibiotics (penicillin or amoxicillin) for 10 days is the most common treatment for group A strep throat. People allergic to penicillin can be treated with other types of antibiotics like first-generation cephalosporins, clindamycin, or macrolide. antibiotics. Nonsteroidal anti-inflammatory drugs (common over-the-counter drugs like ibuprofen or aspirin) are better than Tylenol or nothing for relief of fever and pain caused by group A strep throat. Taking medicated throat lozenges every two hours also helps with fever and pain. Steroids only make the length of symptoms a little shorter and should not be used regularly.",1,C
7,25229278,"Acute pharyngitis/tonsillitis, which is characterized by inflammation of the posterior pharynx and tonsils, is a common disease. Several viruses and bacteria can cause acute pharyngitis; however, Streptococcus pyogenes (also known as Lancefield group A β-hemolytic streptococci) is the only agent that requires an etiologic diagnosis and specific treatment. S. pyogenes is of major clinical importance because it can trigger post-infection systemic complications, acute rheumatic fever, and post-streptococcal glomerulonephritis. Symptom onset in streptococcal infection is usually abrupt and includes intense sore throat, fever, chills, malaise, headache, tender enlarged anterior cervical lymph nodes, and pharyngeal or tonsillar exudate. Cough, coryza, conjunctivitis, and diarrhea are uncommon, and their presence suggests a viral cause. A diagnosis of pharyngitis is supported by the patient's history and by the physical examination. Throat culture is the gold standard for diagnosing streptococcus pharyngitis. However, it has been underused in public health services because of its low availability and because of the 1- to 2-day delay in obtaining results. Rapid antigen detection tests have been used to detect S. pyogenes directly from throat swabs within minutes. Clinical scoring systems have been developed to predict the risk of S. pyogenes infection. The most commonly used scoring system is the modified Centor score. Acute S. pyogenes pharyngitis is often a self-limiting disease. Penicillins are the first-choice treatment. For patients with penicillin allergy, cephalosporins can be an acceptable alternative, although primary hypersensitivity to cephalosporins can occur. Another drug option is the macrolides. Future perspectives to prevent streptococcal pharyngitis and post-infection systemic complications include the development of an anti-Streptococcus pyogenes vaccine.","Sore throat/tonsillitis, or when the back of the throat or tonsils is inflamed, is common. Many viruses and bacteria can cause short-term sore throat. However, group A strep, caused by Group A strep bacteria, is the only cause that must be identified based on signs and symptoms and treated. Group A strep bacteria are important to identify because they can cause post-strep throat complications throughout the body, acute rheumatic fever (a disease that inflames the body's tissues), and post-strep throat kidney disease. Strep throat symptoms usually happen quickly and include severe sore throat, fever, chills, general discomfort, headache, swollen lymph nodes in the front of the neck, and white or yellow spots on the throat or tonsils. Cough, cold symptoms, pink eye, and diarrhea are not common and might be caused by a virus. Learning the person's history and doing a physical exam are used to diagnose strep throat. A throat swab to find, grow, and test bacteria in the throat that make you sick is the best way to diagnose strep throat. However, it has not been used as much as it should because it is not widely available and takes 1 to 2 days to get results. Rapid strep tests have been used to find fragments of bacteria that cause strep throat from swabs within minutes. Scoring systems have been made to predict the risk of strep throat. The modified Centor score is the most common scoring survey. Short-term strep throat often goes away on its own without treatment. Penicillins, a type of antibiotics, are prescribed most commonly. For people allergic to penicillin, cephalosporins, another type of antibiotics, can be prescribed, although people can be allergic to cephalosporins. Another drug option is macrolides, another type of antibiotics. Making an anti-strep throat vaccine could be one way to prevent strep throat and post-strep throat complications throughout the body in the future.",1,C
7,25229278,"Acute pharyngitis/tonsillitis, which is characterized by inflammation of the posterior pharynx and tonsils, is a common disease. Several viruses and bacteria can cause acute pharyngitis; however, Streptococcus pyogenes (also known as Lancefield group A β-hemolytic streptococci) is the only agent that requires an etiologic diagnosis and specific treatment. S. pyogenes is of major clinical importance because it can trigger post-infection systemic complications, acute rheumatic fever, and post-streptococcal glomerulonephritis. Symptom onset in streptococcal infection is usually abrupt and includes intense sore throat, fever, chills, malaise, headache, tender enlarged anterior cervical lymph nodes, and pharyngeal or tonsillar exudate. Cough, coryza, conjunctivitis, and diarrhea are uncommon, and their presence suggests a viral cause. A diagnosis of pharyngitis is supported by the patient's history and by the physical examination. Throat culture is the gold standard for diagnosing streptococcus pharyngitis. However, it has been underused in public health services because of its low availability and because of the 1- to 2-day delay in obtaining results. Rapid antigen detection tests have been used to detect S. pyogenes directly from throat swabs within minutes. Clinical scoring systems have been developed to predict the risk of S. pyogenes infection. The most commonly used scoring system is the modified Centor score. Acute S. pyogenes pharyngitis is often a self-limiting disease. Penicillins are the first-choice treatment. For patients with penicillin allergy, cephalosporins can be an acceptable alternative, although primary hypersensitivity to cephalosporins can occur. Another drug option is the macrolides. Future perspectives to prevent streptococcal pharyngitis and post-infection systemic complications include the development of an anti-Streptococcus pyogenes vaccine.","Immediate pharyngitis/tonsillitis, characterized by inflammation of the pharynx (an airway in the throat) and tonsils, is a common disease. Many viruses and bacteria can cause immediate pharyngitis (throat inflammation). However, only Streptoccocus pyogenes (a specific bacteria) needs identification and specific treatment. S. pyogenes, a specific bacteria, is important since it can trigger post-infection issues, immediate rheumatic fever, and kidney disease. Symptoms start abruptly in strep bacterial infection and include intense sore throat, fever, chills, fatigue, headache, enlarged neck lymph nodes, and pharyngeal or tonsillar fluid leakage. Cough, nose and eye inflammation, and diarrhea are uncommon. Their presence suggests a viral cause. Identifying pharynx, or throat, inflammation is supported by patient history and physical examination. Testing throat cells are the gold standard for identifying strep throat. However, testing isolated throat cells is underused due to its low availability and 1- to 2-day delay for results. Rapid antigen, or foreign protein, detection tests may detect bacterial S. pyogenes from throat swabs in minutes. Medical scoring systems have been created to predict risk of S. pyogenes bacterial infection. The most common scoring system is the modified Centor score. Immediate strep throat is often a self-resolving disease. Penicillins, or antibacterial drugs, are the first-choice treatment. For patients with penicillin allergy, cephalosporins (another antibacterial) can be an alternative. Although, immediate immune responses to cephalosporins can occur. Another antibiotic option is macrolides. Future options to prevent strep throat and associated issues include developing an anti-Streptococcus pyogenes bacterial vaccine.",2,C
7,33178623,"The most common bacterial cause of pharyngitis is infection by Group A β-hemolytic streptococcus (GABHS), commonly known as strep throat. 5-15% of adults and 15-35% of children in the United States with pharyngitis have a GABHS infection. The symptoms of GABHS overlap with non-GABHS and viral causes of acute pharyngitis, complicating the problem of diagnosis. A careful physical examination and patient history is the starting point for diagnosing GABHS. After a physical examination and patient history is completed, five types of diagnostic methods can be used to ascertain the presence of a GABHS infection: clinical scoring systems, rapid antigen detection tests, throat culture, nucleic acid amplification tests, and machine learning and artificial intelligence. Clinical guidelines developed by professional associations can help medical professionals choose among available techniques to diagnose strep throat. However, guidelines for diagnosing GABHS created by the American and European professional associations vary significantly, and there is substantial evidence that most physicians do not follow any published guidelines. Treatment for GABHS using analgesics, antipyretics, and antibiotics seeks to provide symptom relief, shorten the duration of illness, prevent nonsuppurative and suppurative complications, and decrease the risk of contagion, while minimizing the unnecessary use of antibiotics. There is broad agreement that antibiotics with narrow spectrums of activity are appropriate for treating strep throat. But whether and when patients should be treated with antibiotics for GABHS remains a controversial question. There is no clearly superior management strategy for strep throat, as significant controversy exists regarding the best methods to diagnose GABHS and under what conditions antibiotics should be prescribed.","Strep throat caused by bacteria is most commonly caused by group A strep bacteria. 5-15% of adults and 15-35% children in the United States with strep throat have a group A strep bacteria infection. The symptoms of group A strep bacteria are similar to short-term strep throat caused by viruses and other bacteria, which makes strep throat hard to diagnose. Diagnosing strep throat caused by group A strep bacteria begins with a careful physical exam and patient history. Following a physical exam and patient history, there are five ways to diagnose strep throat caused by group A strep bacteria: scoring systems, rapid antigen tests to find strep bacterial fragments, throat swabs to grow strep bacteria, tests for strep genetic material, and computer predictions. Clinical guidelines written by professional groups can help doctors choose which way to diagnose strep throat. However, guidelines for diagnosing group A strep throat created by professional groups in the United States and Europe differ, and many doctors do not follow any guidelines. Treating group A strep throat with painkillers, fever-reducers, and antibiotics aims to relieve symptoms, shorten illness length, prevent later medical problems with pus or without pus, and decrease the spread, while reducing the use of antibiotics when they are not needed. Experts agree that antibiotics that kill fewer bacteria are best to treat strep throat. Experts do not agree whether and when people with group A strep throat should be given antibiotics. There is no best way to treat strep throat, as experts do not agree on the best way to diagnose group A strep throat and when antibiotics should be given.",1,C
7,28385329,"Objective: To compare azithromycin (AZT) and benzathine penicillin (BP) in the treatment of recurrent tonsillitis in children. Methods: The study comprised of 350 children with recurrent streptococcal tonsillitis, 284 of whom completed the study and 162 children received conventional surgical treatment. The rest of the children, 122, were divided randomly into two equal main groups. Group A children received a single intramuscular BP (600,000 IU for children≤27kg and 1,200,000IU for ≥27kg) every two weeks for six months. Group B children received single oral AZT (250mg for children≤25kg and 500mg for ≥25kg) once weekly for six months. Results: Both groups showed marked significant reduction in recurrent tonsillitis that is comparable to results of tonsillectomy. There were no statistical differences between group A and B regarding the recurrence of infections and drug safety after six-month follow-up. Group B showed better compliance. Conclusion: AZT proved to be good alternative to BP in the management of recurrent tonsillitis with results similar to those obtained after tonsillectomy.","Our objective is to compare two antibiotics, azithromycin (AZT) and benzathine penicillin (BP), in treating reoccurring inflamed tonsils in children. 284 of 350 children with reoccurring inflamed tonsils caused by strep bacteria participated in the study. 162 children had surgery to treat reoccurring inflamed tonsils. We divided the rest of the children, 122, into two groups. Group A children got a single BP injection (600,000 international units for children weighing 27 kg or less and 1,200,000 international units for children over 27 kg) once a week for six months. Group B children got a single dose of AZT by mouth (250 mg for children weighing 25 kg or less and 500 mg for children over 25 kg). Once a week for six months. Both groups had results similar to getting surgery to remove the tonsils. Drug safety and the reoccurring of inflamed tonsils were similar in both groups. Group B followed doctor instructions better. We concluded that AZT can treat reoccurring inflamed tonsils similar to BP with results similar to getting surgery to remove the tonsils.",1,C
7,25296661,"Background: Diagnosing group A streptococcus (Strep A) throat infection by clinical examination is difficult, and misdiagnosis may lead to inappropriate antibiotic use. Most patients with sore throat seek symptom relief rather than antibiotics, therefore, therapies that relieve symptoms should be recommended to patients. We report two clinical trials on the efficacy and safety of flurbiprofen 8.75 mg lozenge in patients with and without streptococcal sore throat. Methods: The studies enrolled adults with moderate-to-severe throat symptoms (sore throat pain, difficulty swallowing and swollen throat) and a diagnosis of pharyngitis. The practitioner assessed the likelihood of Strep A infection based on historical and clinical findings. Patients were randomised to flurbiprofen 8.75 mg or placebo lozenges under double-blind conditions and reported the three throat symptoms at baseline and at regular intervals over 24 h. Results: A total of 402 patients received study medication (n = 203 flurbiprofen, n = 199 placebo). Throat culture identified Strep A in 10.0% of patients and group C streptococcus (Strep C) in a further 14.0%. The practitioners' assessments correctly diagnosed Strep A in 11/40 cases (sensitivity 27.5%, and specificity 79.7%). A single flurbiprofen lozenge provided significantly greater relief than placebo for all three throat symptoms, lasting 3-4 h for patients with and without Strep A/C. Multiple doses of flurbiprofen lozenges over 24 h also led to symptom relief, although not statistically significant in the Strep A/C group. There were no serious adverse events. Conclusions: The results highlight the challenge of identifying Strep A based on clinical features. With the growing problem of antibiotic resistance, non-antibiotic treatments should be considered. As demonstrated here, flurbiprofen 8.75 mg lozenges are an effective therapeutic option, providing immediate and long-lasting symptom relief in patients with and without Strep A/C infection.","Diagnosing group A strep throat (Strep A) by a physical exam is difficult, and diagnosing it incorrectly may lead to use of the wrong antibiotic. Doctors should suggest treatments that improve symptoms to people with sore throat because most do not want antibiotics. We looked at two studies on how well flurbiprofen 8.75 mg lozenge works and how safe it is in people with and without strep throat. We studied adults with moderate-to-severe throat symptoms (sore throat pain, difficulty swallowing and swollen throat) and a diagnosis of strep throat. The doctor determined how likely it was that people have Strep A infection based on the history of the patient and a physical exam. We gave people either flurbiprofen 8.75 mg lozenges or sugar lozenges and they reported three throat symptoms (sore throat pain, difficulty swallowing and swollen throat) at the beginning of the study and regularly over 24 h. We gave 203 people flurbiprofen 8.75 mg and 199 people sugar lozenges, for a total of 402 people. A throat swab to find, grow, and test bacteria in the throat found Strep A in 10% of people and group C strep (Strep C) in 14% of people. The doctors correctly diagnosed Strep A in 11 of 40 cases. People had greater symptom relief with one flurbiprofen lozenge than one sugar lozenge for three throat symptoms (sore throat pain, difficulty swallowing and swollen throat), lasting 3-4 h for people with and without Strep A or C. People with Strep A or C may have some symptom relief with more than one flurbiprofen lozenge over 24 h. There were no serious side effects. We conclude that the studies emphasize the difficulty of identifying Strep A based on signs and symptoms. With the growing problem of bacteria able to defeat the antibiotics designed to kill them, treatments that are not antibiotics should be considered. As shown here, flurbiprofen 8.75 mg lozenges work, giving immediate and long-lasting symptom relief in people with and without Strep A or C infection.",1,C
7,25296661,"Background: Diagnosing group A streptococcus (Strep A) throat infection by clinical examination is difficult, and misdiagnosis may lead to inappropriate antibiotic use. Most patients with sore throat seek symptom relief rather than antibiotics, therefore, therapies that relieve symptoms should be recommended to patients. We report two clinical trials on the efficacy and safety of flurbiprofen 8.75 mg lozenge in patients with and without streptococcal sore throat. Methods: The studies enrolled adults with moderate-to-severe throat symptoms (sore throat pain, difficulty swallowing and swollen throat) and a diagnosis of pharyngitis. The practitioner assessed the likelihood of Strep A infection based on historical and clinical findings. Patients were randomised to flurbiprofen 8.75 mg or placebo lozenges under double-blind conditions and reported the three throat symptoms at baseline and at regular intervals over 24 h. Results: A total of 402 patients received study medication (n = 203 flurbiprofen, n = 199 placebo). Throat culture identified Strep A in 10.0% of patients and group C streptococcus (Strep C) in a further 14.0%. The practitioners' assessments correctly diagnosed Strep A in 11/40 cases (sensitivity 27.5%, and specificity 79.7%). A single flurbiprofen lozenge provided significantly greater relief than placebo for all three throat symptoms, lasting 3-4 h for patients with and without Strep A/C. Multiple doses of flurbiprofen lozenges over 24 h also led to symptom relief, although not statistically significant in the Strep A/C group. There were no serious adverse events. Conclusions: The results highlight the challenge of identifying Strep A based on clinical features. With the growing problem of antibiotic resistance, non-antibiotic treatments should be considered. As demonstrated here, flurbiprofen 8.75 mg lozenges are an effective therapeutic option, providing immediate and long-lasting symptom relief in patients with and without Strep A/C infection.","Identifying bacterial group A strep (Strep A) throat infection by examination is difficult. Misidentifying may lead to innappropriate antibacterial antibiotic use. Most with sore throat seek symptom relief rather than antibacterial antibiotics, so therapies that relieve symptoms should be promoted. We show two trials on the success and safety of anti-inflammatory fluriboprofen lozenges in those with and without strep throat. The studies enrolled adults with moderate-to-severe throat symptoms (sore throat pain, difficulty swallowing and swollen throat) and with inflammation of the pharynx (specific throat area). The practitioner measured risk of Strep A bacterial infection by historical and medical findings. Patients were randomised to anti-inflammatory flurbiprofen or inactive treatment. They also reported three throat symptoms at start and regular intervals over 24 hours. 402 patients received treatment (203 with antinflammatory flurbiprofen and 199 with inactive treatment). Testing isolated throat cells identified bacterial Strep A in 10% of patients and group C streptococcus (Strep C) in another 14%. The practitioners correctly identified Strep A in 11/40 cases. A single anti-inflammatory flurbiprofen lozenge gave more relief than the inactive treatment for all three throat symptoms and for 3-4 hours. Multiple doses of flurbiprofen over 24 hours led to mild, negligible symptom relief. There were no serious side effects. It is difficult identifying bacterial Strep A by clinical features. With growing antibiotic resistance, non-antibiotic treatments should be considered. As seen here, anti-inflammatory flurbiprofen lozenges are effective treatment, giving immediate and long-lasting symptom relief.",2,C
7,7864482,"Most patients who seek medical attention for sore throat are concerned about streptococcal tonsillopharyngitis, but fewer than 10% of adults and 30% of children actually have a streptococcal infection. Group A beta-hemolytic streptococci (GAS) are most often responsible for bacterial tonsillopharyngitis, although Neisseria gonorrhea, Arcanobacterium haemolyticum (formerly Corynebacterium haemolyticum), Chlamydia pneumoniae (TWAR agent), and Mycoplasma pneumoniae have also been suggested as possible, infrequent, sporadic pathogens. Viruses or idiopathic causes account for the remainder of sore throat complaints. Reliance on clinical impression to diagnose GAS tonsillopharyngitis is problematic; an overestimation of 80% to 95% by experienced clinicians typically occurs for adult patients. Overtreatment promotes bacterial resistance, disturbs natural microbial ecology, and may produce unnecessary side effects. Existing data suggest that rapid GAS antigen testing as an aid to clinical diagnosis can be very useful. When used appropriately, it is sensitive (79% to 88%) in detecting GAS-infected patients and is specific (90% to 96%) and cost-effective. Penicillin has been the treatment of choice for GAS tonsillopharyngitis since the 1950s; 10 days of treatment are necessary for bacterial eradication. A single IM injection of benzathine penicillin is effective and obviates compliance issues. Until the early 1970s, the bacteriologic failure rate for the treatment of GAS tonsillopharyngitis ranged from 2% to 10% and was attributed to chronic GAS carriers. Since the late 1970s, the penicillin failure rate has frequently exceeded 20% in published reports. Explanations for recurrent GAS tonsillopharyngitis include poor patient compliance; reacquisition from a family member or peer, copathogenic colonization by Staphylococcus aureus, Haemophilus influenzae, Moraxella catarrhalis, anaerobes that inactivate penicillin with beta-lactamase, or all these organisms; suppression of natural immune response by too-early administration of antibiotics; GAS tolerance to penicillin; antibiotic eradication of normal pharyngeal flora that normally act as natural host defenses; and establishment of a true carrier state. When therapy fails, milder symptoms may occur during the relapse. Several antimicrobials have demonstrated superior efficacy compared with penicillin in eradicating GAS and are administered less frequently to enhance patient compliance. In previously untreated GAS throat infections, cephalosporins produce a 5% to 22% higher bacteriologic cure rate; after a penicillin treatment failure, these differences are greater. Amoxicillin/clavulanate and the extended-spectrum macrolides clarithromycin and azithromycin may also produce enhanced bacteriologic eradication in comparison to penicillin.","Most people who go to a doctor for sore throat are worried they have a strep throat and tonsil infection, but fewer than 10% of adults and 30% of children actually have a strep infection. Group A strep bacteria are the most common cause of bacterial strep throat and tonsil infection, but other bacteria known to cause sexually-transmitted gonorrhea or chlamydia, or head, neck, and lung infections occasionally might cause it. Remaining sore throat issues are caused by viruses and unknown causes. Relying on a doctor's exam to diagnose group A strep throat and tonsil infection causes problems. Experienced doctors over-diagnose 80 to 95% of adult cases. Doctors treating someone more than needed leads to the ability of bacteria to defeat the antibiotics designed to kill them, affects small organisms that live in or on the human body, and may cause side effects that are not needed. Studies show combining a rapid strep testing (using a throat swab to detect bacterial fragments) with a doctor exam can be helpful for diagnosis. When used correctly, a rapid strep test has high accuracy for group A strep throat and tonsil infection detection and does not cost a lot. Penicillin has been the preferred treatment for group A strep throat and tonsil infection since the 1950s. Taking penicillin for 10 days is needed to kill all the group A bacteria. One benzathine penicillin shot works and does not require people following doctor instructions. Until the early 1970s, the rate of group A strep bacteria coming back after treatment was low and thought to be caused by people who had long-term Group A bacteria in their bodies. Since the late 1970s, the rate of group A strep bacteria coming back after treatment more than doubled according to studies. Reasons for reoccurring group A strep throat and tonsil infection are people not following doctor instructions; getting the infection again from a family member or peer; infection caused by group A strep bacteria and other bacteria; taking antibiotics too early in the infection; group A strep bacteria defeating antibiotics used to kill them; antibiotics killing small organisms in the throat that protect it; and people who have the bacteria in their bodies but are not sick. When treatment doesn't work, milder symptoms may happen as symptoms return. Many substances that kill small organisms have worked better than penicillin to kill group A strep bacteria and are given less often to increase the rate at which people follow doctor orders. In group A strep and tonsil infection that is not treated, cephalosporins, another type of antibiotic, kill all group A strep bacteria more often than penicillin. After penicillin is taken and does not cure group A strep throat and tonsil infection, this rate is higher. A combination of amoxicillin and clavulanate, other antibiotics, and clarithromycin and azithromycin, another kind of antibiotics called macrolides that kill more types of bacteria, may also kill more bacteria than penicillin.",1,C
7,28730858,"Chronic GAS carrier state is best defined as the prolonged presence of group A β-haemolytic Streptococcus (GAS) in the pharynx without evidence of infection or inflammation. Chronic GAS carriers have a low risk of immune mediated complications. Persistent pharyngeal carriage often raises management issues. In this study, we review the evidence on the management of persistent GAS carriage in children and propose a management algorithm. Areas covered: Chronic GAS pharyngeal carriage is quite common affecting 10-20% of school-aged children. Pathogenesis of carriage has been related to the pharynx microflora and to special properties of GAS, but several aspects are yet to be elucidated. Management greatly depends on whether the individual child belongs to a 'high-risk' group and might benefit from eradication regimens or not, when observation-only and reassurance are enough. Penicillin plus rifampin and clindamycin monotherapy have been recommended for eradication; limited evidence of effectiveness of azithromycin has been reported. Surgical intervention is not indicated. Expert commentary: GAS infection is a common reason for antibiotic use and abuse in children and asymptomatic carriers constitute the major reservoir of GAS in the community. Several aspects are yet to be elucidated and well-designed studies are needed for firm conclusions to be drawn.","Chronic GAS carrier state is defined as the long-term presence of group A strep (GAS) in the throat with no infection or inflammation. Chronic GAS carriers have a low risk of conditions that result from abnormal functioning of the body's immune system. Long-term GAS in the throat often causes treatment issues. In this study, we review the science on treating long-term GAS in children and suggest a way to treat it using computers. Long-term GAS in the throat is found in 10-20% of school-aged children. Whether or not GAS in the throat causes infection depends on small organisms in the throat and special qualities of GAS, but many things are not clear. Treating long-term GAS depends on whether the child is high-risk and might benefit from killing the bacteria or not, when a doctor looking at it and removing fears and concerns about the illness are enough. Combining antibiotics penicillin with rifampin and clindamycin in one drug has been recommended to kill GAS. There is not much proof that the antibiotic azithromycin works. Surgery is not recommended. Experts comment that GAS infection is a common reason for antibiotic use and overuse in children and people who have GAS without symptoms are the most common carriers in the community. Many things are not clear, and good studies are needed to make decisions.",1,C
7,15156437,"We conducted a meta-analysis of 9 randomized controlled trials (involving 2113 patients) comparing cephalosporins with penicillin for treatment of group A beta -hemolytic streptococcal (GABHS) tonsillopharyngitis in adults. The summary odds ratio (OR) for bacteriologic cure rate significantly favored cephalosporins, compared with penicillin (OR,1.83; 95% confidence interval [CI], 1.37-2.44); the bacteriologic failure rate was nearly 2 times higher for penicillin therapy than it was for cephalosporin therapy (P=.00004). The summary OR for clinical cure rate was 2.29 (95% CI, 1.61-3.28), significantly favoring cephalosporins (P<.00001). Sensitivity analyses for bacterial cure significantly favored cephalosporins over penicillin in trials that were double-blinded and of high quality, trials that had a well-defined clinical status, trials that performed GABHS serotyping, trials that eliminated carriers from analysis, and trials that had a test-of-cure culture performed 3-14 days after treatment. This meta-analysis indicates that the likelihood of bacteriologic and clinical failure in the treatment of GABHS tonsillopharyngitis is 2 times higher for oral penicillin than for oral cephalosporins.",We analyzed results from 9 studies (2113 people total) comparing cephalosporins (antibacterial antibiotics) and penicillin (another antibiotic) for treatment of group A strep throat and tonsil infection in adults. Results favored cephalosporins over penicillin. Results favored cephalosporins. Results favored cephalosporins over penicillin. This analysis shows using penicillin to treat group A strep throat and tonsil infection is twice as likely to result in the bacteria and infection coming back as using cephalosporins.,1,C
7,15156437,"We conducted a meta-analysis of 9 randomized controlled trials (involving 2113 patients) comparing cephalosporins with penicillin for treatment of group A beta -hemolytic streptococcal (GABHS) tonsillopharyngitis in adults. The summary odds ratio (OR) for bacteriologic cure rate significantly favored cephalosporins, compared with penicillin (OR,1.83; 95% confidence interval [CI], 1.37-2.44); the bacteriologic failure rate was nearly 2 times higher for penicillin therapy than it was for cephalosporin therapy (P=.00004). The summary OR for clinical cure rate was 2.29 (95% CI, 1.61-3.28), significantly favoring cephalosporins (P<.00001). Sensitivity analyses for bacterial cure significantly favored cephalosporins over penicillin in trials that were double-blinded and of high quality, trials that had a well-defined clinical status, trials that performed GABHS serotyping, trials that eliminated carriers from analysis, and trials that had a test-of-cure culture performed 3-14 days after treatment. This meta-analysis indicates that the likelihood of bacteriologic and clinical failure in the treatment of GABHS tonsillopharyngitis is 2 times higher for oral penicillin than for oral cephalosporins.",We reviewed 9 randomized trials (with 2113 patients) comparing cephalosporins (antibacterial antibiotics) with penicillin (another antibiotic) for treating group A beta -hemolytic streptococcal tonsillopharyngitis (a bacterial throat infection) in adults. Results favored cephalosporins over penicillin. Results favored cephalosporins. Results favored cephalosporins over penicillin. The risk of treatment failure for bacterial strep throat is 2 times higher for oral penicillin antibiotics than for cephalosporins antibiotics.,2,C
13,31509499,"The normal sleep-wake cycle is characterized by diurnal variations in blood pressure, heart rate, and cardiac events. Sleep apnea disrupts the normal sleep-heart interaction, and the pathophysiology varies for obstructive sleep apnea (OSA) and central sleep apnea (CSA). Associations exist between sleep-disordered breathing (which encompasses both OSA and CSA) and heart failure, atrial fibrillation, stroke, coronary artery disease, and cardiovascular mortality. Treatment options include positive airway pressure as well as adaptive servo-ventilation and phrenic nerve stimulation for CSA. Treatment improves blood pressure, quality of life, and sleepiness, the last particularly in those at risk for cardiovascular disease. Results from clinical trials are not definitive in terms of hard cardiovascular outcomes.","The normal sleep-wake cycle (our-24 hour daily sleep pattern) is characterized by fluctuations in the day and variations at night in blood pressure, heart rate, and cardiac events (reduced blood flow that may damage the heart). Sleep apnea is a sleep disorder where breathing repeatedly stops and starts. Sleep apnea disrupts the normal patterns between sleep and how the heart functions, and the physical changes vary for obstructive sleep apnea (OSA), caused by airflow blockage, and central sleep apnea (CSA), when breathing regularly stops while sleeping because the brain doesn't tell the muscles to take in air. There are associations between sleep-disordered breathing, including OSA and CSA, and heart failure, atrial fibrillation (a fluttery and irregular heartbeat that can lead to blood clots) and other heart problems. Treatment options include positive airway pressure (a machine used to pump air under pressure into the airway of the lungs), adaptive servo-ventilation (a device that tracks and adjusts its pressure to match the breathing pattern of a person with sleep apnea), and phrenic nerve stimulation (treatment that sends electrical stimulation to the patient's phrenic nerve to contract the diaphragm and produce breathing). Treatment improves blood pressure, quality of life, and sleepiness. Results from clinical trials are not definite in how they affected common cardiovascular diseases.",1,C
13,31509499,"The normal sleep-wake cycle is characterized by diurnal variations in blood pressure, heart rate, and cardiac events. Sleep apnea disrupts the normal sleep-heart interaction, and the pathophysiology varies for obstructive sleep apnea (OSA) and central sleep apnea (CSA). Associations exist between sleep-disordered breathing (which encompasses both OSA and CSA) and heart failure, atrial fibrillation, stroke, coronary artery disease, and cardiovascular mortality. Treatment options include positive airway pressure as well as adaptive servo-ventilation and phrenic nerve stimulation for CSA. Treatment improves blood pressure, quality of life, and sleepiness, the last particularly in those at risk for cardiovascular disease. Results from clinical trials are not definitive in terms of hard cardiovascular outcomes.","The normal sleep-wake cycle has daily changes in blood pressure, heart rate, and heart-related events. Sleep apnea (a disorder in which breathing is regularly interrupted during sleep) alters the sleep-heart interaction. The disease-related physical effects vary for obstructive and central sleep apnea, sleep apnea by throat blockage and brain dysfunction, respectively. Links exist between sleep-disordered breathing, which includes both types of sleep apnea, heart failure, irregular heart beats, stroke (brain damage from reduced brain blood supply), coronary artery disease, which is plaque buildup blocking blood flow, and cardiovascular death. Machine-based treatments include positive airway pressure and adaptive servo-ventillation, which both involve pumping air into the lungs, and phrenic nerve stimulation, which involves contracting the diaphragm to breathe, for central sleep apnea (sleep apnea by brain dysfunction). Treatment improves blood pressure, quality of life, and sleepiness, the last especially in those at risk for heart- and blood-related disease. Heart-related results from clinical trials are not definitive.",2,C
13,24088747,"Objective: Nighttime onset of atrial fibrillation (AF) is sometimes associated with obstructive sleep apnea accompanied by a characteristic heart rate (HR) pattern known as cyclical variation of HR. The aim of this study was to evaluate whether cyclical variation of HR is prevalent in patients with nocturnal AF. Methods: The subjects consisted of 34 patients (68±12 years) with paroxysmal AF, including 14 patients with daytime AF and 20 patients with nighttime AF. Holter electrocardiogram (ECGs) were examined for the presence of cyclical variation in HR and to quantify the HR variability within the 40-minute period preceding each AF episode using a fast Fourier transform (FFT) methods. Results: Cyclical variation in HR was observed in 12 of 20 (60%) nighttime episodes and in only two of 14 (14%) daytime episodes. The prevalence of cyclical variation in HR was significantly greater in the nighttime AF episodes than in the daytime AF episodes (Chi=5.34, p<0.05). The mean frequency of cyclical variation in HR was 0.015±0.003 Hz. The mean power of the VLF (very low frequency) component (0.008-0.04 Hz) before the onset of AF was significantly greater in the nighttime AF episodes than in the daytime AF episodes. Among the nighttime AF episodes, the power of the HF (high frequency), LF (low frequency) and very low frequency (VLF) components increased significantly just before the onset of AF compared with that observed 40 minutes before onset. Conclusion: The high prevalence of cyclical variation in HR observed before nocturnal AF episodes suggests that sleep apnea may play a role in the onset of nighttime AF.","Nighttime onset of atrial fibrillation (a fluttery and irregular heartbeat that can lead to blood clots or stroke) is sometimes associated with obstructive sleep apnea (where muscles in the throat relax, the airway is narrowed or closed, and breathing is momentarily cut off) along with a specific heart rate pattern called cyclical variation. This study determines whether cyclical variation of heart rate is common in patients with nighttime atrial fibrillation (AF). There are 34 patients in the study with paroxysmal AF (when the heart rate returns to normal within 7 days on its own or with treatment), including 14 patients with daytime AF and 20 patients with nighttime AF. The presence of cyclical variation in heart rate is examined, and the heart rate differences are measured and counted 40 minutes before each AF episode. Cyclical variation in heart rate is found in 12 of 20 (60%) nighttime episodes and only in two of 14 (14%) daytime episodes. Cyclical variation in heart rate was much greater in the nighttime AF than in daytime AF episodes.  The high presence of cyclical variation in heart rate found before nighttime atrial fibrillation episodes suggests that sleep apnea may play a role in the onset of nighttime atrial fibrillation.",1,C
13,11403753,"Background: For patients presenting with atrial fibrillation of only a few weeks duration, the use of transesophageal echocardiography offers the opportunity to markedly abbreviate the duration of atrial fibrillation before cardioversion. We sought to determine if the shorter duration of atrial fibrillation allowed by a transesophageal echocardiography strategy had an impact on the recurrence of atrial fibrillation and prevalence of sinus rhythm during the first year following cardioversion. Methods: Transesophageal echocardiography was attempted in 539 patients (292 men, 247 women; 71.6 +/- 13.0 years.) with atrial fibrillation > or =2 days (66.1% <3 weeks) or of unknown duration before elective cardioversion of atrial fibrillation. Therapeutic anticoagulation at the time of transesophageal echocardiography was present in 94.6% of patients, and 73.4% of subjects were discharged on warfarin. Results: Atrial thrombi were identified in 70 (13.1%) patients. Successful cardioversion in 413 patients without evidence of atrial thrombi was associated with clinical thromboembolism in 1 patient (0.24%, 95% confidence interval: 0.0--0.8%). In patients with atrial fibrillation <3 weeks at the time of cardioversion (a duration incompatible with conventional therapy of 3 to 4 weeks of warfarin before cardioversion), the 1-year atrial fibrillation recurrence rate was lower (41.1% vs. 57.9%, P <0.01), and the prevalence of sinus rhythm at 1 year was increased (65.8% vs. 51.3%, P <0.03). No other clinical or echocardiographic index was associated with recurrence of atrial fibrillation or sinus rhythm at 1 year. Conclusions: Early cardioversion facilitated by transesophageal echocardiography has a favorable safety profile and provides the associated benefit of reduced recurrence of atrial fibrillation for patients in whom the duration of atrial fibrillation is <3 weeks.","Atrial fibrillation is a fluttery and irregular heartbeat that can lead to blood clots or stroke. For patients that had atrial fibrillation for only a few weeks, using a test that produces pictures of the heart called transesophageal echocardiography is an opportunity to shorten the duration of atrial fibrillation before cardioversion, a procedure used to return an irregular or very fast heartbeat to a normal rhythm. Researchers aimed to find out if the shorter time period of atrial fibrillation from using transesophageal echocardiography impacts how often atrial fibrillation returns and the frequency of sinus rhythm in the first year after cardioversion. Sinus rhythm is the pattern of your heartbeat based on the sinus node of your heart which sends out electrical pulses. Transesophageal echocardiography is used in 539 patients who had atrial fibrillation for two or more days (more than half had atrial fibrillation for less than 3 weeks) or for an unknown duration before non-emergency cardioversion of atrial fibrillation. Blood thinners at the time of the transesophageal echocardiography were used in almost all patients, and 73.4% were discharged on warfarin, a blood thinner to prevent blood clots. Heart-related blood clots were found in 70 (13.1%) patients. Among patients who successfully had the cardioversion procedure, 1 patient had clinical thromboembolism, a blood clot in the vein. In the patients who had atrial fibrillation for less than 3 weeks at the time of cardioversion, the return of atrial fibrillation in the first year was lower, and the frequency of sinus rhythm at 1 year increased. No other clincal or heart evaluations are associated with returning atrial fibrillation or sinus rhythm at 1 year. Having the cardioversion procedure earlier by using the transesophageal echocardiography is shown to be a safe method and is associated with reducing the return of atrial fibrillation in patients who have had the heart condition for less than 3 weeks.",1,C
13,20208092,"Circadian variation in atrial fibrillation (AF) frequency is explored in this paper by employing recent advances in signal processing. Once the AF frequency has been estimated and tracked by a hidden Markov model approach, the resulting trend is analyzed for the purpose of detecting and characterizing the presence of circadian variation. With cosinor analysis, the results show that the short-term variations in the AF frequency exceed the variation that may be attributed to circadian. Using the autocorrelation method, circadian variation was found in 13 of 18 ambulatory ECG recordings (Holter) acquired from patients with long-standing persistent AF. Using the ensemble correlation method, the highest AF frequency usually occurred during the afternoon, whereas the lowest usually occurred during late night. It is concluded that circadian variation is present in most patients with long-standing persistent AF though the short-term variation in the AF frequency is considerable and should be taken into account.","Circadian variation (a part of the natural, internal process that regulates the sleep–wake cycle) in atrial fibrillation (a fluttery and irregular heartbeat that can lead to blood clots or stroke) frequency is explored in this paper by using recent advances in signal processing, which monitors the heart's electrical activity. When the atrial fibrillation frequency is estimated and tracked by signal processing tools, the information is further reviewed to detect and describe the presence of circadian variation. The results show that the short-term variations in the atrial fibrillation frequency are greater than the variation that may be attributed to circadian. Circadian variation is found in 13 of 18 patients with long-standing and persistent (last longer than 7 days) atrial fibrillation. The highest atrial fibrillation frequency usually occurred during the afternoon, whereas the lowest usually occurred during late night. Circadian variation is present in most patients with long-standing persistent atrial fibrillation, though the short-term variation in the AF frequency is great and should be taken into account.",1,C
13,20208092,"Circadian variation in atrial fibrillation (AF) frequency is explored in this paper by employing recent advances in signal processing. Once the AF frequency has been estimated and tracked by a hidden Markov model approach, the resulting trend is analyzed for the purpose of detecting and characterizing the presence of circadian variation. With cosinor analysis, the results show that the short-term variations in the AF frequency exceed the variation that may be attributed to circadian. Using the autocorrelation method, circadian variation was found in 13 of 18 ambulatory ECG recordings (Holter) acquired from patients with long-standing persistent AF. Using the ensemble correlation method, the highest AF frequency usually occurred during the afternoon, whereas the lowest usually occurred during late night. It is concluded that circadian variation is present in most patients with long-standing persistent AF though the short-term variation in the AF frequency is considerable and should be taken into account.","Sleep-wake changes in the frequency of atrial fibrillation (irregular or rapid heart beat) (AF) is explored by using new advances in signal processing. Once the frequency of the irregular heart beat is estimated and tracked by a mathematical technique, the result is measured to detect and characterize sleep-wake changes. With mathematical analysis, the results show that short-term changes in the frequency of the irregular heart beat surpass the changes owing to sleep-wake patterns. With mathematical analysis, sleep-wake changes were found in 13 of 18 ambulatory heart-beat recordings from those with long-lasting frequencies of irregular heart beats. With mathematical analysis, the highest frequency of an irregular heart beat usually came in the afternoon, while the lowest usually came at late night. Thus, sleep-wake changes occur in most with long-lasting AF frequency. However, the short-term changes in AF frequency should be examined.",2,C
13,18760136,"Background: Dronedarone is a new multichannel blocker for atrial fibrillation (AF) previously demonstrated to have both rhythm and rate control properties in paroxysmal and persistent AF. The Efficacy and safety of dRonedArone for The cOntrol of ventricular rate during atrial fibrillation (ERATO) trial assessed the efficacy of dronedarone in the control of ventricular rate in patients with permanent AF, when added to standard therapy. Methods: In this randomized, double-blind, multinational trial, dronedarone, 400 mg twice a day (n = 85), or matching placebo (n = 89) was administered for 6 months to adult patients with permanent AF, in addition to standard therapy. The primary end point was the change in mean ventricular rate between baseline and day 14, as assessed by 24-hour Holter. Ventricular rate was also assessed during submaximal and maximal exercise. Results: Dronedarone significantly decreased mean 24-hour ventricular rate. Compared with placebo, the mean treatment effect at day 14 was a reduction of 11.7 beats per minute (beat/min; P < .0001). Comparable reductions were sustained throughout the 6-month trial. During maximal exercise and compared to placebo, there was a mean reduction of 24.5 beat/min (P < .0001), without any reduction in exercise tolerance as measured by maximal exercise duration. The effects of dronedarone were additive to those of other rate-control agents, including beta-blockers, calcium antagonists, and digoxin. Dronedarone was well tolerated, with no organ toxicities or proarrhythmia. Conclusion: In addition to its reported rhythm-targeting and rate-targeting therapeutic actions in paroxysmal and persistent AF, dronedarone improves ventricular rate control in patients with permanent AF. Dronedarone was well tolerated with no evidence of organ toxicities or proarrhythmias in this short-term study.","Dronedarone is a new drug that can treat atrial fibrillation (a fluttery and irregular heartbeat that can lead to blood clots or stroke) or AF and is found to help control the heart rhythm and heart rate in patients with paroxysmal (when the heart rate returns to normal within 7 days on its own or with treatment) or with persistent (greater than 7 days) atrial fibrillation. A clinical trial called the Efficacy and safety of dRonedArone for The cOntrol of ventricular rate during atrial fibrillation (ERATO) reviewed how well dronedarone worked to control the ventricular rate (heart rate) in patients with permanent AF, when added to other treatments. In this clinical trial of patients with permanent AF, 85 patients receive the dronedarone drug, and 85 receive a placebo (an inactive substance that looks like the treatment drug) for 6 months. A key measure is the average change in ventricular rate between the start of the trial and day 14 of the trial. Ventricular rate is also evaluated during submaximal exercise (any physical activity with increased intensity in which heart rate does not go above 85) and maximal exercise (physical activity increased to come close to fatigue). Dronedarone significantly decreased the average 24-hour ventricular rate. When compared to the placebo group, the average effect of dronedarone on day 14 was a reduction of 11.7 beats per minute. Similar reductions continued throughout the 6-month trial. There was a reduction in heart beats per minute during maximal exercise when compared to the placebo group. The effects of dronedarone were an addition to the effects of other drugs that control heart rate. Dronedarone was well tolerated with few side effects. In addition to its previously demonstrated effects on heart rhythm and rate in short-term and long-term AF, dronedarone improves ventricular rate control in patients with permanent AF. Dronedarone was well tolerated with no evidence of damage to organs or worsening of heart condition in this short-term study.",1,C
13,26048677,"Introduction: Sleep apnea-hypopnea syndrome (SAHS) is one of the extracardiac reasons of atrial fibrillation (AF), and the prevalence of AF is high in SAHS-diagnosed patients. Nocturnal hypoxemia is associated with AF, pulmonary hypertension, and nocturnal death. The rate of AF recurrence is high in untreated SAHS-diagnosed patients after cardioversion (CV). In this study, we present a patient whose SAHS was diagnosed with an apnea test performed in the intensive care unit (ICU) and who did not develop recurrent AF after the administration of standard AF treatment and bi-level positive airway pressure (BiPAP). Case presentation: A 57-year-old male hypertensive Caucasian patient who was on medical treatment for 1.5 months for non-organic AF was admitted to the ICU because of high-ventricular response AF (170 per minute), and sinus rhythm was maintained during the CV that was performed two times every second day. The results of the apnea test performed in the ICU on the same night after the second CV were as follows: apnea-hypopnea index (AHI) of 71 per hour, minimum peripheral oxygen saturation (SpO2) of 67%, and desaturation period (SpO2 of less than 90%) of 28 minutes. The patient was discharged with medical treatment and nocturnal BiPAP treatment. The results of the apnea test performed under BiPAP on the sixth month were as follows: AHI of 1 per hour, desaturation period of 1 minute, and minimum SpO2 of 87%. No recurrent AF developed in the patient, and his medical treatment was reduced within 6 months. After gastric bypass surgery on the 12th month, nocturnal hypoxia and AF did not re-occur. Thus, BiPAP and medical treatments were ended. Conclusions: SAHS can be diagnosed by performing an apnea test in the ICU. SAHS should be investigated in patients developing recurrent AF after CV. Recovery of nocturnal hypoxia may increase the success rate of standard AF treatment.","Sleep apnea-hypopnea syndrome (SAHS) (a sleep disorder of recurring episodes of partial or complete upper airway collapse during sleep) is one of the causes of atrial fibrillation (AF) (a fluttery and irregular heartbeat that can lead to blood clots or stroke). The occurrence of AF is high in SAHS-diagnosed patients. Nocturnal hypoxemia (a temporary drop in oxygen while sleeping) is associated with AF, pulmonary hypertension (high blood pressure affecting the arteries in the lungs and heart), and nocturnal death (death in sleep caused by sudden cardiac death). The frequency of AF recurring is high in people with untreated SAHS after cardioversion, a procedure used to return an irregular or very fast heartbeat to a normal rhythm. This study summarizes the case of a patient with SAHS diagnosed by an apnea test (monitoring of breathing and oxygen) and who did not develop recurring (chronic) atrial fibrillation after receiving standard treatment and the bi-level positive airway pressure (BiPAP), a ventilator used to maintain a consistent breathing pattern (often used at night). A 57-year old male patient who was on treatment for 1.5 months for AF was admitted to the hospital because of a high, irregular heart beat. The sinus rhythm (the pattern of your heartbeat based on the sinus node of your heart which sends out electrical pulses) was steady during the cardioversion procedure. Heart and oxygen tests were performed in the hospital after the second cardioversion. The patient was sent home with medical treatment and a nocturnal BiPAP. No recurring atrial fibrillation developed in the patient, and his medical treatment was reduced within 6 months. After gastric bypass surgery on the 12th month, nocturnal hypoxia and atrial fibrillation did not re-occur. Because they did not recur, the BiPAP and medical treatments were ended. In conclusion, SAHS can be diagnosed by performing an apnea test in the hospital. SAHS should be investigated in patients developing recurring atrial fibrillation after a cardioversion procedure. Recovery of nocturnal hypoxia may increase the success of standard atrial fibrillation treatment.",1,C
13,30793493,"Background: Sleep-disordered breathing (SDB) and atrial fibrillation (AF) are associated. This study investigated the impact of AF intervention on 6-month home sleep testing data. Methods: Sixty-seven patients (aged 66 to 86, 53% male) with persistent AF were randomized (1:1:1) to direct current cardioversion (DCCV) (22 patients), permanent pacemaker (PPM) + atrioventricular node ablation (AVNA) + DCCV (22 patients) or AF ablation (23 patients). Baseline and 6-month multichannel home sleep tests with the Watch-PAT200 (Itamar Medical Lts., Caesarea, Israel) were recorded. Implantable cardiac monitors (ICMs) (Medtronic Reveal XT, Minneapolis, Minnesota) in the DCCV and AF ablation groups, and PPM Holters in the 'pace and ablate' group were utilized to assess cardiac rhythm beat-to-beat throughout the study period. Results: The prevalence of moderate-to-severe SDB [apnoea-hypopnoea index (AHI) ≥ 15/h] was 60%. At 6 months there was no change in AHI, Epworth sleepiness scale, sleep time, % REM sleep, respiratory desaturation index or central apnoeic events. Twenty-five patients (15 AF ablation, 9 DCCV and 1 following DCCV post-AVNA) maintained SR at 6 months confirmed on ICMs in these patients. AHI fell from 29.8 ± 26.6/h to 22.2 ± 20.4/h; P = 0.049. Conclusions: SDB is highly prevalent in patients with persistent AF. Restoration of sinus rhythm, and the associated long-term recovery of haemodynamics, is associated with a significant reduction in AHI. This implicates reversal of fluid shift from the lower limbs to the neck region, a key mechanism in the pathogenesis of SDB.","There are associations between sleep-disordered breathing (SDB), a potentially serious sleep disorder in which breathing repeatedly stops and starts, and atrial fibrillation (AF), an irregular and often very rapid heart rhythm that can lead to blood clots in the heart. This study investigates the impact of AF interventions and treatments on 6-month home sleep testing data. Sixty-seven patients with persistent (lasting longer than 7 days) atrial fibrillation were randomly put in 3 different treatment groups: 1) cardioversion (a procedure used to return an irregular or very fast heartbeat to a normal rhythm), 2) permanent pacemaker (a small device that is inserted under the skin of the chest to help the heart beat normally) with atrioventricular node ablation (heat energy to destroy a small amount of tissue between the upper and lower chambers of your heart) with cardioversion, or 3) AF ablation (using small burns or freezes to cause some scarring on the inside of the heart to help break up the electrical signals that cause irregular heartbeats). Home sleep tests were recorded at the start of the study and at 6 months. Other devices were used to monitor heart rhythm throughout the study period. The occurrence of moderate-to-severe SBD, that measured over 15 for apnoea-hypopnoea index (AHI), the number of breathing pauses or disruptions per hour while asleep, was found in more than half (60%) of patients. At six months there was no change in AHI, sleepiness during the day, sleep time, rapid eye movement (REM) sleep, and other tests that measure sleep and breathing patterns. At 6 months, 25 patients (15 from the AF ablation group) had a steady sinus rhythm, the pattern of your heartbeat based on the sinus node of your heart which sends out electrical pulses. AHI fell from 29.8 ± 26.6/h to 22.2 ± 20.4/h. In conclusion, sleep-disordered breathing is very common in patients with persistent atrial fibrillation. Restoration of sinus rhythm, and the associated long-term recovery of normal heart function, is associated with a large reduction in AHI. This finding suggests that reversing fluid from the lower limbs to the neck area is a key process in the development of sleep-disordered breathing.",1,C
13,30793493,"Background: Sleep-disordered breathing (SDB) and atrial fibrillation (AF) are associated. This study investigated the impact of AF intervention on 6-month home sleep testing data. Methods: Sixty-seven patients (aged 66 to 86, 53% male) with persistent AF were randomized (1:1:1) to direct current cardioversion (DCCV) (22 patients), permanent pacemaker (PPM) + atrioventricular node ablation (AVNA) + DCCV (22 patients) or AF ablation (23 patients). Baseline and 6-month multichannel home sleep tests with the Watch-PAT200 (Itamar Medical Lts., Caesarea, Israel) were recorded. Implantable cardiac monitors (ICMs) (Medtronic Reveal XT, Minneapolis, Minnesota) in the DCCV and AF ablation groups, and PPM Holters in the 'pace and ablate' group were utilized to assess cardiac rhythm beat-to-beat throughout the study period. Results: The prevalence of moderate-to-severe SDB [apnoea-hypopnoea index (AHI) ≥ 15/h] was 60%. At 6 months there was no change in AHI, Epworth sleepiness scale, sleep time, % REM sleep, respiratory desaturation index or central apnoeic events. Twenty-five patients (15 AF ablation, 9 DCCV and 1 following DCCV post-AVNA) maintained SR at 6 months confirmed on ICMs in these patients. AHI fell from 29.8 ± 26.6/h to 22.2 ± 20.4/h; P = 0.049. Conclusions: SDB is highly prevalent in patients with persistent AF. Restoration of sinus rhythm, and the associated long-term recovery of haemodynamics, is associated with a significant reduction in AHI. This implicates reversal of fluid shift from the lower limbs to the neck region, a key mechanism in the pathogenesis of SDB.","Sleep-disordered breathing and irregular heart beats are linked. This study examines irregular heart beat treatment on 6-month home sleep testing data. Sixty-seven patients (aged 66 to 86, 53% male) with long-lasting atrial fibrillation (irregular or rapid heart beat) were randomly grouped (1:1:1) to three standard treatments for atrial fibrillation.  The presence of moderate-to-severe sleep-disordered breathing [number of breathing interruptions during sleep ≥ 15/h] was 60%. At 6 months, there was no change in sleep-disordered breathing measurements. Twenty-five patients (with standard treatments for irregular heart beat) maintained sinus rhythm for 6 months confirmed on implanted heart beat monitors in these patients. Number of breathing interruptions during sleep fell from 29.8 ± 26.6/h to 22.2 ± 20.4/h. Sleep-disordered breathing is very present in patients with lasting irregular heart beats. Recovery of sinus rhythm, and the linked long-term recovery of blood flow, is linked with a noticeable reduction in the number of breathing interruptions during sleep. These fewer breathing interruptions during sleep imply reversal of fluid shift from the lower limbs to the neck region, which is key for causing sleep-disordered breathing.",2,C
13,19009769,"Obstructive sleep apnea syndrome (OSA) is associated with different types of cardiac arrhythmias. The original studies, concentrated mostly on nocturnal brady- and tachyarrhythmias. More recent studies documented high prevalence of atrial fibrillation (AF) and its association with obesity and other risk factors for AF. In addition, continuous positive airway pressure (CPAP) prevents recurrence of AF after cardioversion. In, OSA the highest risk for sudden death is at night in comparison to general population most of who die suddenly between six and noon. This observation suggests that hypoxia or other nocturnal abnormality, trigger sudden death. An important recent finding is the beneficial effect of CPAP on sudden death. The role of pacing in OSA remains controversial. In general, pacemaker therapy is not indicated in patients with nocturnal bradyarrhythmias. However, some authors recommend pacing in those with severe nocturnal bradyarrhythmias not tolerating or not responding to CPAP. According to a recent study, 59% of patients with permanent pacemaker have OSA.","Obstructive sleep apnea syndrome (OSA) occurs as muscles in the throat relax and the airway narrows or closes, and breathing is momentarily cut off. OSA is associated with different types of cardiac arrhythmias (irregular heartbeat occuring when electrical impulses in the heart do not work properly). Past studies mostly focus on nighttime heartbeats that are too slow (bradyarrhythmias) or too fast (tachyarrhythmias). Recent studies document very common occurrences of atrial fibrillation (an irregular and often very rapid heart rhythm that can lead to blood clots in the heart) and their association with obesity and other risk factors for atrial fibrillation. In addition, continuous positive airway pressure (CPAP), a device to help people with OSA breathe more easily while sleeping, prevent atrial fibrillation from returning after cardioversion (a procedure used to return an irregular or very fast heartbeat to a normal rhythm). For people with obstructive sleep apnea syndrome, the highest risk for sudden death is at night. This observation suggests that hypoxia (not enough oxygen in the tissues for the body to function properly) or other nighttime abnormalities trigger sudden death. An important recent finding is the beneficial impact of CPAP on sudden death. The role of pacing (controlling the heartbeat) in patients with OSA remains controversial. In general, using a pacemaker (a small device that's placed or implanted in the chest to help control the heartbeat) is not recommended in patients with nighttime heart rate that is too slow (bradyarrhythmias). However, some researchers recommend pacing in people with severe nighttime bradyarrhythmias who are not able to use or are not responding to CPAP. According to a recent study, 59% of patients with a permanent pacemaker have obstructive sleep apnea syndrome (OSA).",1,C
13,30571183,"Background: Paroxysmal atrial fibrillation (AF) can be caused by gain-of-function mutations in genes, encoding the cardiac potassium channel subunits KCNJ2, KCNE1, and KCNH2 that mediate the repolarizing potassium currents Ik1, Iks, and Ikr, respectively. Methods: Linkage analysis, whole-exome sequencing, and Xenopus oocyte electrophysiology studies were used in this study. Results: Through genetic studies, we showed that autosomal dominant early-onset nocturnal paroxysmal AF is caused by p.S447R mutation in KCND2, encoding the pore-forming (α) subunit of the Kv4.2 cardiac potassium channel. Kv4.2, along with Kv4.3, contributes to the cardiac fast transient outward K+ current, Ito. Ito underlies the early phase of repolarization in the cardiac action potential, thereby setting the initial potential of the plateau phase and governing its duration and amplitude. In Xenopus oocytes, the mutation increased the channel's inactivation time constant and affected its regulation: p.S447 resides in a protein kinase C (PKC) phosphorylation site, which normally allows attenuation of Kv4.2 membrane expression. The mutant Kv4.2 exhibited impaired response to PKC; hence, Kv4.2 membrane expression was augmented, enhancing potassium currents. Coexpression of mutant and wild-type channels (recapitulating heterozygosity in affected individuals) showed results similar to the mutant channel alone. Finally, in a hybrid channel composed of Kv4.3 and Kv4.2, simulating the mature endogenous heterotetrameric channel underlying Ito, the p.S447R Kv4.2 mutation exerted a gain-of-function effect on Kv4.3. Conclusions: The mutation alters Kv4.2's kinetic properties, impairs its inhibitory regulation, and exerts gain-of-function effect on both Kv4.2 homotetramers and Kv4.2-Kv4.3 heterotetramers. These effects presumably increase the repolarizing potassium current Ito, thereby abbreviating action potential duration, creating arrhythmogenic substrate for nocturnal AF. Interestingly, Kv4.2 expression was previously shown to demonstrate circadian variation, with peak expression at daytime in murine hearts (human nighttime), with possible relevance to the nocturnal onset of paroxysmal AF symptoms in our patients. The atrial-specific phenotype suggests that targeting Kv4.2 might be effective in the treatment of nocturnal paroxysmal AF, avoiding adverse ventricular effects.","Paroxysmal atrial fibrillation (AF) (an irregular heart rate that returns to normal within 7 days on its own or with treatment) can be caused by mutations in genes. Genetic linkage analysis to trace diseases in families using genes, whole-exome sequencing to find a genetic cause of a disease, and models to study biological processes of cells were used in this study. Through these genetic studies, researchers show that one genetic trait or mutation that is passed down from parent to child causing nighttime paroxysmal atrial fibrillation is caused by a mutation in the KCND2 gene. Kv4.2 and Kv4.3 are potassium channels that release potassium from cells and contribute to the heart-related, temporary, fast, outward potassium (K+) current, Ito. Ito is the base of the early phase of repolarization (when the outward current of ions exceeds the inward current) in the cardiac action potental (where unique properties necessary for function of the electrical conduction system of the heart occur), and creates the initial potential of the plateau phase (the time that allows for longer muscle contraction and allows the heart to contract in a steady, uniform, and forceful manner). In studies that modeled the cell's biology, the mutation increased the potassium channel's inactivation time (the time when the channel no longer allows potassium to be passed through it) and affected its regulation. The mutant Kv4.2 showed an impaired response to protein kinase C (PKC), a protein that regulates cells growth and plays a major role in sending signals to the heart.  The gene mutation alters Kv4.2's transfer properties, impairs its regulation process, and exerts gain-of-function effect (changes the activity or function of a protein) in both Kv4.2 and Kv4.2-Kv4.3 potassium channels. These effects possibly increase the repolarizing potassium current Ito, creating arrhythmogenic substrates (factors that can produce or lead to arrhythmia) for nocturnal atrial fibrillation (an irregular and often very rapid heart rhythm occuring at night that can lead to blood clots in the heart). Kv4.2 expression has previously been shown to demonstrate circadian variation (the natural, internal process that regulates the sleep–wake cycle and repeats roughly every 24 hours), with peak expression at daytime, with possible relevance to the nighttime onset of symptoms of paroxysmal atrial fibrillation (an irregular heart rate that returns to normal within 7 days on its own or with treatment) in patients. Targeting Kv4.2 might be effective in the treatment of nocturnal paroxysmal atrial fibrillation.",1,C
13,18612186,"In the treatment of arrhythmia, beta-blockers are mainly used to regulate the heart rate. However, beta-blockers are also known as drugs with an antiarrhythmic effect due to the suppression of sympathetic activity. We evaluated the antiarrhythmic effects of a highly selective beta(1)-blocker, bisoprolol, in patients with diurnal paroxysmal atrial fibrillation (P-AF). A total of 136 patients with symptomatic diurnal P-AF were enrolled. Patients were divided into a diurnal-specific P-AF group and a diurnal & nocturnal P-AF group, as well as into a bisoprolol single use group and a combined use group with an antiarrhythmic drug. The effects of bisoprolol were evaluated in 3 categories: subjective symptom improvement, quality of life (QOL) improvement, and elimination of P-AF episode in Holter electrocardiograms (ECGs). For patients with effective treatment, a long-term effect up to 24 months was evaluated. Five patients (3.7%) discontinued bisoprolol due to side effects. Following administration of bisoprolol, 109 patients (80%) experienced subjective symptom improvement, 103 patients (76%) experienced QOL improvement, and elimination of P-AF episodes in ECGs was observed in 84 patients (62%). The elimination rate of P-AF episodes in ECGs was higher in the diurnal P-AF group than in the diurnal & nocturnal P-AF group (P=0.042). There was no significant difference between the bisoprolol single use group and the combined use group. A long-term suppressive effect by bisoprolol was observed in 70 of 83 patients (84%). The results demonstrate that bisoprolol has an antiarrhythmic effect against sympathetic diurnal P-AF, improving subjective symptoms and QOL and eliminating P-AF episodes in ECGs.","In the treatment of arrhythmia (an irregular heartbeat), beta-blockers (medications that reduce blood pressure) are mainly used to regulate the heart rate. However, beta-blockers are also known as drugs with an antiarrhythmic effect (drugs that slow down the electrical impulses of the heart) due to the suppression of sympathetic activity (the part of the nervous system that increases heart rate, blood pressure, and other heart functions). Researchers evaluated how electrical impulses of the heart are slowed down when using a beta-blocker called bisoprolol in patients with diurnal (during the day) paroxysmal atrial fibrillation (an irregular heart rate that returns to normal within 7 days on its own or with treatment). A total of 136 patients with symptomatic daytime paroxysmal atrial fibrillation (P-AF) were included in the study. Patients were divided into a daytime P-AF or a daytime and nighttime P-AF group, as well as into a group that only uses bisoproplol and a group that uses a combination of heart treatment drugs. The effects of bisoprolol are evaluated in 3 categories: symptom improvement, quality of life improvement, and elimination of paroxysmal atrial fibrillation (P-AF) events, which are measured using a portable device called Holter electrocardiograms (ECGs) to record heart rhythms. For patients with effective treatment, a long-term effect up to 24 months was evaluated. Five patients stopped using bisoprolol due to side effects. Following use of bisoprolol, 80% experienced symptom improvement, 76% experienced quality of life improvement, and elimination of P-AF episodes in ECGs was observed in 62%. The elimination rate of paroxysmal atrial fibrillation episodes in ECGs was higher in the daytime group than in the daytime and nighttime group. There was no significant difference between the group that only used bisoprolol and the combined use group. A long-term effect of reducing P-AF using bisoprolol was found in 84% of patients. This study shows that bisoprolol can slow down the electrical impulses of the heart in daytime paroxysmal atrial fibrillation, improve symptoms and quality of life, and eliminate paroxysmal atrial fibrillation episodes in ECGs.",1,C
13,18612186,"In the treatment of arrhythmia, beta-blockers are mainly used to regulate the heart rate. However, beta-blockers are also known as drugs with an antiarrhythmic effect due to the suppression of sympathetic activity. We evaluated the antiarrhythmic effects of a highly selective beta(1)-blocker, bisoprolol, in patients with diurnal paroxysmal atrial fibrillation (P-AF). A total of 136 patients with symptomatic diurnal P-AF were enrolled. Patients were divided into a diurnal-specific P-AF group and a diurnal & nocturnal P-AF group, as well as into a bisoprolol single use group and a combined use group with an antiarrhythmic drug. The effects of bisoprolol were evaluated in 3 categories: subjective symptom improvement, quality of life (QOL) improvement, and elimination of P-AF episode in Holter electrocardiograms (ECGs). For patients with effective treatment, a long-term effect up to 24 months was evaluated. Five patients (3.7%) discontinued bisoprolol due to side effects. Following administration of bisoprolol, 109 patients (80%) experienced subjective symptom improvement, 103 patients (76%) experienced QOL improvement, and elimination of P-AF episodes in ECGs was observed in 84 patients (62%). The elimination rate of P-AF episodes in ECGs was higher in the diurnal P-AF group than in the diurnal & nocturnal P-AF group (P=0.042). There was no significant difference between the bisoprolol single use group and the combined use group. A long-term suppressive effect by bisoprolol was observed in 70 of 83 patients (84%). The results demonstrate that bisoprolol has an antiarrhythmic effect against sympathetic diurnal P-AF, improving subjective symptoms and QOL and eliminating P-AF episodes in ECGs.","To treat irregular heart beats, beta-blockers (drugs that reduce blood pressure) are used to regulate heart rate. However, beta-blockers also prevent irregular heart beats due to the reduction of nerve-related, stimulating activity. We measured the irregular-heart-beat-preventing effects of a highly specific drug that reduces blood pressure, bisoprolol, in patients with daily paroxysmal atrial fibrillation (P-AF), which involves sudden occurences of an irregular or rapid heart beat. 136 patients with typical daily P-AF were employed. Patients were divided into a group with daily-specific P-AF and a daily and nightly P-AF group. Patients were also divided into a group with single use of bisoprlol and a combined use group with an drug that prevents an irregular heart beat. For patients with treatment, a long-term effect up to 24 months was measured. Five patients (3.7%) stopped bisoprolol due to side effects. After using a drug that reduces blood pressure, 109 patients (80%) had personal improvement, and 103 pateints (76%) had quality-of-life improvement. Elimination of sudden occurences of an irregular heart beat was measured in 84 patients (62%). The elimination rate of episodes of P-AF was higher in the daily P-AF group than in the daily and nightly P-AF group. There was no difference between the group with single use of bisoprolol and the combined use group. The long-term effect of bisoprolol was seen in 70 of 83 patients (84%). Bisoprolol helps prevent an irregular heart beat against nerve-related, sudden episodes of an irregular heart beat, improving personal symptoms and eliminating episodes.",2,C
17,1233473,"180 cases of head trauma were classified according to the degree of impairment of consciousness, clinical and neurological symptoms and EEG patterns. Based on the radiological and clinical findings and blood gas analyses a study was made of the incidence and extent of aspiration of blood, vomit or debris into the tracheo-bronchial tree and of the resultant pulmonary complications. As loss of consciousness became more complete the incidence of aspiration and the amount of material inhaled increased. Clinically and radiologically proven aspiration occurred in 60 per cent of cases of severe head trauma. A comparison of two groups after they had been given first aid and artificial respiration showed that the paO2 values were significantly lower in patients with radiologically proven aspiration and infiltration of the lungs than they were in those with normal chest radiograms. These observations point to the relationship between the quantity of material inhaled and the extent of intra-pulmonary shunting. There was no difference in the incidence of aspiration between persons who had been intubated and those who had not been intubated prior to admission to hospital. Although in many cases of head trauma aspiration of blood immediately after the accident can not be prevented prompt intubation is the only measure that will mitigate the consequences of aspiration and prevent its recurrence. As the latter is a very real risk in the unconscious person intubation in these cases is a ""must"". The study also showed that aspiration of foreign material into the tracheobronchial system and the resultant pulmonary complications can be successfully treated even if the head trauma is very severe. In none of the cases studied was death attributable to these causes. Apart from intubation and bronchial toilet artificial respiration with oxygen-enriched gas mixtures has a decisive influence on the course of the aspiration-induced pulmonary complications.","We divided 180 cases of head injury based on the level of loss of wakefulness and awareness, medical and brain-related symptoms observed by a doctor, and a common brain function test. Based on x-rays, symptoms observed by a doctor, and blood gas tests, we studied how often and how much blood, vomit or debris entered the airways and the lung complications that occurred as a result. As consciousness went away, how often material entered the airways and the amount of material breathed in increased. Material entering the airways happened in 60 percent of cases of severe head injury based on symptoms observed by a doctor and x-rays. A comparison of two groups after they received first aid and artificial aid to breathe showed that the levels of oxygen dissolved in the blood were much lower in patients with x-rays that showed material entered the airways and the lungs than those with normal chest x-rays. These findings suggest a relationship between the amount of material breathed in and how much blood put out by the heart lacks enough oxygen. Persons who had a breathing tube and those who did not before admission to the hospital did not differ in how often material entered the airways. Although often in head injuries blood entering the airways after the accident cannot be prevented quickly, putting in a breathing tube is the only thing that will lessen the consequences of material entering the airways and prevent it from happening again. As the second result is a very real risk in the unconscious person, putting in a breathing tube in these cases is a ""must"". The study also showed that material from outside the body entering the airways and the lung complications that occurred as a result can be successfully treated even if the head injury is very severe. In the cases we looked at, death did not result from any of these causes. Aside from putting in a breathing tube and clearing mucus and secretions from the airways, substituting a person's breathing with gas mixtures with high levels of oxygen plays a big role in lung complications from material entering the airways.",1,C
17,1233473,"180 cases of head trauma were classified according to the degree of impairment of consciousness, clinical and neurological symptoms and EEG patterns. Based on the radiological and clinical findings and blood gas analyses a study was made of the incidence and extent of aspiration of blood, vomit or debris into the tracheo-bronchial tree and of the resultant pulmonary complications. As loss of consciousness became more complete the incidence of aspiration and the amount of material inhaled increased. Clinically and radiologically proven aspiration occurred in 60 per cent of cases of severe head trauma. A comparison of two groups after they had been given first aid and artificial respiration showed that the paO2 values were significantly lower in patients with radiologically proven aspiration and infiltration of the lungs than they were in those with normal chest radiograms. These observations point to the relationship between the quantity of material inhaled and the extent of intra-pulmonary shunting. There was no difference in the incidence of aspiration between persons who had been intubated and those who had not been intubated prior to admission to hospital. Although in many cases of head trauma aspiration of blood immediately after the accident can not be prevented prompt intubation is the only measure that will mitigate the consequences of aspiration and prevent its recurrence. As the latter is a very real risk in the unconscious person intubation in these cases is a ""must"". The study also showed that aspiration of foreign material into the tracheobronchial system and the resultant pulmonary complications can be successfully treated even if the head trauma is very severe. In none of the cases studied was death attributable to these causes. Apart from intubation and bronchial toilet artificial respiration with oxygen-enriched gas mixtures has a decisive influence on the course of the aspiration-induced pulmonary complications.","180 cases of head trauma were classified by severity of damage to consciousness, medical and brain-related effects, and brain activity measurements. Based on x-rays, medical findings, and blood analyses, the frequency and extent of breathing in blood, vomit, or debris into the lungs and of its effects were measured. As consciousness decreased, the frequency and extent of breathing in material increased. Medically and x-ray proven breathing in of fluid occured in 60% of reports of severe head trauma. Comparing two groups after both received first aid and artificial breathing showed that oxygen was lower in patients with x-ray proven breathing in of fluids in the lungs than unaffected patients. A relationship exists between the amount inhaled and extent of fluid moved into the lungs. There was no change in the frequency of breathing in fluids between patients with breathing tubes and those without before arriving to the hospital. In many cases, while breathing in of blood cannot be stopped after head trauma, prompt use of breathing tubes is the only way to lessen the effects of breathing in fluids and prevent its return. Breathing in fluids again is a serious risk. For unconscious patients, breathing tubes are a ""must"". The study showed that breathing in foreign material into the lungs and the resulting effects can be treated even with severe head trauma. In no cases studied was death owing to these causes. Besides adding tubes and removing lung waste, artificial breathing in of oxygen-enriched gases greatly influence breathing-related effects.",2,C
17,12224233,"The patient who presents with a serious head injury is often very difficult to manage. The airways is of primary concern; adequate ventilation must be provided and aspiration protected against. Recent studies suggest that hyperventilation may be as beneficial as was earlier believed. As the pCO2 level decreases, vasoconstriction occurs. If the level falls too low, cerebral perfusion is restricted, and profound cerebral anoxia may ensue. Current standards call for a ventilatory rate to allow for moderate respiratory alkalosis, in theory to mildly constrict teh vessels but still provide adequate perfusion. Arterial blood gas analysis in the ED is the definitive measurement of airway management in the field. Remember that the anatomy of the meningeal layers places the arteries primarily in the epidural space and the veins in the subdural space. A bleed in the epidural space often presents with a rapid onset of signs and symptoms, as was obvious in this traumatized patient. When a bleed occurs in the subdural space, the onset is usually more insidious, and an accurate history is a key to field diagnosis. As the hemorrhage expands, compression displaces the brain within the cranial vault. This displacement causes pressure to be exerted on the medulla of the brainstem. Cushing's Traid is a result of this pressure on the medulla and is evidence by the pulse slowing while systolic blood pressure rises and respirations become ataxic. Vomiting is often associated, and as the bleed continues, herniation syndrome begins. Decorticate posturing is displayed, followed by decerebrate posturing if relief is not provided. It is important to distinguish between decorticate and decerebrate posturing. It is important to distinguish between decorticate and decerebrate posturing. An easy way to remember the differences is to picture the anatomy of the brain. The cerebral cortex lies above the cerebellum, so when a patient's arms flexed up toward the face , he is pointing to his ""core"" (de-cor-ticate). As the arms extend downward, he is pointing to his cerebellum(de-cere-brate). T o manage the head-injured patient, it is imperative to anticipate potential developments, as well as protect against underlying injuries that may not be fully evaluated until arrival at the ED. Cervical spine often accompany head injuries, and full spinal immobilization is a mandatory precaution in all presentations. With the expanding hematoma found on this patient's neck, vascular damage ws obvious and contributed to the suspicion of spinal injury. As the intracranial pressure rise, vomiting and seizures are common. Placement of an endotracheal tube and having suction equipment ready are the best tools to prevent against aspiration. It is possible to angle the long spine board 10-15 degrees, exercising caution to ensure the patient's spinal alignment is not manipulated during the process. Seizures are usually treated with anticonvulsants like Valium. When a seizure accompanies a head injury, it is a direct result of the increased intracranial pressure and has a generally poor response to Valium, as the underlying cause of the seizure still exists. In this case, the patient had a full neuromuscular blockade, and any seizure would not have been recognized as long as the paralytics were on board. Early notification to the ED is essential, reporting all findings and interventions. This can alert them and give them the opportunity to prepare specialized equipment, such as CT scanners, mechanical ventilators, etc. Also, consider transportation options and the length of time to definitive care, including neurosurgical evaluation. This patient needs to be seen in a trauma center capable of the most thorough evaluation and management. Evacuation by air ambulance may be the most appropriate method of transport.","A person with a serious head injury is often very difficult to treat. The airways are the most important concern; enough air movement must be provided, and material entering the airway or lungs by accident must be avoided. Recent studies suggest that quick breathing or hyperventilation may be as helpful as was thought before. As the amount of carbon dioxide gas dissolved in the blood decreases, blood vessels narrow. If the amount of carbon dioxide gas dissolved in the blood gets too low, blood flow to the brain is limited, and not enough oxygen getting to the brain may follow. Current standards suggest a rate of movement of air into and out of the lungs to allow for a decrease in carbon dioxide gas dissolved in the blood, in theory to narrow the vessels but still allow enough circulation of blood through organs and tissues. Measuring oxygen and carbon dioxide levels in the blood in the emergency department is the best measurement of airway treatment by first responders. Remember that the structure of the brain's protective layers puts the arteries in the space between the outermost layer and the skull (epidural) and the veins in the space between the outermost layer and the brain (subdural). A bleed in the epidural space often has quick signs and symptoms, as was obvious in this injured patient. A bleed in the subdural space usually happens more slowly, and knowing what happened to the person is key to a diagnosis by first responders. As the bleeding spreads, pressure pushing on the brain moves the brain within the skull. This movement of the brain puts pressure on the area of the brain that controls things like heartbeat and breathing (medulla). Pressure on the medulla causes Cushing's Triad, which is a slowing heartbeat while blood pressure increases and breathing becomes abnormal. Vomiting often occurs, and as the bleed continues, something inside the skull produces pressure that moves brain tissues. A person gets stiff with bent arms, clenched fists, and legs held out straight (decorticate posturing), followed by the arms and legs being held straight out, the toes being pointed downward, and the head and neck being arched backward (decerebrate posturing) if relief is not given. It is important to recognize the difference between decorticate and decerebrate posturing. It is important to recognize the differences between decorticate and decerebrate posturing. An easy way to remember the differences is to picture the structure of the brain. One part of the brain, the cerebral cortex, lies above another part of the brain, the cerebellum, so when a patient's arms point toward the face, he is pointing to his ""core"" (de-cor-ticate). As the arms go down to his side, he is pointing to his cerebellum (de-cere-brate). To treat the patient with a head injury, it is important to predict what might happen, and protect against other injuries that may not be realized until the patient gets to the hospital. Neck injuries often happen with head injuries, and making sure the spine cannot move is required in all cases. With the growing bruise on this patient's neck, damage to blood vessels was clear and helped lead to the belief of a spinal injury. As the pressure in the brain rises, vomiting and seizures are common. Putting in a breathing tube and having suction tools ready are the best ways to prevent material entering the airway or lungs by accident. It is possible to angle the rescue board 10-15 degrees, making sure to not change the alignment of the patient's spine. Seizures are usually treated with drugs that prevent convulsions, like Valium. When a seizure happens with a head injury, it is caused by rising pressure in the brain, and Valium does not usually help, as the cause for the seizure is not resolved. In this case, the patient had a full nerve block (paralyzed skeletal muscles). Any seizure would not have been seen while the paralyzing drugs were working. Notifying the emergency department is very important, reporting everything the first responders see and do to treat the patient. This can warn emergency staff and give them the chance to get specialized equipment ready (e.g., CT scanners, mechanical ventilators). Also, think about how to get the patient to the hospital and how much time it will take to best care for the patient, including seeing a brain surgeon. This patient needs to be seen in a trauma center able to do the most complete evaluation and treatment. A medical helicopter may be the best way to get the patient to the hospital.",1,C
17,18372539,"The airway obstruction concerns any situation that clogs partially or totally the normal pulmonary ventilation. In this way, it is an absolute emergency which in case of not being solved leads to death in afew minutes. One of the most common airway obstructions is the one that results from an extrinsic cause to the airway--food, blood or vomit. Any solid object can work as a foreign body and cause an airway obstruction--mechanical obstruction. Evaluation and control of the airway are carried out through quick and simple procedures. Initially there is no need for any equipment, being just enough the application of manual techniques for control and disobliteration. Interscapular claps, the Heimlich Manoeuvre and the Thoracic Compressions, are manual techniques used in the disobliteration of the respiratory tract due to a solid body.","The airway blockage has to do with any situation that partially or totally clogs normal breathing. In this way, it is an emergency that if not solved leads to death in a few minutes. One of the most common airway blockages is the one due to an outside cause--food, blood or vomit. Any solid object can be an object that shouldn't be swallowed and block an airway--mechanical obstruction. Assessing and managing the airway are done through quick and simple procedures. At the beginning, there is no need for any equipment, with techniques by hand being enough for managing the airway and removing the blockage. Backslaps, the Heimlich Maneuver and the Thoracic Compressions are techniques done using the hands to remove a solid blockage from the airways.",1,C
17,460879,"The tongue is the most common cause of upper airway obstruction, a situation seen most often in patients who are comatose or who have suffered cardiopulmonary arrest. Other common causes of upper airway obstruction include edema of the oropharynx and larynx, trauma, foreign body, and infection. The management of the patient with upper airway obstruction depends upon the cause of the obstruction, the training and skills of the rescuer, and the availability of adjuncts necessary to perform advanced airway techniques. In most cases, merely positioning the patient or performing one of the three maneuvers designed to elevate the tongue will open the airway of the comatose patient or the victim of cardiac arrest. In patients with suspected foreign body obstruction, abdominal or chest compression should be performed immediately, with digital extraction of the foreign body reserved for those in whom these maneuvers are unsuccessful. Most patients with obstruction secondary to edema, trauma, or infection can be managed initially with orotracheal or nasotracheal intubation. Intubation should be attempted prior to surgical management of the airway in most cases of upper airway obstruction. Occasionally, however, cricothyroidotomy or tracheostomy is necessary to establish an airway. The choice of technique depends primarily on the experience and skills of the rescuing physician or paramedic. In most cases, cricothyroidotomy is technically more simple and more easily performed than tracheotomy, especially for the physician who has not been trained in surgery or otolaryngology and for the nonphysician rescuer. No matter what the method employed in establishing an airway in a patient with upper airway obstruction, it must be performed quickly and a source of ventilation provided for the patient once the airway has been secured.","The tongue is the most common cause of blocked upper airways, seen most often in people in comas or cardiac arrest (abrupt heart stop). Other common causes of blocked upper airways include swelling of the middle part of the throat and voice box, injury, objects that shouldn’t be swallowed, and infection. Treatment of the patient with blocked upper airways depends on the cause of the blockage, the training and skills of the rescuer, and the availability of additional treatments needed for advanced airway methods. In most cases, simply positioning the patient or doing one of the three maneuvers to raise the tongue will open the airway of the patient in a coma or cardiac arrest. In people thought to have swallowed an object that should not be swallowed, stomach or chest compression should be done immediately, with removing the object with the fingers used only when these maneuvers do not work. Most people with blocked airways that occur due to swelling, injury, or infection can be treated first without breathing tubes through the mouth or nose. Breathing tubes should be used before surgery in most instances of blocked upper airways. Sometimes, however, surgery to cut a hole in the neck is needed to open the airway. The experience and skills of the rescuing doctor or paramedic mostly determines the approach. Usually, a surgery to cut a hole in the neck is simpler and easier to do than opening the windpipe, especially for a doctor who does not have surgery training or the rescuer who is not a doctor. Regardless of the method used to open the airway in a patient with blocked upper airways, it must be done quickly, and air must be supplied to the person once the airway is open.",1,C
17,460879,"The tongue is the most common cause of upper airway obstruction, a situation seen most often in patients who are comatose or who have suffered cardiopulmonary arrest. Other common causes of upper airway obstruction include edema of the oropharynx and larynx, trauma, foreign body, and infection. The management of the patient with upper airway obstruction depends upon the cause of the obstruction, the training and skills of the rescuer, and the availability of adjuncts necessary to perform advanced airway techniques. In most cases, merely positioning the patient or performing one of the three maneuvers designed to elevate the tongue will open the airway of the comatose patient or the victim of cardiac arrest. In patients with suspected foreign body obstruction, abdominal or chest compression should be performed immediately, with digital extraction of the foreign body reserved for those in whom these maneuvers are unsuccessful. Most patients with obstruction secondary to edema, trauma, or infection can be managed initially with orotracheal or nasotracheal intubation. Intubation should be attempted prior to surgical management of the airway in most cases of upper airway obstruction. Occasionally, however, cricothyroidotomy or tracheostomy is necessary to establish an airway. The choice of technique depends primarily on the experience and skills of the rescuing physician or paramedic. In most cases, cricothyroidotomy is technically more simple and more easily performed than tracheotomy, especially for the physician who has not been trained in surgery or otolaryngology and for the nonphysician rescuer. No matter what the method employed in establishing an airway in a patient with upper airway obstruction, it must be performed quickly and a source of ventilation provided for the patient once the airway has been secured.","The tongue is the most common cause of airway blockage, especially in comatose patients or those suffering cardiac arrest. Other common causes of airway blockage include swelling from trapped fluid of the airway, trauma, something stuck in the airway, and infection. Treating patients with airway blockage depends on its cause, the training and skills of the rescuer, and available airway devices needed for complex procedures. Mostly, just arranging the patient or using one of three methods to elevate the tongue will open the airway of the comatose patient or person with cardiac arrest. In patients with something stuck in the airway, abdominal or chest squeezing should be done quickly, with removal by fingers when squeezing is unsuccessful. Most with blockage due to swelling from trapped fluid, trauma, or infection can be treated initially with breathing tubes. Breathing tubes should be used before surgery of the airway in most cases of airway blockage. Sometimes, however, airway surgery is needed to create an airway. The chosen procedure depends largely on the experience and skills of the rescuer. Mostly, surgery to add a tube to a specific airway location is simpler and easier than surgery to cut a hole in the windpipe, especially for rescuers and those not specialized or trained in surgery. Despite the method used to create an airway for those with airway blockage, it must be done quickly and fresh air provided for the patient afterward.",2,C
17,1557731,"Ensuring free passage of air is the first priority in emergency care of patients. Removing obstruction to softtissue, dislodging obstructing foreign bodies and positioning the patient correctly usually secure open airways and respiration in trauma patients. If respiration has ceased, oroendotracheal intubation is necessary and should be performed by trained personnel. Correct control of airways may reduce morbidity and mortality. The author discusses the practical aspects of control of airways and unobstructed respiration.","The most important thing in the emergency care of patients is making sure air passes freely. Removing blocked tissue, loosening objects that shouldn't be swallowed and putting the patient in the right position usually open airways and allow breathing in injured people. If breathing has stopped, a breathing tube is needed and should be given by a trained professional. Correct control of airways may decrease illness and death. The author discusses control of airways and normal breathing in real life situations.",1,C
17,3730995,"Definitive management of the unconscious choking victim, whether in hospital or in the field, should include removal of the foreign body by instrumentation under direct visualization. However, there is debate as to the best management of the conscious victim with an obstructed upper airway and of the unconscious victim for whom such definitive instrumentation is not available. Which artificial-cough maneuver is the most eficacious in clearing the obstructed airway? Which maneuver should be used first? What are the complications of the various techniques? Is any maneuver dangerous or deleterious? To date there is no consensus on any of these issues. There are significant discrepancies in the literature as to which technique produces the highest intrathoracic pressures and airflow rates. Most of the data seem to support the conclusion that blows to the back generate the highest intrathoracic pressure, whereas chest or abdominal thrust produces the highest airflow rate. Clinically, all the maneuvers are somewhat efficacious in clearing the obstructed airway when used alone; however, each maneuver seems to be substantially more efficacious when used in combination with another maneuver. Also, the results appear to be more successful when pressure is applied as a series of jolts rather than applied steadily.","The best treatment for an unconscious person choking, whether in the hospital or by first responders, should include removal of the object that should not be swallowed with a medical tool while clearly seeing the object. However, experts do not agree about the best way to treat a conscious person with partial or complete blockage of the airway and the unconscious person for whom medical tools are not available. Which way to force air out of a person's lungs works the best to clear the blocked airway? Which way should be used first? What are the complications of the different ways? Is any way dangerous or harmful? To date, there is no agreement on any of these issues. Studies disagree as to which way creates the highest pressure in the chest that forces air out of the lungs and highest airflow rates. Most studies support the idea that blows to the back make the highest pressure in the chest that forces air out of the lungs, while stomach thrusts make the highest airflow rate. All the ways work somewhat to clear the blocked airway on their own; however, each way seems to work much better when combined with another way. Also, pressure applied as a series of jolts seems to work better than steadily applied pressure.",1,C
17,30565220,"Background: Oral poisoning is a major cause of mortality and disability worldwide, with estimates of over 100,000 deaths due to unintentional poisoning each year and an overrepresentation of children below five years of age. Any effective intervention that laypeople can apply to limit or delay uptake or to evacuate, dilute or neutralize the poison before professional help arrives may limit toxicity and save lives. Objectives: To assess the effects of pre-hospital interventions (alone or in combination) for treating acute oral poisoning, available to and feasible for laypeople before the arrival of professional help. Authors' conclusions: The studies included in this review provided mostly low- or very low-certainty evidence about the use of first aid interventions for acute oral poisoning. A key limitation was the fact that only one included study actually took place in a pre-hospital setting, which undermines our confidence in the applicability of these results to this setting. Thus, the amount of evidence collected was insufficient to draw any conclusions.","Poisoning caused by swallowing a toxic substance is a big cause of death and disability worldwide, with over 100,000 deaths due to accidental poisoning each year largely from children younger than five years. Anything that bystanders can do to reduce or delay how much is swallowed or to make ineffective, make weaker, or remove the poison before professional help arrives may limit the harm and save lives. Our objectives were to measure the effects of pre-hospital treatments (alone or in combination) for treating poisoning caused by swallowing a toxic substance that are available to and doable for bystanders before professional help arrives. We concluded that the studies we looked at had mostly unreliable findings about the use of first aid treatments for poisoning caused by swallowing a toxic substance once or many times over a short period of time. An important limitation was that only one study happened in a pre-hospital setting, which does not make us confident that these results apply to this setting. Therefore, there were not enough results to draw any conclusions.",1,C
17,30565220,"Background: Oral poisoning is a major cause of mortality and disability worldwide, with estimates of over 100,000 deaths due to unintentional poisoning each year and an overrepresentation of children below five years of age. Any effective intervention that laypeople can apply to limit or delay uptake or to evacuate, dilute or neutralize the poison before professional help arrives may limit toxicity and save lives. Objectives: To assess the effects of pre-hospital interventions (alone or in combination) for treating acute oral poisoning, available to and feasible for laypeople before the arrival of professional help. Authors' conclusions: The studies included in this review provided mostly low- or very low-certainty evidence about the use of first aid interventions for acute oral poisoning. A key limitation was the fact that only one included study actually took place in a pre-hospital setting, which undermines our confidence in the applicability of these results to this setting. Thus, the amount of evidence collected was insufficient to draw any conclusions.","Oral poisoning is a major cause of death and disability worldwide, with over 100,000 deaths from unintended poisoning yearly and noticeably by chlidren younger than five years. Any useful treatment that anyone can use to prevent intake or control the poison before help arrives may limit the poisoning and save lives. The objective is to measure the effects of pre-hospital treatments (alone or in combination) for immediate oral poisoning, available to anyone before professional help arrives. The studies in this review provided mostly unreliable evidence about using first aid treatments for immediate oral poisoning. An key limitation is that only one study actually occured in a pre-hospital setting, which weakens our faith in the usefulness of these results to this setting. Thus, the evidence is not enough to make any conclusions.",2,C
17,30784327,"Introduction: In acute oral poisoning, any first aid intervention that limits or delays the uptake of the ingested substance, and which can be performed by bystanders as first responders, might assist in reducing morbidity if a toxic substance has been ingested. The current recommendation by the International Federation of Red Cross/Red Crescent Societies is to place a victim in the left lateral decubitus position. Conclusions: The identified studies provide evidence of very low certainty. However, based on the evidence that the left lateral decubitus position may be effective in decreasing the absorption of several drugs, the simplicity of the intervention and the generally low perceived risk of this intervention, the recommendation of the first aid guidelines of the International Federation of Red Cross and Red Crescent Societies can remain unchanged.","In poisoning caused by swallowing a toxic substance once or many times over a short period of time, any first aid assistance that reduces or delays how much is swallowed, and which ones bystanders can do, might help in reducing illness. The International Federation of Red Cross/Red Crescent Societies recommends a victim be placed on the person's left side. We conclude that the studies provide unreliable results. However, based on studies that show placing a person on the person's left side may decrease absorption of many drugs, and the ease and low risk of doing so, the recommendation of the International Federation of Red Cross and Red Crescent Societies stands.",1,C
17,1536482,"Study objectives: Many factors influence the rate of gastric emptying and therefore the rate of drug absorption in the orally poisoned patient. Limited studies have evaluated the effect of body position on the rate of gastric emptying of radiographically marked foods and contrast media, but effects on drug absorption have not been studied previously. Our hypothesis was that body position would have an effect on the rate of drug absorption in an oral overdose model. Design: A blinded, within-subjects (crossover) design. Participants: Six male and six female healthy, adult volunteer subjects with no concurrent drug use or medications affecting gastrointestinal function. Interventions: Five body positions commonly used in prehospital and emergency department settings were examined: left lateral decubitus, right lateral decubitus, supine, prone, and sitting. All were performed by all subjects in random order with a three-day washout phase between trials. To simulate an acute overdose, fasted subjects ingested 80 mg/kg acetaminophen in the form of 160-mg pediatric tablets. Each subject then remained in the body position for that trial for two hours. Acetaminophen levels were obtained at 15-minute intervals, and a two-hour area under the curve (AUC) was calculated for each subject trial to represent total drug absorption during each study period. Investigators were blinded to all results until all trials were completed. Measurements and main results: All subjects completed the study. Group mean drug absorption as represented by two-hour AUC (mg.L.min) was calculated for each body position. AUC for left lateral decubitus (6,006 +/- 2,614) was lowest but did not significantly differ from that for supine (6,649 +/- 2,761). Both were significantly less than those for prone (7,432 +/- 1,809), right lateral decubitus (8,950 +/- 1,405), and sitting (8,608 +/- 1,725) positions (P less than .05 by one-way analysis of variance and follow-up paired t tests). Conclusion: Initial drug absorption as determined by two-hour AUC was lowest in the left lateral decubitus position. Although the difference between the left lateral decubitus and supine positions did not reach statistical significance, both left lateral decubitus and supine were significantly lower than three other common patient body positions tested. Because the left lateral decubitus position has other advantages (eg, prevention of aspiration) in addition to minimizing drug absorption, we recommend that orally poisoned patients be placed in the left lateral decubitus position for prehospital and initial ED management.","Many things determine how fast the stomach empties and therefore how fast a drug is taken in by the body in a person who swallows a toxic substance. Few studies have rated the effect of body position on how fast the stomach empties via a technique to more easily see stomach contents with x-rays. However, effects on how fast a drug is absorbed have not been studied before. We thought that body position would have an effect on how fast a drug is absorbed in a person who swallows a toxic substance. Participants were six male and six female healthy, adult volunteers not taking any drugs or medicine affecting stomach function. We looked at five body positions often used before hospital and emergency room treatment: left side, right side, back, stomach, and sitting. Participants did every position in random order with a 3-day break between trials. To pretend that participants swallowed a toxic substance, participants did not eat prior to taking 80 mg of Tylenol/kg of body weight in the form of 160-mg children's tablets. Each participant then stayed in the body position for that trial for two hours. We measured Tylenol levels every 15 minutes. We calculated how much Tylenol participants absorbed over 2 hours for each trial. Investigators did not know the results until after the trials were done. All participants finished the study. We calculated average amount of drug absorbed for each body position. The average amount of drug absorbed was lowest for the left side, which was similar to the back position. Absorption for the left side and back were less than for stomach, right side, and sitting positions. We concluded that drug absorption was lowest for the left side position. Although the difference between the left side and back positions did not differ significantly, both the left side and back positions were significantly lower than the three other positions tested. Because the left side position is better for other reasons (e.g., material entering the airway or lungs by accident) than decreasing how much drug is absorbed, we suggest that people who swallow a toxic substance be put in the left side position before going to the hospital and emergency room.",1,C
17,30285362,"Abdominal thrusts or the Heimlich maneuver is a first-aid procedure used to treat upper airway obstruction caused by a foreign body. This skill is commonly taught during basic life support (BLS) and advanced cardiac life support (ACLS) classes, but it never receives as much attention as chest compressions and rescue breaths do. The abdominal thrust maneuver can be performed in both children and adults via different techniques. In the 1960s, choking on food, toys, and other objects was the sixth leading cause of accidental death in the United States. Slapping individuals on the back was the main response and was frequently found to be ineffective, at times even lodging the object further down. The Heimlich maneuver was initially introduced in 1974 by Dr. Henry Heimlich after proving his theory that the reserve of air in the lung could serve to dislodge objects from the esophagus by quick upwards thrust under the ribcage. The medical community of the time did not embrace the maneuver right away. The American Red Cross (ARC) and the American Heart Association (AHA) continued to promoted backslaps for ten years after the introduction of the Heimlich maneuver. Today, the Heimlich maneuver is accepted and taught during BLS and ACLS for conscious adults, but backslaps are still a recommendation for infants, and chest compressions are recommended for unconscious patients. Furthermore, different techniques of the maneuver have been developed with conflicting effectiveness results.","Stomach thrusts or the Heimlich maneuver is the first-aid procedure used to treat partial or complete blockage of the upper airway from an object that shouldn't be swallowed. This skill is commonly taught during basic life support and advanced heart life support classes, but it never gets as much attention as chest compressions and rescue breaths do. The stomach thrust maneuver can be done in both children and adults using different ways. In the 1960s, choking on food, toys, and other objects was the sixth leading cause of accidental death in the United States. Slapping a person on the back was the most common response and was often found to not work, sometimes even pushing the object further down. Dr. Henry Heimlich introduced the Heimlich maneuver in 1974 after proving his idea that stored air in the lung could push objects out of the throat by fast upward thrusts under the ribcage. The medical community then did not accept the maneuver right away. The American Red Cross (ARC) and the American Heart Association (AHA) pushed backslaps for 10 years after the Heimlich maneuver was introduced. Today, the Heimlich maneuver is accepted and taught during basic life support and advanced heart life support classes for conscious adults, but backslaps are still recommended for infants. Chest compressions are recommended for unconscious people. Furthermore, people have come up with different ways of doing the maneuver with mixed results.",1,C
17,30285362,"Abdominal thrusts or the Heimlich maneuver is a first-aid procedure used to treat upper airway obstruction caused by a foreign body. This skill is commonly taught during basic life support (BLS) and advanced cardiac life support (ACLS) classes, but it never receives as much attention as chest compressions and rescue breaths do. The abdominal thrust maneuver can be performed in both children and adults via different techniques. In the 1960s, choking on food, toys, and other objects was the sixth leading cause of accidental death in the United States. Slapping individuals on the back was the main response and was frequently found to be ineffective, at times even lodging the object further down. The Heimlich maneuver was initially introduced in 1974 by Dr. Henry Heimlich after proving his theory that the reserve of air in the lung could serve to dislodge objects from the esophagus by quick upwards thrust under the ribcage. The medical community of the time did not embrace the maneuver right away. The American Red Cross (ARC) and the American Heart Association (AHA) continued to promoted backslaps for ten years after the introduction of the Heimlich maneuver. Today, the Heimlich maneuver is accepted and taught during BLS and ACLS for conscious adults, but backslaps are still a recommendation for infants, and chest compressions are recommended for unconscious patients. Furthermore, different techniques of the maneuver have been developed with conflicting effectiveness results.","Abdominal thrusts or the Heimlich manuever is a first-aid procedure for airway blockage due to something being stuck in throat. Abdominal thrusts are usually taught during basic life support (BLS) and advanced cardiac, or heart-related, life support (ACLS) classes, but it never gets as much attention as chest squeezes and mouth-to-mouth rescue breaths do. Abdominal thrusts can be used on both children and adults via different ways. In the 1960s, choking on objects was the sixth leading cause of accidental death in the US. Slapping others on the back was the main treatment and found to not be useful, sometimes forcing the object further down. Abdominal thrusts were introduced in 1974 by Dr. Henry Heimlich after proving his theory that air in the lungs could remove objects in the airway by quick upward thrusts under the ribs. The medical groups of the time did not employ the technique right away. The American Red Cross (ARC) and the American Heart Association (AHA) still used backslaps for ten years after the introduction of abdominal thrusts. Today, abdominal thrusts are accepted and taught during basic and advanced cardiac, or heart-related, life support classes. Still, backslaps are approved for infants and chest compressions for unconscious patients. Also, different techniques of abdominal thrusts have been created with conflicting success.",2,C
26,34368314,"Background: As a highly contagious disease, coronavirus disease 2019 (COVID-19) is wreaking havoc around the world due to continuous spread among close contacts mainly via droplets, aerosols, contaminated hands or surfaces. Therefore, centralized isolation of close contacts and suspected patients is an important measure to prevent the transmission of COVID-19. At present, the quarantine duration in most countries is 14 d due to the fact that the incubation period of severe acute respiratory syndrome coronavirus type 2 (SARS-CoV-2) is usually identified as 1-14 d with median estimate of 4-7.5 d. Since COVID-19 patients in the incubation period are also contagious, cases with an incubation period of more than 14 d need to be evaluated. Case summary: A 70-year-old male patient was admitted to the Department of Respiratory Medicine of The First Affiliated Hospital of Harbin Medical University on April 5 due to a cough with sputum and shortness of breath. On April 10, the patient was transferred to the Fever Clinic for further treatment due to close contact to one confirmed COVID-19 patient in the same room. During the period from April 10 to May 6, nucleic acid and antibodies to SARS-CoV-2 were tested 7 and 4 times, respectively, all of which were negative. On May 7, the patient developed fever with a maximum temperature of 39℃, and his respiratory difficulties had deteriorated. The results of nucleic acid and antibody detection of SARS-CoV-2 were positive. On May 8, the nucleic acid and antibody detection of SARS-CoV-2 by Heilongjiang Provincial Center for Disease Control were also positive, and the patient was diagnosed with COVID-19 and reported to the Chinese Center for Disease Control and Prevention. Conclusion: This case highlights the importance of the SARS-CoV-2 incubation period. Further epidemiological investigations and clinical observations are urgently needed to identify the optimal incubation period of SARS-CoV-2 and formulate rational and evidence-based quarantine policies for COVID-19 accordingly.","Coronavirus disease 2019, also known as COVID-19, is a highly contagious, viral, breathing-related disease that has caused world-wide distress. Continual spread of COVID-19 occurs between people in close contact with one another through coughing, sneezing, breathing, talking, and touching dirty hands or surfaces. To prevent further spread of COVID-19, a period of quarantine (isolation) is recommended for those suspected of having COVID-19 and/or those who believe they have come in contact with a COVID-19-infected person. In most countries, the recommended quarantine duration is 14 days. This is because the incubation period, or the time between exposure and the first signs of illness, of respiratory or breathing-related illnesses is normally between 4 to 7.5 days. However, potential COVID-19 patients are still contagious during the incubation period. Cases with incubation periods longer than 14 days need further evaluation by doctors. For example, a 70-year-old man was admitted to the hospital on April 5th, reporting a cough, the spitting up of saliva and mucus, and shortness of breath. On April 10th, the man was transferred to the Fever Clinic within the hospital for additional treatment as he had experienced close contact with a confirmed COVID-19 patient. From April 10th to May 6th, the man was tested for COVID-19 several times. All tests returned negative, detecting no COVID-19. On May 7th, the man developed a severe fever, and his breathing issues become worse. The man was tested again for COVID-19 and was positive, detecting COVID-19. On May 8th, a second COVID-19 test was conducted by the Heilongjiang Provincial Center for Disease Control and was returned positive. The man was diagnosed with COVID-19, and his health status was recorded by the Chinese Center for Disease Control and Prevention. This example shows the importance of the COVID-19 incubation period. Additional research is needed to better define the incubation period of COVID-19 to create quarantine measures that best protect human health.",1,C
26,32150748,"Background: A novel human coronavirus, severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), was identified in China in December 2019. There is limited support for many of its key epidemiologic features, including the incubation period for clinical disease (coronavirus disease 2019 [COVID-19]), which has important implications for surveillance and control activities. Objective: To estimate the length of the incubation period of COVID-19 and describe its public health implications. Design: Pooled analysis of confirmed COVID-19 cases reported between 4 January 2020 and 24 February 2020. Setting: News reports and press releases from 50 provinces, regions, and countries outside Wuhan, Hubei province, China. Participants: Persons with confirmed SARS-CoV-2 infection outside Hubei province, China. Measurements: Patient demographic characteristics and dates and times of possible exposure, symptom onset, fever onset, and hospitalization. Results: There were 181 confirmed cases with identifiable exposure and symptom onset windows to estimate the incubation period of COVID-19. The median incubation period was estimated to be 5.1 days (95% CI, 4.5 to 5.8 days), and 97.5% of those who develop symptoms will do so within 11.5 days (CI, 8.2 to 15.6 days) of infection. These estimates imply that, under conservative assumptions, 101 out of every 10 000 cases (99th percentile, 482) will develop symptoms after 14 days of active monitoring or quarantine. Limitation: Publicly reported cases may overrepresent severe cases, the incubation period for which may differ from that of mild cases. Conclusion: This work provides additional evidence for a median incubation period for COVID-19 of approximately 5 days, similar to SARS. Our results support current proposals for the length of quarantine or active monitoring of persons potentially exposed to SARS-CoV-2, although longer monitoring periods might be justified in extreme cases.","In December 2019, a new human coronavirus that affects the breathing or respiratory system, known as COVID-19, was identified in China. Little is known about how COVID-19 impacts human health, including how long the disease's incubation period (the time between initial exposure and the first signs of illness) lasts. Knowing the incubation period of the disease is important for preventing further spread. The goal of this paper was to estimate the length of the COVID-19 incubation period and how it impacts the public's health. To reach this goal, the authors reviewed confirmed COVID-19 cases that were reported between January 4th to February 24th in 2020. The information was gathered from news reports and press releases from over 50 locations outside of Wuhan city within the Hubei province of China. Only people with confirmed COVID-19 infection outside of the Hubei province of China were of interest for the evaluation. Patient characteristics (e.g. age, ethnicity) and specific health measurements (e.g. dates of exposure, start of symptoms) were reviewed. In total, 181 confirmed COVID-19 cases with definable exposure points and start of symptoms were evaluated for incubation period estimation. The average incubation period was estimated to be 5.1 days. Overall, the majority of people will develop symptoms within 11.5 days after time of infection. These estimates imply that a portion of the population (101 out of every 10,000 COVID-19 patients) will show symptoms after 14 days of quarantine. Because only cases from new outlets and press releases were evaluated, the estimations may be based on more severe COVID-19 cases. Mild cases, which may go unreported, may have a different incubation period. Even though the data for mild COVID-19 cases may differ, this estimate provides support for an average incubation period for COVID-19 of 5 days. This estimation may help in the creation of appropriate quarantine measures for persons potentially exposed to COVID-19.",1,C
26,32449789,"Recurrence of positive SARS CoV‐2 PCR has been described in patients discharged from hospital after 2 consecutive negative PCR. We discuss possible explanations including false negative, reactivation and re‐infection and propose different strategy to solve this issue. Prolonged SARS‐CoV‐2 RNA shedding and recurrence of viral RNA shedding in asymptomatic patients remain unknown. Transmission of SARS‐CoV‐2 by asymptomatic carriers had been documented. Considering the significance of this ongoing global public health emergency, it is necessary to carry out large studies to better understand the issue of potential SARS‐ CoV‐2 recurrence in COVID‐19 patients.","Patients previously discharged from hospitals with negative (or undetected) COVID-19 tests have been seen to later test positive (detecting COVID-19). This paper aims to explain possible reasons for these events. These reasons include false or incorrect negative test results, the virus transitioning from a sleeping to an active phase within the patient, or a patient being exposed and infected after leaving the hospital. The reasons why people with no COVID-19 related symptoms test positive for the virus are unknown. However, it is known that people with no COVID-19 related symptoms can still spread the virus to others. Due to the large scale impact the COVID-19 pandemic is having on the world, it is important to conduct research to better understand how previous COVID-19 patients can become ill with the virus more than once.",1,C
26,32449789,"Recurrence of positive SARS CoV‐2 PCR has been described in patients discharged from hospital after 2 consecutive negative PCR. We discuss possible explanations including false negative, reactivation and re‐infection and propose different strategy to solve this issue. Prolonged SARS‐CoV‐2 RNA shedding and recurrence of viral RNA shedding in asymptomatic patients remain unknown. Transmission of SARS‐CoV‐2 by asymptomatic carriers had been documented. Considering the significance of this ongoing global public health emergency, it is necessary to carry out large studies to better understand the issue of potential SARS‐ CoV‐2 recurrence in COVID‐19 patients.","Patients released from a hospital after 2 consecutive test results detecting no SARS-CoV-2 (a viral breathing-related illness) have shown reappearance of SARS CoV-2 in test results. We discuss possible explanations including inaccurate results, reactivation, and re-infection. We also propose a new strategy to solve the issue. Prolonged and recurring virus release and emission from patients without symptoms is unknown. Transmission of SARS-CoV-2 by carriers without symptoms had been documented. Considering this ongoing global public health emergency, large studies are needed to better understand the issue of possible SARS-CoV-2 reapparance in infected patients.",2,C
26,32941052,"Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), the etiologic agent of coronavirus disease 2019 (COVID-19), has spread globally in a few short months. Substantial evidence now supports preliminary conclusions about transmission that can inform rational, evidence-based policies and reduce misinformation on this critical topic. This article presents a comprehensive review of the evidence on transmission of this virus. Although several experimental studies have cultured live virus from aerosols and surfaces hours after inoculation, the real-world studies that detect viral RNA in the environment report very low levels, and few have isolated viable virus. Strong evidence from case and cluster reports indicates that respiratory transmission is dominant, with proximity and ventilation being key determinants of transmission risk. In the few cases where direct contact or fomite transmission is presumed, respiratory transmission has not been completely excluded. Infectiousness peaks around a day before symptom onset and declines within a week of symptom onset, and no late linked transmissions (after a patient has had symptoms for about a week) have been documented. The virus has heterogeneous transmission dynamics: Most persons do not transmit virus, whereas some cause many secondary cases in transmission clusters called ""superspreading events."" Evidence-based policies and practices should incorporate the accumulating knowledge about transmission of SARS-CoV-2 to help educate the public and slow the spread of this virus.","Severe acute respiratory syndrome coronavirus 2, also known as SARS-CoV-2, is the cause of coronavirus disease 2019 or COVID-19 (a viral lung infection). Within a short period of time, SARS-CoV-2 has spread around the world. A significant amount of strong scientific evidence now backs up the initial thoughts on how COVID spreads from one person to another. This can improve current policies surrounding COVID-19 health safety rules and prevent the spread of false information. This paper offers a thorough review of the scientific reports concerning the spread of COVID-19. Several laboratory studies have been able to grow live COVID-19 viruses from the air and surfaces, even several hours after the virus was placed there. However, real-world studies that detect COVID-19 genetic material in the environment report very low levels. Few have been able to grow the virus from these samples. Strong evidence from cases and outbreaks shows that COVID-19 mostly spreads through the airways (lungs, throat, nose, and mouth). Distance and ventilation are key factors for risk of spreading COVID-19. Even in the few cases where scientists think the virus spread through contact with people or surfaces, spreading through the airways has not been ruled out. The rate that a person spreads the virus is highest a day before symptoms appear and declines within a week of symptoms appearing. The spread of the COVID-19 virus is different for each person. Most people infected do not spread COVID-19. However, some people infected with COVID-19 can cause many infections in groups, known as ""superspreading events."" Rules and procedures around COVID-19 should include the growing amount of scientific evidence about how it spreads. This will help educate the public and slow the spread of the virus.",1,C
26,32473952,"Since December 2019, severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has infected over four million people worldwide. There are multiple reports of prolonged viral shedding in people infected with SARS-CoV-2 but the presence of viral RNA on a test does not necessarily correlate with infectivity. The duration of quarantine required after clinical recovery to definitively prevent transmission is therefore uncertain. In addition, asymptomatic and presymptomatic transmission may occur, and infectivity may be highest early after onset of symptoms, meaning that contact tracing, isolation of exposed individuals and social distancing are essential public health measures to prevent further spread. This review aimed to summarise the evidence around viral shedding vs infectivity of SARS-CoV-2.","Since December 2019, severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), the causing agent of COVID-19, has infected over four million people around the world. The are several reports of people infected with COVID-19 being active spreaders of the virus for extended periods of time. However, the results of COVID-19 tests do not always correlate to the duration of time a person can spread the virus. Meaning a person can test negative but may be actively giving the virus to others unknowingly. Because of this, the duration of time needed before a previously COVID-19 infected person is no longer able to infect others is not known. People without symptoms can spread the virus. COVID-19 can also be spread before people begin to show symptoms. The spread of the virus may actually occur the most right after symptoms show. This means the tracing of potential exposures, isolation of exposed people, and social distancing are needed to improve public health and reduce virus spread. The goal of this paper was summarize the scientific research around the spreading of COVID-19.",1,C
26,33029620,"Defining the duration of infectivity of Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) has major implications for public health and infection control practice in healthcare facilities. Early in the pandemic, most hospitals required 2 negative RT-PCR tests before discontinuing isolation in patients with Covid-19. Many patients, however, have persistently positive RT-PCR tests for weeks to months following clinical recovery, and multiple studies now indicate that these generally do not reflect replication-competent virus. SARS-CoV-2 appears to be most contagious around the time of symptom onset, and infectivity rapidly decreases thereafter to near-zero after about 10 days in mild-moderately ill patients and 15 days in severely-critically ill and immunocompromised patients. The longest interval associated with replication-competent virus thus far is 20 days from symptom onset. This review summarizes evidence-to-date on the duration of infectivity of SARS-CoV-2, and how this has informed evolving public health recommendations on when it is safe to discontinue isolation precautions.","Defining the period of time someone can spread Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2), also known as COVID-19, to others can positively impact public health and prevent infection spreading within healthcare facilities. COVID-19 is a harmful, breathing-related, viral disease. Early in the pandemic, most hospitals required two negative (or undetected) COVID-19 tests before COVID-19 infected patients could come out of quarantine (isolation). However, several COVID-19 patients continually test positive (with COVID-19 detected) after clinically recovering from the virus. Based on several reports, this is not believed to be the norm for the replication-competent virus, or a virus that is able to reproduce itself in one person and infect other people. The virus appears to be contagious (easily spread) around the time symptoms first appear. The ability of the virus to spread decreases as symptoms progress. The ability of the virus to spread becomes near-zero around 10 days in mild to moderately ill patients. The ability of the virus to spread becomes near-zero around 15 days in severely to critically ill and immunocompromised (those with decreased immune system function) patients. The longest documented duration between symptom onset and viral spread is 20 days. This review summarizes the most recent evidence on the length of time COVID-19 is able to spread from one patient to another. Additionally, this paper states how this knowledge has helped create improved COVID-19 mandates or rules on quarantine lengths.",1,C
26,33029620,"Defining the duration of infectivity of Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) has major implications for public health and infection control practice in healthcare facilities. Early in the pandemic, most hospitals required 2 negative RT-PCR tests before discontinuing isolation in patients with Covid-19. Many patients, however, have persistently positive RT-PCR tests for weeks to months following clinical recovery, and multiple studies now indicate that these generally do not reflect replication-competent virus. SARS-CoV-2 appears to be most contagious around the time of symptom onset, and infectivity rapidly decreases thereafter to near-zero after about 10 days in mild-moderately ill patients and 15 days in severely-critically ill and immunocompromised patients. The longest interval associated with replication-competent virus thus far is 20 days from symptom onset. This review summarizes evidence-to-date on the duration of infectivity of SARS-CoV-2, and how this has informed evolving public health recommendations on when it is safe to discontinue isolation precautions.","Defining the duration of infectvity of Severe Acute Respiratory Syndrome Coronovirus 2 (SARS-CoV-2), which is a viral breathing-related illness, can influence public health and infection control practice for healthcare. Early in the pandemic, most hospitals needed 2 test results detecting no disease before releasing patients with Covid-19, or SARS-CoV-2. Many patients still have test results detecting the disease for weeks to months after recovery. Studies note that these results generally do not reflect contagious viruses. SARS-CoV-2 seems the most contagious around the time of symptom onset. The infectivity or ability to spread the virus quickly decreases to near-zero after about 10 days in mildly ill patients and 15 days in severely ill and immunocompromised patients. The longest interval for contagious viruses so far is 20 days from symptom onset. This review summarizes current evidence on the duration of infectivity of SARS-CoV-2, and how this affects new public health recommendation for releasing people in isolation.",2,C
26,32771633,"Objectives: The distribution of the transmission onset of COVID-19 relative to the symptom onset is a key parameter for infection control. It is often not easy to study the transmission onset time, as it is difficult to know who infected whom exactly when. Methods: We inferred transmission onset time from 72 infector-infectee pairs in South Korea, either with known or inferred contact dates, utilizing the incubation period. Combining this data with known information of the infector's symptom onset, we could generate the transmission onset distribution of COVID-19, using Bayesian methods. Serial interval distribution could be automatically estimated from our data. Results: We estimated the median transmission onset to be 1.31 days (standard deviation, 2.64 days) after symptom onset with a peak at 0.72 days before symptom onset. The pre-symptomatic transmission proportion was 37% (95% credible interval [CI], 16-52%). The median incubation period was estimated to be 2.87 days (95% CI, 2.33-3.50 days), and the median serial interval to be 3.56 days (95% CI, 2.72-4.44 days). Conclusions: Considering that the transmission onset distribution peaked with the symptom onset and the pre-symptomatic transmission proportion is substantial, the usual preventive measures might be too late to prevent SARS-CoV-2 transmission.","Understanding when a potential COVID-19 patient is contagious in relation to when they first show symptoms is important to help reduce the spread of the virus. Is it not easy to determine COVID-19 transmission duration, or how long the virus was spread, as it is difficult to trace who had contact with who. The goal of this paper was to determine COVID-19 transmission by evaluating 72 infector-infected pairs from South Korea, with known or estimated contact dates, by reviewing the pairs' incubation period. The incubation period is a time between the date of exposure and the first day of virus related symptoms. Using this data in comparison with the confirmed date of the infector's first day of symptoms, the authors aim to estimate when an infected person first becomes contagious (able to spread the virus). The time between the date of infection and the time a person is capable of spreading COVID-19 to others could be predicted. The estimated average time of COVID-19 spreading to others after a patient first shows symptoms was 1.31 days. However, the peak of transmissibility was 0.72 days before symptoms appear. The amount of cases that displayed patients who could spread the virus before they started showing symptoms accounted for 37% of the 72 reviewed cases. The average time between COVID-19 entering a person's body (time of exposure) and the onset of symptoms was 2.87 days. This paper has shown that ability of a person to spread COVID-19 was highest at the time symptoms first occurred. Additionally, it has been demonstrated a large portion of the population is able to spread the virus even before symptoms occur. Because of this, the usual measures to prevent the spread of COVID-19 may not be enough to improve public health.",1,C
26,33049331,"Objectives: To summarise the evidence on the duration of infectiousness of individuals in whom SARS-CoV-2 ribonucleic acid is detected. Methods: A rapid review was undertaken in PubMed, Europe PubMed Central and EMBASE from 1 January 2020 to 26 August 2020. Results: We identified 15 relevant studies, including 13 virus culture studies and 2 contact tracing studies. For 5 virus culture studies, the last day on which SARS-CoV-2 was isolated occurred within 10 days of symptom onset. For another 5 studies, SARS-CoV-2 was isolated beyond day 10 for approximately 3% of included patients. The remaining 3 virus culture studies included patients with severe or critical disease; SARS-CoV-2 was isolated up to day 32 in one study. Two studies identified immunocompromised patients from whom SARS-CoV-2 was isolated for up to 20 days. Both contact tracing studies, when close contacts were first exposed greater than 5 days after symptom onset in the index case, found no evidence of laboratory-confirmed onward transmission of SARS-CoV-2. Conclusion: COVID-19 patients with mild-to-moderate illness are highly unlikely to be infectious beyond 10 days of symptoms. However, evidence from a limited number of studies indicates that patients with severe-to-critical illness or who are immunocompromised, may shed infectious virus for longer.","The goal of this paper was to summarize scientific reports detailing the amount of time someone positive for (or with) COVID-19 (a viral, breathing-related disease) can infect others. To do this, the authors reviewed papers published in public databases (e.g. PubMed, Europe PubMed Central, EMBASE) between the dates of January 1, 2020 to August 26, 2020. Fifteen studies were identified for review. Thirteen reports focused on COVID-19 that was grown within a laboratory (in culture) from human biological sampling. Two studies followed contact tracing between humans. For 5 viral culture studies, the last day that COVID-19 was able to be identified in biological samples was 10 days before symptoms occurred. For another 5 culture studies, COVID-19 was identified in biological samples past day 10. The remaining 3 virus culture studies evaluated patients with severe or critical COVID-19 illness. COVID-19 was isolated up to 32 days in one of these studies. Two studies identified immunocompromised patients (patients with decreased immune function) from whom COIVD-19 was able to be isolated from for up to 20 days. For both contact tracing or spreading studies, when exposure occurred more than five days after symptoms were apparent, there was no evidence of COVID-19 spreading. The authors concluded that COVID-19 patients with mild to moderate symptoms are unlikely to spread the virus to others beyond 10 days of symptoms. However, studies have shown that patients with severe to critical symptoms, or those who are immunocompromised, may spread the virus for longer periods of time.",1,C
26,33176093,"Background: The efficacy of public health measures to control the transmission of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has not been well studied in young adults. Methods: We investigated SARS-CoV-2 infections among U.S. Marine Corps recruits who underwent a 2-week quarantine at home followed by a second supervised 2-week quarantine at a closed college campus that involved mask wearing, social distancing, and daily temperature and symptom monitoring. Study volunteers were tested for SARS-CoV-2 by means of quantitative polymerase-chain-reaction (qPCR) assay of nares swab specimens obtained between the time of arrival and the second day of supervised quarantine and on days 7 and 14. Recruits who did not volunteer for the study underwent qPCR testing only on day 14, at the end of the quarantine period. We performed phylogenetic analysis of viral genomes obtained from infected study volunteers to identify clusters and to assess the epidemiologic features of infections. Results: A total of 1848 recruits volunteered to participate in the study; within 2 days after arrival on campus, 16 (0.9%) tested positive for SARS-CoV-2, 15 of whom were asymptomatic. An additional 35 participants (1.9%) tested positive on day 7 or on day 14. Five of the 51 participants (9.8%) who tested positive at any time had symptoms in the week before a positive qPCR test. Of the recruits who declined to participate in the study, 26 (1.7%) of the 1554 recruits with available qPCR results tested positive on day 14. No SARS-CoV-2 infections were identified through clinical qPCR testing performed as a result of daily symptom monitoring. Analysis of 36 SARS-CoV-2 genomes obtained from 32 participants revealed six transmission clusters among 18 participants. Epidemiologic analysis supported multiple local transmission events, including transmission between roommates and among recruits within the same platoon. Conclusions: Among Marine Corps recruits, approximately 2% who had previously had negative results for SARS-CoV-2 at the beginning of supervised quarantine, and less than 2% of recruits with unknown previous status, tested positive by day 14. Most recruits who tested positive were asymptomatic, and no infections were detected through daily symptom monitoring. Transmission clusters occurred within platoons.","The strength of current public health measures to prevent the spread of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), or COVID-19 (a viral respiratory disease), has not been well studied in young adults. The authors investigated COVID-19 infections amongst U.S. Marine Corps recruits. These recruits underwent a two-week quarantine )or isolation) within their personal homes before participating in a second two-week quarantine. The second quarantine was supervised at a closed college campus where recruits wore masks, practiced social distancing, and received daily temperature and symptoms monitoring. Participants in the study were tested for COVID-19 by using nose swabs taken between the time of arrival and the second day of supervised quarantine. A second COVID-19 test run with nose swab samples was conducted on days 7 and 14. Marine recruits who did not want to participate in the study only received one COVID-19 test on day 14 (the final day of supervised quarantine). To identify clusters of unique COVID-19 cases and to better understand how the virus affects public health, the researchers studied the genetic makeup of COVID-19 within samples from the nose swabs. In total, 1848 recruits volunteered to participate in the study. Within the first two days of supervised quarantine, 16 recruits tested positive for (or had) COVID-19. Fifteen of the 16 positive cases did not show symptoms of illness. An additional 35 participants tested positive on day 7 or on day 14. Fifty-one participants tested positive at any time. Five of these patients had symptoms before their COVID-19 test returned positive. Of the recruits who declined to participate in the study, 26 of the 1554 recruits with available COVID-19 test results were positive on day 14. No COVID-19 infections were identified through clinical testing performed as a result of daily symptom monitoring. The evaluation of the genetic makeup of the virus identified six spreading clusters among 18 participants. Tracing of the virus transmission identified several spreading events, including between roommates and among recruits within the same platoon. The authors concluded that among the recruits, around 2% of those who had tested negative for (or did not have) COVID-19 on day 1 of supervised quarantine, along with 2% of those with unknown previous status, tested positive by day 14. Most recruits who tested positive showed no signs of illness. No infections were detected through daily symptom monitoring. Spreading clusters occurred within platoons.",1,C
26,33176093,"Background: The efficacy of public health measures to control the transmission of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has not been well studied in young adults. Methods: We investigated SARS-CoV-2 infections among U.S. Marine Corps recruits who underwent a 2-week quarantine at home followed by a second supervised 2-week quarantine at a closed college campus that involved mask wearing, social distancing, and daily temperature and symptom monitoring. Study volunteers were tested for SARS-CoV-2 by means of quantitative polymerase-chain-reaction (qPCR) assay of nares swab specimens obtained between the time of arrival and the second day of supervised quarantine and on days 7 and 14. Recruits who did not volunteer for the study underwent qPCR testing only on day 14, at the end of the quarantine period. We performed phylogenetic analysis of viral genomes obtained from infected study volunteers to identify clusters and to assess the epidemiologic features of infections. Results: A total of 1848 recruits volunteered to participate in the study; within 2 days after arrival on campus, 16 (0.9%) tested positive for SARS-CoV-2, 15 of whom were asymptomatic. An additional 35 participants (1.9%) tested positive on day 7 or on day 14. Five of the 51 participants (9.8%) who tested positive at any time had symptoms in the week before a positive qPCR test. Of the recruits who declined to participate in the study, 26 (1.7%) of the 1554 recruits with available qPCR results tested positive on day 14. No SARS-CoV-2 infections were identified through clinical qPCR testing performed as a result of daily symptom monitoring. Analysis of 36 SARS-CoV-2 genomes obtained from 32 participants revealed six transmission clusters among 18 participants. Epidemiologic analysis supported multiple local transmission events, including transmission between roommates and among recruits within the same platoon. Conclusions: Among Marine Corps recruits, approximately 2% who had previously had negative results for SARS-CoV-2 at the beginning of supervised quarantine, and less than 2% of recruits with unknown previous status, tested positive by day 14. Most recruits who tested positive were asymptomatic, and no infections were detected through daily symptom monitoring. Transmission clusters occurred within platoons.","The effectiveness of public health policies to control the spread of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), which is a viral breathing-related disease, has not been well studied in young adults. We checked SARS-CoV-2 infections in U.S. Marine Corps recruits who had a 2-week quarantine or isolation at home. They then had a second supervised 2-week quarantine at a closed college campus with mask wearing, social distancing from others, and daily temperature and symptom checks. Study volunteers were tested for SARS-CoV-2 by swabs obtained between the time of arrival and second day of supervised isolation and on days 7 and 14. Recruits who did not volunteer had testing only on day 14, at the end of the isolation period. 1848 recruits participated in the study. Within 2 days after arrival on campus, 16 (0.9%) tested positive for or had SARS-CoV-2, 15 of whom showed no symptoms. Another 35 participants (1.9%) tested positive or had SARS-CoV-2 on day 7 or day 14. Five of the 51 participants (9.8%) who tested positive at all had symptoms in the week before a positive test. Of those who decline to participate in the study, 26 (1.7%) of the 1554 recruits with test results tested positive on day 14. No SARS-CoV-2 infections were detected by clinical testing performed for daily symptom checks. Analyzing 36 sets of genetic data from 32 participants showed 6 disease-spreading clusters among 18 participants. Analysis notes multiple disease-spreading events, including spreading between roommates and among recruits in the same military group. Among Marine Corps recruits, around 2% with past test results detecting no SARS-CoV-2 at the beginning of supervised isolation, and less than 2% of recruits of unknown status previously, had results detecting SARS-CoV-2 by day 14. Most recruits with positive tests had no symptoms. No infections were detected from daily symptom checks. Disease-spreading clusters occurred within military groups.",2,C
26,32594116,"Background: Patients recovering from coronavirus disease 2019 (COVID-19) often continue to test positive for the causative virus by polymerase chain reaction (PCR) even after clinical recovery, thereby complicating return-to-work plans. The purpose of this study was to evaluate transmission potential of COVID-19 by examining viral load with respect to time. Methods: Health care personnel (HCP) at Cleveland Clinic diagnosed with COVID-19, who recovered without needing hospitalization, were identified. Threshold cycles (Ct) for positive PCR tests were obtained and viral loads calculated. The association of viral load with days since symptom onset was examined in a multivariable regression model, which was reduced by stepwise backward selection to only keep variables significant at a level of .05. Viral loads by day since symptom onset were predicted using the model and transmission potential evaluated by examination of a viral load-time curve. Results: Over 6 weeks, 230 HCP had 528 tests performed. Viral loads declined by orders of magnitude within a few days of symptom onset. The only variable significantly associated with viral load was time since onset of symptoms. Of the area under the curve (AUC) spanning symptom onset to 30 days, 96.9% lay within the first 7 days, and 99.7% within 10 days. Findings were very similar when validated using split-sample and 10-fold cross-validation. Conclusions: Among patients with nonsevere COVID-19, viral loads in upper respiratory specimens peak by 2 or 3 days from symptom onset and decrease rapidly thereafter. The vast majority of the viral load-time AUC lies within 10 days of symptom onset.","Patients recovering from COVID-19 (a viral respiratory disease) oftentimes continue to test positive for (or have) the virus. This can make ""return to work"" plans difficult. The goal of this study is to evaluate COVID-19's ability to spread by determining the amount of the virus within an organism, known as the viral load, over time. Health care personnel at Cleveland Clinic diagnosed with COVID-19, who recovered without needing hospitalization, were identified. The viral load within the personnel was calculated. The link between the viral load within the patient and the first day of symptoms was evaluated. The viral load per day since the beginning of symptom onset within the patient were predicted using statistical models. Over six weeks, 230 health care personnel had 528 tests performed. The viral load within patients decreased within a few days of the beginning of virus-related symptoms. The viral load within the patient was significantly linked to time since onset of symptoms. The majority of the participants had peak viral load within the first 7 to 10 days. Findings were similar when other statistical tests were run. The authors concluded that among patients with no mild to moderate COVID-19, viral loads peaked by 2 or 3 days from symptom onset and decreased rapidly thereafter. The largest amount of viral load was on average within 10 days of symptom onset.",1,C
34,33439824,"Background: Approximately 10% of adults in Germany have chronic kidney disease (CKD). The prevalence of CKD among patients being cared for by general practitioners is approximately 30%, and its prevalence in nursing homes is over 50%. An S3 guideline has been developed for the management of CKD in primary care. Methods: The guideline is based on publications retrieved by a systematic search of the literature for international guidelines published in the period 2013-2017, and additional searches on specific questions. It was created by the German College of General Practitioners and Family Physicians (Deutsche Gesellschaft für Allgemeinmedizin und Familienmedizin, DEGAM) and consented with the German Societies of Nephrology and Internal Medicine (DGfN, DGIM) and patient representation. Results: Upon the initial diagnosis of CKD (glomerular filtration rate [GFR] <60 mL/ min), the patient's blood pressure and urinary albumin-to-creatinine ratio (ACR) should be measured, and the urine should be examined for hematuria. Monitoring intervals are determined on an individual basis depending on the stage of disease and the patient's general state of health and personal preferences. Nephrological consultation should be obtained if the GFR is less than 30 mL/min, if CKD is initially diagnosed (GFR 30-59 mL/min) in the presence of persistent hematuria without any urological explanation or of albinuria in stage A2 or higher, if the patient has refractory hypertension requiring three or more antihypertensive drugs, or if the renal disease is rapidly progressive. The threshold for referring a patient should be kept low for persons under age 50; persons over age 70 should be referred only if warranted in consideration of their comorbidities and individual health goals. Conclusion: The main elements of the treatment of CKD are the treatment of hypertension and diabetes and the modification of lifestyle factors. An innovation from the primary care practioner's perspective is the assessment of albuminuria with the albumin-to-creatinine ratio.","About 10% of adults in Germany have chronic kidney disease, which is when the kidneys are damaged and can't filter waste and extra fluid from the blood. The percentage of chronic kidney disease among patients being cared for by primary care doctors (family doctors) is about 30% and is over 50% in nursing homes. A guideline has been developed for the care of chronic kidney disease in primary (family) care offices. This guideline is based on existing, available data in articles published during 2013 - 2017 and also on additional searches of data on specific questions. It is created by the German College of General Practitioners and Family Physicians and agreed with by the German Societies of Nephrology and Internal Medicine and patient representation. At the first diagnosis of chronic kidney disease, the patient's blood pressure, as well as a urine sample that may indicate a kidney complication from high proteins, should be measured. The urine should also be checked for the presence of blood. How often the patient should be monitored is based on the individual and depends on the stage of disease, the patient's overall health, and personal preferences. A doctor who specializes in kidney disease should be consulted based on tests that check how well the kidneys are working, if there is blood in the urine, if the patient's blood pressure requires 3 or more drugs, and if the kidney disease is quickly getting worse. The requirements and test levels to refer a patient under the age of 50 years for specialty care should be kept low. People over 70 should be referred only if necessary due to other illnesses and individual health goals. The main parts of treating chronic kidney disease are the treatment of high blood pressure and diabetes and changing lifestyle. A new assessment from the family doctor is a sign of kidney disease due to too much of a protein called albumin in the urine.",1,C
34,22421952,"Background: Methoxy polyethylene glycol-epoetin beta (PEG-EPO) is indicated for the treatment of anaemia due to chronic kidney disease. Its long half-life allows it to be administered once per month in maintenance therapy. Objective: To evaluate the use, effectiveness and cost of PEG-EPO in a group of pre-dialysis chronic renal failure patients. Method: Retrospective observational study in pre-dialysis patients who began treatment with PEG-EPO between May 2008 and February 2009. The following data were gathered: age, sex, haemoglobin levels (Hb) and erythropoiesis-stimulating agent (ESA) dose and frequency. The follow-up period was 12 months. Results: We included 198 patients. Mean Hb upon starting PEG-EPO in patients who had received no prior treatment was 10.8g/l, and 11.6g/l at 90 days (P<.0001). In patients previously treated with ESA, mean Hb before starting PEG-EPO treatment was 11.2g/l, and 11.4g/l at 12 months (P=.846). Hb values were higher than 12g/l (P<.0001) after 12 months of treatment in 25% of patients; of these, 45% had values above 13g/l. We observed doses 39% lower than those indicated on the drug leaflet, resulting in a reduction in the originally expected theoretical costs. Conclusions: The doses of PEG-EPO administered to patients with a prior history of ESA treatment were lower than those indicated by the drug leaflet, and Hb remained stable after 12 months of treatment. A large portion of the patients had levels above the 13g/l threshold.","Methoxy polyethylene glycol-epoetin beta (PEG-EPO) is an injection that is often used to treat anemia (low red blood cells) due to chronic kidney disease. It stays in the body long enough to be given once per month. The objective of this study is to evaluate the use, effectiveness and cost of PEG-EPO in a group of chronic kidney failure patients who have not started dialysis, a process of using a machine to clean the blood of a person whose kidneys are not working normally. This study uses data from pre-dialysis patients who started treatment with Methoxy polyethylene glycol-epoetin beta (PEG-EPO) between May 2008 and February 2009. The following data are gathered: age, sex, hemoglobin levels (count of proteins that carry oxygen in the blood) and the dose and frequency of medication called erythropoiesis-stimulating agent (ESA) to help make red blood cells. The follow-up period is 12 months. The study included 198 patients. The average hemoglobin levels when patients start PEG-EPO who had received no prior treatment is 10.8 grams/liter, and is 11.6 grams/liter at 90 days. In patients who are previously treated with ESA medications to help make red blood cells, the average hemoglobin levels before starting PEG-EPO treatment is 11.2 grams/liter, and is 11.4 grams/liter at 12 months. Hemoglobin levels are higher than 12 grams/liter after 12 months of treatment in 25% of patients. Among these patients, 45% have levels above 13 grams/liter. Researchers observed doses 39% lower than those listed on the drug leaflet or description, resulting in less cost than originally expected. In conclusion, the doses of PEG-EPO given to patients with a prior history of erythropoiesis-stimulating agent (ESA) treatment are lower than those noted in the drug leaflet. Also, hemoglobin levels remained stable after 12 months of treatment. A large portion of the patients had levels above the 13 grams/liter threshold.",1,C
34,26391748,"A panel of internists and nephrologists developed this practical approach for the Kidney Disease Outcomes Quality Initiative to guide assessment and care of chronic kidney disease (CKD) by primary care clinicians. Chronic kidney disease is defined as a glomerular filtration rate (GFR) <60 mL/min/1.73m  and/or markers of kidney damage for at least 3 months. In clinical practice the most common tests for CKD include GFR estimated from the serum creatinine concentration (eGFR) and albuminuria from the urinary albumin-to-creatinine ratio. Assessment of eGFR and albuminuria should be performed for persons with diabetes and/or hypertension but is not recommended for the general population. Management of CKD includes reducing the patient's risk of CKD progression and risk of associated complications, such as acute kidney injury and cardiovascular disease, anemia, and metabolic acidosis, as well as mineral and bone disorder. Prevention of CKD progression requires blood pressure <140/90 mm Hg, use of angiotensin-converting enzyme inhibitors or angiotensin receptor blockers for patients with albuminuria and hypertension, hemoglobin A1c ≤7% for patients with diabetes, and correction of CKD-associated metabolic acidosis. To reduce patient safety hazards from medications, the level of eGFR should be considered when prescribing, and nephrotoxins should be avoided, such as nonsteroidal anti-inflammatory drugs. The main reasons to refer to nephrology specialists are eGFR <30 mL/min/1.73 m(2), severe albuminuria, and acute kidney injury. The ultimate goal of CKD management is to prevent disease progression, minimize complications, and promote quality of life.","A group of internal medicine and kidney doctors developed a practical approach to guide the assessment (diagnosis) and care of chronic kidney disease by primary care doctors. Chronic kidney disease is defined by using a glomerular filtration rate, a blood test that checks how well your kidneys are working, and/or other measurements or conditions that are a sign of kidney damage for at least 3 months. The most common tests for chronic kidney disease include the glomerular filtration rate that is estimated from the amount of creatinine (a waste product from the normal wear and tear on muscles) in the blood and too much albumin (a blood protein) in the urine which is called albuminuria. These tests for creatinine levels and albumin proteins should be done for people with diabetes and/or high blood pressure but are not recommended for the general population. Managing chronic kidney disease includes reducing the patient's risk of the disease getting worse and risk of related complications, such as acute or immediate kidney injury or heart disease. To prevent chronic kidney disease from worsening, managing blood pressure, using medications to treat high albumin levels and high blood pressure, measuring hemoglobin levels (red blood cells) for patients with diabetes, and correcting when there is too much acid in the body's fluids is needed. To reduce the negative effect of medications on patients, the level of creatinine should be noted when prescribing drugs, and nephrotoxins which can damage the kidneys should be avoided. The main reasons to send a patient to a kidney specialist are based on creatinine levels, severe albuminuria (too much protein in urine), and acute kidney injury (a sudden episode of kidney failure). The main goal of managing chronic kidney disease is to prevent the disease from getting worse, to minimize complications, and to promote quality of life.",1,C
34,26391748,"A panel of internists and nephrologists developed this practical approach for the Kidney Disease Outcomes Quality Initiative to guide assessment and care of chronic kidney disease (CKD) by primary care clinicians. Chronic kidney disease is defined as a glomerular filtration rate (GFR) <60 mL/min/1.73m  and/or markers of kidney damage for at least 3 months. In clinical practice the most common tests for CKD include GFR estimated from the serum creatinine concentration (eGFR) and albuminuria from the urinary albumin-to-creatinine ratio. Assessment of eGFR and albuminuria should be performed for persons with diabetes and/or hypertension but is not recommended for the general population. Management of CKD includes reducing the patient's risk of CKD progression and risk of associated complications, such as acute kidney injury and cardiovascular disease, anemia, and metabolic acidosis, as well as mineral and bone disorder. Prevention of CKD progression requires blood pressure <140/90 mm Hg, use of angiotensin-converting enzyme inhibitors or angiotensin receptor blockers for patients with albuminuria and hypertension, hemoglobin A1c ≤7% for patients with diabetes, and correction of CKD-associated metabolic acidosis. To reduce patient safety hazards from medications, the level of eGFR should be considered when prescribing, and nephrotoxins should be avoided, such as nonsteroidal anti-inflammatory drugs. The main reasons to refer to nephrology specialists are eGFR <30 mL/min/1.73 m(2), severe albuminuria, and acute kidney injury. The ultimate goal of CKD management is to prevent disease progression, minimize complications, and promote quality of life.","A group of specialists created this practical approach for Kidney Disease Outcomes Quality Initiative to guide analysis and care of long-lasting or chronic kidney disease (CKD) by primary care workers. Long-lasting kidney disease is a low kidney filtration rate and/or markers of kidney damage for at least 3 months. In clinical practice, the most common tests for CKD include kidney filtration rate of blood creatinine, a chemical waste, (eGFR) and high blood albumin, a blood protein, from the albumin-to-creatinine ratio in urine. Measuring eGFR and albumin levels should be done in persons with diabetes and/or high blood pressure but not for the general population. Managing CKD includes lowering the patient's risk of CKD worsening and risk of associated issues, like immediate kidney injury, heart disease, low blood level, high acid level, and mineral and bone disorder. Preventing CKD progression requires blood pressure <140/90 mm Hg, certain blood pressure medications for patients with high blood pressure and albumin, lower blood sugar for diabetics, and correcting high body acid levels linked with CKD. To reduce patient safety hazards from medications, eGFR should be considering when prescribing. Kidney toxins should be avoided, like a class of anti-inflammatory drugs. The main reasons to contact kidney specialists are eGFR <30 mL/min/1.73 m(2), high blood albumin, and immediate kidney injury. The goal of CKD treatment is prevent disease worsening, reduce issues, and better quality of life.",2,C
34,31573641,"Importance: Chronic kidney disease (CKD) is the 16th leading cause of years of life lost worldwide. Appropriate screening, diagnosis, and management by primary care clinicians are necessary to prevent adverse CKD-associated outcomes, including cardiovascular disease, end-stage kidney disease, and death. Observations: Defined as a persistent abnormality in kidney structure or function (eg, glomerular filtration rate [GFR] <60 mL/min/1.73 m2 or albuminuria ≥30 mg per 24 hours) for more than 3 months, CKD affects 8% to 16% of the population worldwide. In developed countries, CKD is most commonly attributed to diabetes and hypertension. However, less than 5% of patients with early CKD report awareness of their disease. Among individuals diagnosed as having CKD, staging and new risk assessment tools that incorporate GFR and albuminuria can help guide treatment, monitoring, and referral strategies. Optimal management of CKD includes cardiovascular risk reduction (eg, statins and blood pressure management), treatment of albuminuria (eg, angiotensin-converting enzyme inhibitors or angiotensin II receptor blockers), avoidance of potential nephrotoxins (eg, nonsteroidal anti-inflammatory drugs), and adjustments to drug dosing (eg, many antibiotics and oral hypoglycemic agents). Patients also require monitoring for complications of CKD, such as hyperkalemia, metabolic acidosis, hyperphosphatemia, vitamin D deficiency, secondary hyperparathyroidism, and anemia. Those at high risk of CKD progression (eg, estimated GFR <30 mL/min/1.73 m2, albuminuria ≥300 mg per 24 hours, or rapid decline in estimated GFR) should be promptly referred to a nephrologist. Conclusions and relevance: Diagnosis, staging, and appropriate referral of CKD by primary care clinicians are important in reducing the burden of CKD worldwide.","Chronic kidney disease (CKD) is the 16th leading cause of years of life lost worldwide. Appropriate screening, diagnosis, and care by primary care (family) doctors are necessary to prevent negative outcomes associated with CKD, including heart disease, end-stage kidney disease, and death. Defined as an ongoing impairment in kidney structure or function for more than 3 months, chronic kidney disease affects 8% to 16% of people worldwide. In developed countries, CKD is most commonly associated with diabetes and high blood pressure. However, less than 5% of patients with early CKD report knowing about of their disease. Among people diagnosed as having CKD, new risk assessment tools can help guide treatment, monitoring, and inform when to send patients to a specialist. These new tools include using the glomerular filtration rate, a blood test that checks how well your kidneys are working, and albuminuria, which is too much albumin (a blood protein) in the urine. The best management of CKD includes reducing risk of heart disease, treating albuminuria, avoiding medications that are toxic to the kidneys, and changes to drug dosing, such as antibiotics. Patients also need monitoring for complications with CKD, such as higher than normal potassium levels, too much acid in the fluids of the body, too much phosphorus in the blood, low vitamin D, overactive glands in the neck that produce parathyroid (calcium-regulating) hormone, and anemia (low red blood cells). Those at high risk of chronic kidney disease progression based on tests that check how well the kidneys are working or too much albumin (a protein) in the urine, which is called albuminuria, should be quickly sent to a kidney specialist. Diagnosis, determining the stage, and appropriate referral of chronic kidney disease by primary care doctors are important in reducing the negative impact of chronic kidney disease worldwide.",1,C
34,34249961,"Due to the unique role of the kidney in the metabolism of nutrients, patients with chronic kidney disease (CKD) lose the ability to excrete solutes and maintain homeostasis. Nutrient intake modifications and monitoring of nutritional status in this population becomes critical, since it can affect important health outcomes, including progression to kidney failure, quality of life, morbidity, and mortality. Although there are multiple hemodynamic and metabolic factors involved in the progression and prognosis of CKD, nutritional interventions are a central component of the care of patients with non-dialysis CKD (ND-CKD) and of the prevention of overweight and possible protein energy-wasting. Here, we review the reno-protective effects of diet in adults with ND-CKD stages 3-5, including transplant patients.","Due to the unique role of the kidney in turning nutrients into fuel, patients with chronic kidney disease (CKD) lose the ability to release dissolved substances and to maintain a healthy internal balance of water, sodium, and other elements in the body. Changing the nutrients that are consumed and checking for how nutrients impact health in this population is important, since it can impact health results, including advancing to kidney failure, quality of life, illness, and death. Although there are other factors involved in the progression and the likely course of chronic kidney disease, nutritional steps are a main part of the care of patients with chronic kidney disease who are not on dialysis, a process of filtering the blood of a person whose kidneys are not working normally by using a machine. This review discusses how diet in adults with chronic kidney disease stages 3-5 who are not on dialysis can have a protective effect on kidneys, including transplant patients.",1,C
34,28434208,"Background: Dietary changes are routinely recommended in people with chronic kidney disease (CKD) on the basis of randomised evidence in the general population and non-randomised studies in CKD that suggest certain healthy eating patterns may prevent cardiovascular events and lower mortality. People who have kidney disease have prioritised dietary modifications as an important treatment uncertainty. Objectives: This review evaluated the benefits and harms of dietary interventions among adults with CKD including people with end-stage kidney disease (ESKD) treated with dialysis or kidney transplantation. Main results: We included 17 studies involving 1639 people with CKD. Three studies enrolled 341 people treated with dialysis, four studies enrolled 168 kidney transplant recipients, and 10 studies enrolled 1130 people with CKD stages 1 to 5. Eleven studies (900 people) evaluated dietary counselling with or without lifestyle advice and six evaluated dietary patterns (739 people), including one study (191 people) of a carbohydrate-restricted low-iron, polyphenol enriched diet, two studies (181 people) of increased fruit and vegetable intake, two studies (355 people) of a Mediterranean diet and one study (12 people) of a high protein/low carbohydrate diet. Risks of bias in the included studies were generally high or unclear, lowering confidence in the results. Participants were followed up for a median of 12 months (range 1 to 46.8 months). Studies were not designed to examine all-cause mortality or cardiovascular events. In very-low quality evidence, dietary interventions had uncertain effects on all-cause mortality or ESKD. In absolute terms, dietary interventions may prevent one person in every 3000 treated for one year avoiding ESKD, although the certainty in this effect was very low. Across all 17 studies, outcome data for cardiovascular events were sparse. Dietary interventions in low quality evidence were associated with a higher health-related quality of life (2 studies, 119 people: MD in SF-36 score 11.46, 95% CI 7.73 to 15.18; I2 = 0%). Adverse events were generally not reported. Dietary interventions lowered systolic blood pressure (3 studies, 167 people: MD -9.26 mm Hg, 95% CI -13.48 to -5.04; I2 = 80%) and diastolic blood pressure (2 studies, 95 people: MD -8.95, 95% CI -10.69 to -7.21; I2 = 0%) compared to a control diet. Dietary interventions were associated with a higher estimated glomerular filtration rate (eGFR) (5 studies, 219 people: SMD 1.08; 95% CI 0.26 to 1.97; I2 = 88%) and serum albumin levels (6 studies, 541 people: MD 0.16 g/dL, 95% CI 0.07 to 0.24; I2 = 26%). A Mediterranean diet lowered serum LDL cholesterol levels (1 study, 40 people: MD -1.00 mmol/L, 95% CI -1.56 to -0.44). Authors' conclusions: Dietary interventions have uncertain effects on mortality, cardiovascular events and ESKD among people with CKD as these outcomes were rarely measured or reported. Dietary interventions may increase health-related quality of life, eGFR, and serum albumin, and lower blood pressure and serum cholesterol levels. Based on stakeholder prioritisation of dietary research in the setting of CKD and preliminary evidence of beneficial effects on risks factors for clinical outcomes, large-scale pragmatic RCTs to test the effects of dietary interventions on patient outcomes are required.","Changes in diet are often recommended for people with chronic kidney disease on the basis of evidence in the general population and in other studies of chronic kidney disease. People who have kidney disease have prioritized changes in their diet as uncertain treatment. This review evaluates the benefits and harms of dietary interventions (changing diet and diet behavior to reach a health goal) among adults with chronic kidney disease, including people with end-stage kidney disease treated with dialysis (a process that uses a machine to clean the blood because the kidneys are not working) or kidney transplantation or donation. Researchers include 17 studies involving 1639 people with chronic kidney disease. Three studies include 341 people treated with dialysis, four studies have 168 kidney transplant recipients, and 10 studies have 1130 people with chronic kidney disease at stages ranging from 1 to 5. Among these studies, 11 evaluated dietary counselling with or without lifestyle advice, and 6 evaluated dietary patterns, including 1 study of a low-carb/low-iron with many plant-based foods diet, 2 studies of increased fruit and vegetable intake, 2 studies of a Mediterranean diet and 1 study of a high protein/low carb diet. Risks of bias in these studies are generally high or unclear, lowering confidence in how true the results are in these papers. Participants are followed up for about 12 months, but the time ranges between 1 to 46.8 months. Studies are not designed to examine all causes of illness or heart disease events. In very low quality evidence, dietary interventions or treatment have uncertain effects on deaths from any cause or end-stage kidney disease. Dietary interventions to treat kidney disease may prevent one person in every 3000 treated for one year to avoid end-stage kidney disease, although the certainty that this result will happen is very low. Across all 17 studies, outcome data for heart events are limited. Dietary interventions in low quality evidence studies are associated with a higher health-related quality of life. Adverse (unexpected and negative) events are generally not reported. In some studies, dietary interventions lowered systolic blood pressure (top blood pressure number) and diastolic blood pressure (bottom blood pressure number) compared to a control diet. Dietary interventions are associated with a higher estimated glomerular filtration rate (eGFR), a blood test that measures removal of creatinine levels which are waste products from digestion and muscle breakdown. They are also linked to higher albumin (liver proteins that keep fluid in the bloodstream) levels in the blood. A Mediterranean diet lowered LDL (bad) cholesterol levels. In conclusion, dietary interventions have uncertain effects on death, heart events, and end-stage kidney disease among people with chronic kidney disease because these effects are rarely measured or described. Dietary interventions may increase health-related quality of life, eGFR, and albumin levels in the blood, and lower blood pressure and cholesterol levels. Large-scale clinical studies to test the effects of dietary interventions on patient outcomes are needed.",1,C
34,28434208,"Background: Dietary changes are routinely recommended in people with chronic kidney disease (CKD) on the basis of randomised evidence in the general population and non-randomised studies in CKD that suggest certain healthy eating patterns may prevent cardiovascular events and lower mortality. People who have kidney disease have prioritised dietary modifications as an important treatment uncertainty. Objectives: This review evaluated the benefits and harms of dietary interventions among adults with CKD including people with end-stage kidney disease (ESKD) treated with dialysis or kidney transplantation. Main results: We included 17 studies involving 1639 people with CKD. Three studies enrolled 341 people treated with dialysis, four studies enrolled 168 kidney transplant recipients, and 10 studies enrolled 1130 people with CKD stages 1 to 5. Eleven studies (900 people) evaluated dietary counselling with or without lifestyle advice and six evaluated dietary patterns (739 people), including one study (191 people) of a carbohydrate-restricted low-iron, polyphenol enriched diet, two studies (181 people) of increased fruit and vegetable intake, two studies (355 people) of a Mediterranean diet and one study (12 people) of a high protein/low carbohydrate diet. Risks of bias in the included studies were generally high or unclear, lowering confidence in the results. Participants were followed up for a median of 12 months (range 1 to 46.8 months). Studies were not designed to examine all-cause mortality or cardiovascular events. In very-low quality evidence, dietary interventions had uncertain effects on all-cause mortality or ESKD. In absolute terms, dietary interventions may prevent one person in every 3000 treated for one year avoiding ESKD, although the certainty in this effect was very low. Across all 17 studies, outcome data for cardiovascular events were sparse. Dietary interventions in low quality evidence were associated with a higher health-related quality of life (2 studies, 119 people: MD in SF-36 score 11.46, 95% CI 7.73 to 15.18; I2 = 0%). Adverse events were generally not reported. Dietary interventions lowered systolic blood pressure (3 studies, 167 people: MD -9.26 mm Hg, 95% CI -13.48 to -5.04; I2 = 80%) and diastolic blood pressure (2 studies, 95 people: MD -8.95, 95% CI -10.69 to -7.21; I2 = 0%) compared to a control diet. Dietary interventions were associated with a higher estimated glomerular filtration rate (eGFR) (5 studies, 219 people: SMD 1.08; 95% CI 0.26 to 1.97; I2 = 88%) and serum albumin levels (6 studies, 541 people: MD 0.16 g/dL, 95% CI 0.07 to 0.24; I2 = 26%). A Mediterranean diet lowered serum LDL cholesterol levels (1 study, 40 people: MD -1.00 mmol/L, 95% CI -1.56 to -0.44). Authors' conclusions: Dietary interventions have uncertain effects on mortality, cardiovascular events and ESKD among people with CKD as these outcomes were rarely measured or reported. Dietary interventions may increase health-related quality of life, eGFR, and serum albumin, and lower blood pressure and serum cholesterol levels. Based on stakeholder prioritisation of dietary research in the setting of CKD and preliminary evidence of beneficial effects on risks factors for clinical outcomes, large-scale pragmatic RCTs to test the effects of dietary interventions on patient outcomes are required.","Diet changes are usually recommended in people with long-lasting or chronic kidney disease (CKD) since studies suggest certain healthy eating patterns may prevent heart-related events and lower deaths. People with kidney disease have important diet changes as a treatment uncertainty. This review explored the pros and cons of dietary treatment in adults with CKD including those with kidney failure treated with an artificial kidney machine or kidney transplant. We included 17 studies with 1639 people with CKD. Three studies used 341 people treated with an artifical kidney disease. Four studies had 168 kidney transplant receivers. 10 studies had 1130 people with low to severe CKD. Eleven studies (900 people) evaluated diet counselling with or without lifestyle advice. Six evaluated diet patterns (739 people), including one study (191 people) of a carb-restricted low-iron, high plant-nutrient diet, two studies (181 people) of increased fruit and vegetable intake, two studies (355 people) of a Mediterranean diet and one study (12 people) of a high protein/low carb diet. Risk of bias in the studies were generally high or unclear, casting doubts in the results. Participants were checked for an average of 12 months. Studies were not made to examine all-cause death or heart-related events. In very-low quality results, diet interventions had uncertain effects on all-cause deaths or kidney failure. In established results, diet treatments may prevent kidney failure in one in every 3000 people, although the certainty of this effect was very low. Across all 17 studies, outcome data for heart-related evens was rare. Diet treatments in low quality evidence was linked with a higher heart-related quality of life. Side effects were generally not reported. Diet treatments lowered systolic and diastolic blood pressure compared to a control or regular diet. Diet treatments were linked with a higher kidney filtration rate and blood levels of albumin, a blood protein. A Mediterranean diet lowered ""unwanted"" blood cholesterol levels. Diet treatments have uncertain effects on death, heart-related events, and kidney failure among people with CKD since these outcomes were rarely measured or reported. Diet treatment may increased health-related quality of life, kidney filtration rate, and blood albumin. Diet treatment may lower blood pressure and blood cholesterol levels. Based on the importance of diet research for CKD and initial evidence of beneficial effects on health, large-scale studies to test the effects of diet treatments on patient outcomes are needed.",2,C
34,23636234,"Protein-energy wasting (PEW), characterized by a decline in body protein mass and energy reserves, including muscle and fat wasting and visceral protein pool contraction, is an underappreciated condition in early to moderate stages of chronic kidney disease (CKD) and a strong predictor of adverse outcomes. The prevalence of PEW in early to moderate CKD is ≥20-25% and increases as CKD progresses, in part because of activation of proinflammatory cytokines combined with superimposed hypercatabolic states and declines in appetite. This anorexia leads to inadequate protein and energy intake, which may be reinforced by prescribed dietary restrictions and inadequate monitoring of the patient's nutritional status. Worsening uremia also renders CKD patients vulnerable to potentially deleterious effects of uncontrolled diets, including higher phosphorus and potassium burden. Uremic metabolites, some of which are anorexigenic and many of which are products of protein metabolism, can exert harmful effects, ranging from oxidative stress to endothelial dysfunction, nitric oxide disarrays, renal interstitial fibrosis, sarcopenia, and worsening proteinuria and kidney function. Given such complex pathways, nutritional interventions in CKD, when applied in concert with nonnutritional therapeutic approaches, encompass an array of strategies (such as dietary restrictions and supplementations) aimed at optimizing both patients' biochemical variables and their clinical outcomes. The applicability of many nutritional interventions and their effects on outcomes in patients with CKD with PEW has not been well studied. This article reviews the definitions and pathophysiology of PEW in patients with non-dialysis-dependent CKD, examines the current indications for various dietary modification strategies in patients with CKD (eg, manufactured protein-based supplements, amino acids and their keto acid or hydroxyacid analogues), discusses the rationale behind their potential use in patients with PEW, and highlights areas in need of further research.","Protein-energy wasting is a decline in the amount of protein in the body and leads to less stored energy. It is a condition present in early to moderate stages of chronic kidney disease and a signal that negative health outcomes may occur. Protein-energy wasting often increases as chronic kidney disease gets worse, in part because of more inflammation (redness and swelling from fighting an infection) combined with too much breakdown of proteins and loss of appetite. This leads to not enough proteins and energy, which may be held up by dietary restrictions from doctors and not enough monitoring of how the patient's nutrition impacts their health. Worsening uremia, which is when there is too much waste in the blood, may make uncontrolled diets have a negative impact on chronic disease patients. Too many waste products in the blood that would normally be removed by urine can have harmful effects, including an imbalance of free radicals and antioxidants in the body (which can lead to cell and tissue damage), endothelial dysfunction which is damaged functioning of the lining of blood vessels impacting the heart, and other conditions. Nutritional interventions (changing diet and diet behavior to reach a health goal) in chronic kidney disease, when combined with other therapies unrelated to nutrition, create a number of strategies aimed at improving the internal systems of the body in the patient and the patient's health outcomes. How nutritional interventions can work and their effects on patients with chronic kidney disease with protein-energy wasting is not well studied. This article reviews the definitions and the process of protein-energy wasting in patients with chronic kidney disease who are not on dialysis, and examines when changes in the diet is appropriate and areas that need further research.",1,C
34,34164121,"Adherence to a Mediterranean lifestyle may be a useful primary and secondary prevention strategy for chronic kidney disease (CKD). This cross-sectional study aimed to explore adherence to a Mediterranean lifestyle and its association with cardiometabolic markers and kidney function in 99 people aged 73⋅2 ± 10⋅5 years with non-dialysis dependant CKD (stages 3-5) at a single Australian centre. Adherence was assessed using an a priori index, the Mediterranean Lifestyle (MEDLIFE) index. Cardiometabolic markers (total cholesterol, LDL-cholesterol, HbA1c and random blood glucose) and kidney function (estimated GFR) were sourced from medical records and blood pressure measured upon recruitment. Overall, adherence to a Mediterranean lifestyle was moderate to low with an average MEDLIFE index score of 11⋅33 ± 3⋅31. Adherence to a Mediterranean lifestyle was associated with employment (r 0⋅30, P = 0⋅004). Mediterranean dietary habits were associated with cardiometabolic markers, such as limiting sugar in beverages was associated with lower diastolic blood pressure (r 0⋅32, P = 0⋅002), eating in moderation with favourable random blood glucose (r 0⋅21, P = 0⋅043), having more than two snack foods per week with HbA1c (r 0⋅29, P = 0⋅037) and LDL-cholesterol (r 0⋅41, P = 0⋅002). Interestingly, eating in company was associated with a lower frequency of depression (χ 2 5⋅975, P = 0⋅015). To conclude, Mediterranean dietary habits were favourably associated with cardiometabolic markers and management of some comorbidities in this group of people with non-dialysis dependent CKD.","Keeping a Mediterranean lifestyle may be a useful primary and secondary prevention plan for chronic kidney disease (CKD). This study aims to explore adherence (commitment) to a Mediterranean lifestyle and its association with blood pressure, cholesterol, and other heart related measures, as well as its impact on kidney function. This study includes 99 people aged 73.2 years (plus or minus 10.5 years) with chronic kidney disease who are not on dialysis, a process that uses a machine to filter blood. Adherence is assessed (measured) using the Mediterranean Lifestyle (MEDLIFE) index, that includes questions on food consumption, dietary habits, physical activity, rest, and social interactions. Tests on total cholesterol, LDL (""bad"") cholesterol, blood sugar levels in the last 2-3 months and at any random time, and kidney function are collected from medical records and blood pressure measured at the start of the study. Overall, adherence to a Mediterranean lifestyle is moderate to low with an average MEDLIFE index score of 11⋅33 ± 3⋅31. Adherence to a Mediterranean lifestyle is associated with employment. Mediterranean dietary habits are associated with certain heart-related measures, such as how limiting sugar in beverages is associated with lower diastolic blood pressure (bottom number of blood pressure readings), eating in moderation with favorable blood sugar levels tested at random, having more than two snack foods per week with blood sugar tests, and LDL-cholesterol. Interestingly, eating with others is associated with a lower frequency of depression. In conclusion, Mediterranean dietary habits are positively linked with heart-related measures and care of other health problems in this group of people with non-dialysis chronic kidney disease.",1,C
34,33993809,"Introduction: Secondary hyperparathyroidism (SHPT) represents a complication of chronic kidney disease (CKD). Vitamin D system is altered since early CKD, and vitamin D deficiency is an established trigger of SHPT. Although untreated SHPT may degenerate into tertiary hyperparathyroidism with detrimental consequences in advanced CKD, best treatments for counteracting SHPT from stage 3 CKD are still debated. Enthusiasm on prescription of vitamin D receptor activators (VDRA) in non-dialysis renal patients, has been mitigated by the risk of low bone turnover and positive calcium-phosphate balance. Nutritional vitamin D is now suggested as first-line therapy to treat SHPT with low 25(OH)D insufficiency. However, no high-grade evidence supports the best choice between ergocalciferol, cholecalciferol, and calcifediol (in its immediate or extended-release formulation). Areas covered: The review discusses available data on safety and efficacy of nutritional vitamin D, VDRA and nutritional therapy in replenishing 25(OH)D deficiency and counteracting SHPT in non-dialysis CKD patients. Expert opinion: Best treatment for low 25(OH)D and SHPT remains unknown, due to incomplete understanding of the best homeostatic, as mutable, adaptation of mineral metabolism to CKD progression. Nutritional vitamin D and nutritional therapy appear safest interventions, whenever contextualized with single-patient characteristics. VDRA should be restricted to uncontrolled SHPT by first-line therapy.","A disease of the parathyroid (calcium-regulating) glands in the neck that is caused by another disease is called secondary hyperparathyroidism (SHPT), with symptoms that include weak bones, kidney stones, and tiredness. SHPT represents a complication or bad effect of chronic kidney disease (CKD). The vitamin D system is altered early in CKD, and not enough vitamin D is an established trigger of SHPT. Untreated SHPT may become tertiary hyperparathyroidism (when too much of the parathyroid hormone is produced even when the original problem is corrected) with harmful consequences in advanced chronic kidney disease. However, the best treatments for acting against SHPT from stage 3 chronic kidney disease are still debated. Enthusiasm on prescription of vitamin D treatments in non-dialysis kidney patients is lessened by the risk of low bone turnover (when the bone tissue is reabsorbed and replaced by a new bone), and positive calcium-phosphate balance that makes sure systems in the body work well. Nutritional vitamin D is now suggested as first-line therapy to treat secondary hyperparathyroidism with low vitamin D blood test scores. However, no high-grade evidence supports the best choice between which vitamin D product to prescribe. Other areas covered in this review are the data available on safety and effectiveness (success) of vitamin D, vitamin D prescriptions, and nutritional therapy in restoring vitamin D to normal levels (via diet) and acting against secondary hyperparathyroidism. The expert opinion is that the best treatment for low vitamin D levels and secondary hyperparathyroidism remains unknown due to some missing key information. Nutritional vitamin D and nutritional therapy appear to be the safest interventions (treatments), when considering the individual characteristics of each patient. Prescriptions for vitamin D should be limited to only uncontrolled secondary hyperparathyroidism by first treatment recommended.",1,C
34,33993809,"Introduction: Secondary hyperparathyroidism (SHPT) represents a complication of chronic kidney disease (CKD). Vitamin D system is altered since early CKD, and vitamin D deficiency is an established trigger of SHPT. Although untreated SHPT may degenerate into tertiary hyperparathyroidism with detrimental consequences in advanced CKD, best treatments for counteracting SHPT from stage 3 CKD are still debated. Enthusiasm on prescription of vitamin D receptor activators (VDRA) in non-dialysis renal patients, has been mitigated by the risk of low bone turnover and positive calcium-phosphate balance. Nutritional vitamin D is now suggested as first-line therapy to treat SHPT with low 25(OH)D insufficiency. However, no high-grade evidence supports the best choice between ergocalciferol, cholecalciferol, and calcifediol (in its immediate or extended-release formulation). Areas covered: The review discusses available data on safety and efficacy of nutritional vitamin D, VDRA and nutritional therapy in replenishing 25(OH)D deficiency and counteracting SHPT in non-dialysis CKD patients. Expert opinion: Best treatment for low 25(OH)D and SHPT remains unknown, due to incomplete understanding of the best homeostatic, as mutable, adaptation of mineral metabolism to CKD progression. Nutritional vitamin D and nutritional therapy appear safest interventions, whenever contextualized with single-patient characteristics. VDRA should be restricted to uncontrolled SHPT by first-line therapy.","Secondary hyperparathyroidism (SHPT), which is when the parathyroid glands become hyperactive due to a disease outside the glands, can be an effect of long-lasting or chronic kidney disease (CKD). Vitamin D metabolism is altered since early CKD. Vitamin D loss is a known trigger of SHPT. While untreated SHPT may worsen to uncontrolled, excess parathyroid hormone production with harmful effects in advanced CKD, best treatments for preventing this worsening are unknown. The idea of using vitamin D target site or receptor activators (VDRA) in working kidney patients, has been weakened by the risk of bone breakdown and calcium-phosphate imbalance. Nutritional vitamin D is now suggested as first-choice therapy for SHPT and low active vitamin D levels. Still, no high-quality evidence supports the best choice among different vitamin D medications. The review explores available data on the safety and success of nutritional vitamin D, VDRA, and nutritional therapy in replenishing active vitamin D shortage and treating SHPT in working kidney CKD patients. Best treatment for low active vitamin D and SHPT is unknown, due to a partial grasp of the best self-regulating mechanism of mineral metabolism to CKD progression. Nutritional vitamin D and therapy appear safest treatments, when considering single-patient characteristics. VDRA should be limited to uncontrolled SHPT by first-choice therapy.",2,C
34,33388899,"Chronic kidney disease (CKD) is a prevalent worldwide public burden that increasingly compromises overall health as the disease progresses. Two of the most negatively affected tissues are bone and skeletal muscle, with CKD negatively impacting their structure, function and activity, impairing the quality of life of these patients and contributing to morbidity and mortality. Whereas skeletal health in this population has conventionally been associated with bone and mineral disorders, sarcopenia has been observed to impact skeletal muscle health in CKD. Indeed, bone and muscle tissues are linked anatomically and physiologically, and together regulate functional and metabolic mechanisms. With the initial crosstalk between the skeleton and muscle proposed to explain bone formation through muscle contraction, it is now understood that this communication occurs through the interaction of myokines and osteokines, with the skeletal muscle secretome playing a pivotal role in the regulation of bone activity. Regular exercise has been reported to be beneficial to overall health. Also, the positive regulatory effect that exercise has been proposed to have on bone and muscle anatomical, functional, and metabolic activity has led to the proposal of regular physical exercise as a therapeutic strategy for muscle and bone-related disorders. The detection of bone- and muscle-derived cytokine secretion following physical exercise has strengthened the idea of a cross communication between these organs. Hence, this review presents an overview of the impact of CKD in bone and skeletal muscle, and narrates how these tissues intrinsically communicate with each other, with focus on the potential effect of exercise in the modulation of this intercommunication.","Chronic kidney disease is a common health condition around the world and impacts the overall health of a person as the disease gets worse. Bones and muscles attached to bones are tissues highly affected by chronic kidney disease, which damages their functions and activities and contributes to poor quality of life. Sarcopenia is a disorder that results in loss of muscle mass and function and is found to impact overall skeletal muscle health in chronic kidney disease. Bone and muscle tissues are linked in the body, and together, they regulate systems in the body that help the body function and process and distribute nutrients. The skeletal muscle cells release small proteins that regulate different parts of the body, including bone activity. Regular exercise is found to be beneficial to overall health. Also, the positive effect exercise is thought to have on bones and muscles, as well as on function and activity, leads to the suggestion of regular physical exercise as a way to help muscle and bone-related disorders. The detection of bone and muscle proteins after exercise strengthens the idea of a cross communication between these organs. This review presents an overview of the impact of chronic kidney disease in bones and muscles attached to bones and describes how these tissues communicate with each other, with a focus on the possible effect of exercise.",1,C
40,32710511,"The presence of chronic heart failure (CHF) results in a significant risk of leg oedema. Medical compression (MC) treatment is one of the basic methods of leg oedema elimination in patients with chronic venous disease and lymphedema, but it is not routinely considered in subjects with CHF-related swelling. In the study, an overview of the current knowledge related to the benefits and risk of using MC in the supportive treatment of leg oedema in CHF patients is presented. The available studies dedicated the comprehensive management of leg swelling using MC in CHF patients published in the English language literature till December 2019 were evaluated in term of the treatment efficacy and safety. In studies performed on CHF populations, manual lymphatic drainage, MC stocking, multilayer bandaged, as well as intermittent pneumatic compression or electric calf stimulations were used. The current evidence is based on non-randomized studies, small study cohorts, as well as very heterogenous populations. The use of the intermittent pneumatic compression in CHF patients significantly increases the right auricular pressure and mean pulmonary artery pressures as well as decreases systemic vascular resistance in most patients without the clinical worsening. The transient and rapid increase in the human atrial natriuretic peptide, after an application of the MC stocking in New York Heart Association (NYHA) class II patients was observed without clinical exacerbation. An application of the multilayer bandages in NYHA classes III and IV patients lead a significant increase in the right arterial pressure and lead to transient deterioration of the right and the left ventricular functions. In the manual lymphatic drainage study, aside from expected leg circumference reduction, no clinical worsening was observed. In a pilot study performed in a small cohort of CHF patients, electrical calf stimulation use resulted in a reduction in the lean mass of the legs without cardiac function worsening. The use of local leg compression can be considered stable CHF patients without decompensated heart function for both CHF-related oedema treatment and for treatment of the concomitant diseases leading to leg swelling occurrence. The use of MC in more severe classes of CHF (NYHA III and IV) should be the subject of future clinical studies to select the safest and most efficient compression method as well as to select the patients who benefit most from this kind of treatment.","Long-lasting or chronic heart failure (CHF) results in a significant risk of leg oedema. Leg oedema is swelling due to excess fluid accumulation in the body. Medical compression (MC) treatment is a basic method to eliminate leg oedema in patients with chronic venous disease (abnormal veins) and lymphedema (swelling). However, it is not routinely considered in subjects with CHF-related swelling. This study is an overview of evidence related to the benefits and risk of using MC in the treatment of leg oedema in CHF patients. This paper reviews research published in English up until December 2019. The reviewed paper focused on the management of leg swelling through the use of MC within CHF patients. All reviewed papers were evaluated for treatment efficacy and safety. In studies of CHF populations, several treatment options were used. These options include manual lymphatic drainage, MC stocking, multilayer bandaged, and electric calf stimulations. The current evidence is based on non-randomized studies, small study cohorts, as well as very diverse populations. Intermittent pneumatic compression is a treatment that uses a device that squeezes fluids from an infected area. The treatment in CHF patients significantly increased pressure in the lungs and heart. The treatment decreased systemic vascular resistance, or the resistance that must be overcome to push blood through veins, in most patients without the disease worsening. Human atrial natriuretic peptide, a substance in the body that reduces fluid volume, increased in patients without clinical exacerbation or pain. Use of multilayer bandages in patients significantly increased the right arterial (section of the heart) pressure. Use of multilayer bandages also lead to deterioration of heart chamber function. In the manual lymphatic drainage study, aside from reducing leg size, no clinical worsening was observed. In a trial study performed in a small cohort of CHF patients, electrical calf stimulation resulted in reduced lean mass of the legs without cardiac function worsening. The use of local leg compression within CHF patients could be considered. Use of local leg compression did not decompensate heart function in patients with either CHF-related oedema or concomitant (accompanying) diseases leading to leg swelling. The use of MC in more severe classes of CHF should be further studied to select the safest and most efficient compression method. The patients who would benefit most from MC treatment should also be identified.",1,C
40,29981730,"Objectives: This study was conducted to determine hemodynamic and clinical tolerance under short-stretch compression therapy in elderly patients suffering from mixed-etiology leg ulcers. Design: Transversal observational study conducted in 25 hospitalized patients with a moderate peripheral arterial occlusive disease defined as an ankle-brachial pressure index>0.5, an ankle pressure of> 70mm Hg and a toe cuff pressure (TP)> 50mm Hg. Material and methods: Short-stretch bandages were applied daily with pressures from 20 to 30mm Hg. Ankle-brachial pressure, great toe laser Doppler flowmetry (LDF) and transcutaneous oxygen pressure (TcPO2) on dorsum of the foot were measured at baseline and after its removal at 24hours. Great toe LDF was also measured at 10minutes after bandage application. Compression pressure (CP) was measured with a sub-bandage device at baseline, at 10minutes and before bandage removal at 24hours. Clinical tolerance was evaluated taking into account the patient's pain and skin tolerance. Results: Mean age of patients was 80±15 years. Median duration of ulcers was 18 months. Hypertension was highly prevalent. One third of patients had diabetes. Toe pressure index and TcPO2 values did not significantly change under compression therapy (P=0.51 and P=0.09, respectively) whereas CP decreased significantly during 24hours. The loss of CP was significant 10minutes after bandage application (P<0.001). Nearly all ulcers were painful prior to placement of compression therapy and required level 1 analgesics. One patient required level 2 analgesic for pain relief. No increase in pain and no ischemic skin damage occurred under compression therapy. Conclusions: In elderly patients with mixed leg ulcers and with an absolute TP>50mm Hg, short-stretch compression of up to 30mm Hg does not adversely affect arterial flow and appears clinically well tolerated. Such bandages with appropriate levels of compression may aid ulcer healing by treating the venous part of the disease.","The aim of this study was to determine how short-stretch compression therapy affected blood flow and clinical tolerance (how much a patient could take). The study was conducted in elderly patients suffering from leg ulcers (sores) caused by various reasons. This observational study was conducted in 25 hospitalized patients with moderate peripheral aterial occlusive disease, or limited blood flow to lower limbs. Short-stretch bandages were applied daily with pressures ranging from 20 to 30mm Hg. Several health measures were taken before the bandages were applied and after 24 hours. These measures include: ankle-brachial pressure, great toe laser Doppler flowmetry (LDF), and transcutaneous oxygen pressure (TcPO2) on top of the foot. LDF measures blood flow. TcPO2 measures oxygen in the skin. Great toe LDF was also measured at 10 minutes after bandage application. Compression pressure (CP) was measured before bandages were placed on, at 10 minutes, and before bandage removal at 24 hours. Clinical tolerance was evaluated taking into account the patient's pain and skin tolerance. The average age of patients was 80 ± 15 years. Median (average) duration of ulcers was 18 months. Hypertension (high blood pressure) was highly common. One third of patients had diabetes. Toe pressure index and TcPO2 values did not significantly change under compression therapy. CP decreased significantly during 24 hours. The loss of CP was significant 10 minutes after bandages were put on. Nearly all ulcers were painful prior to placement of compression therapy and required level 1 (minor) pain medication. One patient required level 2 (more intense) pain medication for pain relief. No increase in pain and no skin damage due to low blood flow occurred under compression therapy. The authors conclude that elderly patients with mixed leg ulcers using short-stretch compression did not have adversely (negatively) affected blood flow to the heart. The treatment was clinically well tolerated. Bandages with appropriate levels of compression may aid ulcer healing by treating the diseased veins.",1,C
40,29981730,"Objectives: This study was conducted to determine hemodynamic and clinical tolerance under short-stretch compression therapy in elderly patients suffering from mixed-etiology leg ulcers. Design: Transversal observational study conducted in 25 hospitalized patients with a moderate peripheral arterial occlusive disease defined as an ankle-brachial pressure index>0.5, an ankle pressure of> 70mm Hg and a toe cuff pressure (TP)> 50mm Hg. Material and methods: Short-stretch bandages were applied daily with pressures from 20 to 30mm Hg. Ankle-brachial pressure, great toe laser Doppler flowmetry (LDF) and transcutaneous oxygen pressure (TcPO2) on dorsum of the foot were measured at baseline and after its removal at 24hours. Great toe LDF was also measured at 10minutes after bandage application. Compression pressure (CP) was measured with a sub-bandage device at baseline, at 10minutes and before bandage removal at 24hours. Clinical tolerance was evaluated taking into account the patient's pain and skin tolerance. Results: Mean age of patients was 80±15 years. Median duration of ulcers was 18 months. Hypertension was highly prevalent. One third of patients had diabetes. Toe pressure index and TcPO2 values did not significantly change under compression therapy (P=0.51 and P=0.09, respectively) whereas CP decreased significantly during 24hours. The loss of CP was significant 10minutes after bandage application (P<0.001). Nearly all ulcers were painful prior to placement of compression therapy and required level 1 analgesics. One patient required level 2 analgesic for pain relief. No increase in pain and no ischemic skin damage occurred under compression therapy. Conclusions: In elderly patients with mixed leg ulcers and with an absolute TP>50mm Hg, short-stretch compression of up to 30mm Hg does not adversely affect arterial flow and appears clinically well tolerated. Such bandages with appropriate levels of compression may aid ulcer healing by treating the venous part of the disease.","This study aimed to find blood flow and tolerance under less-stretchable compression treatment in elderly patients with mixed-cause leg sores or open wounds (leg ulcers). We studied 25 hospitalized patients with blocked blood vessels in the limbs. Less-stretchable compression bandages were applied daily. Ankle pressure, toe blood flow, and oxygen pressure at the skin on the top of the foot was measured at the start and after bandage removal at 24 hours. Big toe blood flow was measured 10 minutes after applying the bandage. Compression pressure was measured with a special device at the start, at 10 minutes, and before bandage removal at 24 hours. Patient tolerance was measured while considering the patient's pain and skin tolerance. Average age of patients was 80 years. Average duration of sores or open wounds was 18 months. High blood pressure was common. One third of patients had diabetes. Neither pressure nor oxygen to the toes changed with compression therapy, but compression pressure decreased over 24 hours. The compression pressure loss was noticeable 10 minutes after applying the bandage. Nearly all ulcers were painful before compression therapy and needed painkillers. One patient needed a more powerful pain killer. No increase in pain or blood-flow-related skin damage occured with compression. In elderly patients with mixed leg ulcers and sufficient toe pressure, less-stretchable compression does not negatively affect blood flow and appears tolerated. Bandages with appropriate compression may aid ulcer healing by treating the blood flow part of the disease.",2,C
40,24371979,"Compression therapy is the basic therapy in phlebology and lymphology. The pressure under the bandages has to exceed the intravenous pressure especially in standing position. Different compression materials such as short stretch systems, long stretch bandages and compression garments work differently on ambulatory venous hypertension, speed of reducing edema and arterial flow. Compression with high stiffness, inelastic materials is more effective than compression with low stiffness, elastic materials. These materials have to be placed correctly. Inelastic systems should be applied with high initial pressure because the pressure will loose at some time after walking. Even after one week of wearing, inelastic bandages keep higher resting and working pressure during walking than elastic bandages. However, more important is that they have lower resting pressure than elastic materials. Long stretch bandages and compression garments with great extensibility ensure low working pressure and higher resting pressure than short stretch systems.","Compression therapy is the basic therapy when treating the circulatory (blood) or lymphatic (body drainage) system. The pressure under the bandages has to exceed the intravenous pressure, especially when standing. Intravenous pressure is the pressure of blood in veins. Different compression materials work differently on ambulatory venous hypertension (excess pressure in veins). The material used can affect both the rate in which swelling is reduced and blood flow. Some compression materials include short stretch systems, long stretch bandages, and compression garments. Compression with very stiff materials is more effective than compression with low stiffness, elastic materials. These materials have to be placed correctly. Inelastic systems should be applied with high initial pressure. This is because the pressure will decrease after walking. Even after one week of wearing, inelastic bandages keep higher optimal pressure during walking than elastic bandages. More importantly, inelastic bandages have lower resting pressure than elastic materials. Long stretch bandages and compression garments that are able to stretch ensure low working pressure and higher resting pressure than short stretch systems.",1,C
40,23641263,"Aim: A review is given on the different tools of compression therapy and their mode of action. Methods: Interface pressure and stiffness of compression devices, alone or in combination can be measured in vivo. Hemodynamic effects have been demonstrated by measuring venous volume and flow velocity using MRI, Duplex and radioisotopes, venous reflux and venous pumping function using plethysmography and phlebodynamometry. Oedema reduction can be measured by limb volumetry. Results: Compression stockings exerting a pressure of ~20 mm Hg on the distal leg are able to increase venous blood flow velocity in the supine position and to prevent leg swelling after prolonged sitting and standing. In the upright position, an interface pressure of more than 50 mmHg is needed for intermittent occlusion of incompetent veins and for a reduction of ambulatory venous hypertension during walking. Such high intermittent interface pressure peaks exerting a ""massaging effect"" may rather be achieved by short stretch multilayer bandages than by elastic stockings. Conclusion: Compression is a cornerstone in the management of venous and lymphatic insufficiency. However, this treatment modality is still underestimated and deserves better understanding and improved educational programs, both for patients and medical staff.","This study aimed to evaluate different tools used in compression therapy and assess (measure) how they work. The levels of pressure and stiffness of the compression devices, alone or in combination with other materials, can be measured on living organisms. Hemodynamic (blood flow) effects have been demonstrated by measuring venous (vein) volume and flow velocity using several testing methods. Venous reflux (reverse flow) and venous pumping function have been demonstrated using bodily fluid volume and pressure measurements. Oedema (swelling) reduction can be measured by limb volumetry, a method of suspending a limb in water to determine the amount of water displaced. Compression stockings putting pressure on the lower leg are able to increase venous blood flow velocity while patients are lying down. Compression stockings are also able to prevent leg swelling after prolonged sitting and standing. In the upright position, specific pressure (>50 mmHg) is needed for intermittent (irregular) closing of abnormal veins and for pressure reduction during walking. High, intermittent pressure peaks exerting a ""massaging effect"" may be achieved by short stretch multilayer bandages instead of elastic stockings. Compression is important in the management of not fully functioning venous and lymphatic systems. However, using compression as a treatment is still underestimated. Compression deserves better understanding and improved educational programs for both patients and medical staff.",1,C
40,20483864,"Background: A too high resting pressure of compression devices is poorly tolerated and may cause skin defects, especially in patients with concomitant arterial occlusive disease. Aim: To investigate whether low compression pressure will improve venous pumping function in patients with venous incompetence. Material and methods: Venous pumping function was assessed in 20 patients with severe reflux in the great saphenous vein by measuring ejection fraction (EF) using strain-gauge plethysmography. Measurements were repeated after application of knee-high medical compression stockings and of inelastic bandages applied with a pressure of 20, 40 and 60 mmHg in the supine position. Results: EF was significantly reduced compared with healthy controls. Compression stockings exerting a median pressure of 27 mmHg (interquartile range [IQR] 25-29) in the supine and 30.5 mmHg (IQR 28.25-34.25) in the standing position produced a moderate, non-significant improvement of EF of 17%. Inelastic bandages with a resting pressure of 20.5 mmHg (IQR 20-22) in the supine position resulting in a standing pressure of 36 mmHg (IQR 33-40.75) led to a significant increase of EF of 61.5% (P < 0.01). A further increase of the resting pressure to 40 and 60 mmHg achieved an increase of the EF of 91% and 98%, respectively (P < 0.001). Conclusions: In patients with venous pumping failure, inelastic bandages produce a significant pressure-dependent increase of EF. A significant improvement in venous pumping function was achieved with inelastic bandages even at a resting pressure of 20 mmHg.","A too high resting pressure of compression devices, or when the device is just lying on the skin, is poorly tolerated and may cause skin defects. This is especially common in patients with concomitant arterial occlusive disease, blockage of an artery. The goal of this study was to investigate if low compression pressure improves vein pumping function in patients with defective veins. Vein pumping function was assessed (measured) in 20 patients with severe reflux, or the backup of blood, in the great saphenous vein in the leg. This was done by measuring ejection fraction (EF; percentage of blood leaving the heart when it contracts). Measurements were taken after application of knee-high medical compression stockings and of inelastic bandages applied while lying down. Results showed EF was significantly reduced compared with healthy controls. Compression stockings produced a moderate, non-significant improvement of EF. Inelastic bandages led to a significant increase of EF. Increased pressure of the inelastic bandages achieved an increase of the EF. The study concluded that, in patients with venous (vein) pumping failure, inelastic bandages produce a significant pressure-dependent increase of EF. This means that as pressure increases, EF increases. Significant improvement in venous pumping function was achieved with inelastic bandages, even at resting pressures.",1,C
40,20483864,"Background: A too high resting pressure of compression devices is poorly tolerated and may cause skin defects, especially in patients with concomitant arterial occlusive disease. Aim: To investigate whether low compression pressure will improve venous pumping function in patients with venous incompetence. Material and methods: Venous pumping function was assessed in 20 patients with severe reflux in the great saphenous vein by measuring ejection fraction (EF) using strain-gauge plethysmography. Measurements were repeated after application of knee-high medical compression stockings and of inelastic bandages applied with a pressure of 20, 40 and 60 mmHg in the supine position. Results: EF was significantly reduced compared with healthy controls. Compression stockings exerting a median pressure of 27 mmHg (interquartile range [IQR] 25-29) in the supine and 30.5 mmHg (IQR 28.25-34.25) in the standing position produced a moderate, non-significant improvement of EF of 17%. Inelastic bandages with a resting pressure of 20.5 mmHg (IQR 20-22) in the supine position resulting in a standing pressure of 36 mmHg (IQR 33-40.75) led to a significant increase of EF of 61.5% (P < 0.01). A further increase of the resting pressure to 40 and 60 mmHg achieved an increase of the EF of 91% and 98%, respectively (P < 0.001). Conclusions: In patients with venous pumping failure, inelastic bandages produce a significant pressure-dependent increase of EF. A significant improvement in venous pumping function was achieved with inelastic bandages even at a resting pressure of 20 mmHg.","A too high pressure from compression devices is poorly tolerated and may causes skin damage, espeically in those with blocked blood vessels in limbs. We aim to check if low compression pressure may improve blood vessel pumping in patients with impaired blood vessels. Blood vessel pumping function was measured in 20 patients with severely impaired blood flow in the body's largest vein. Measurements were repeated after using knee-high compression socks and inelastic bandages applied with low, medium, and high pressure while lying down. The amount of blood pumped was compared to healthy patients. Compression socks exerted an average pressure of 27 mmHg while lying down and slightly higher pressure of 30.5 mmHg while standing. This led to a minor improvement in the amount of blood pumped, or ejection fraction (EF), by 17%. Inelastic bandages with a pressure of 20.5 mmHg while lying down led to a standing pressure of 36 mmHg, which led to a major increase in EF by 61.5%. A larger increase of pressure to 40 and 60 mmHg led to an increase in the amount of blood pumped of 91% and 98%, respectively. In those with impaired blood vessel pumping, inelastic bandages lead to a pressure-dependent increase of EF. Inelastic bandages even with a pressure of 20 mmHg improved blood vessel pumping.",2,C
40,21286870,"Although compression therapy was initially described over 2,000 years ago (Felty and Rooke Semin Vasc Surg Mar 18:36-40, 1), several patients with edema do not receive appropriate compression therapy. Instead, most patients with edema are treated primarily with diuretics. Compression therapy is the cornerstone of treatment of venous edema and lymphatic disorders. Compression therapy decreases the foot and leg volume and reduces venous reflux and venous hypertension. Compression can be achieved by multiple different modalities, such as inelastic bandages; multilayered wraps; short, medium, and long stretch bandages; graduated compression stockings; and pneumatic compression devices. The major criticism of compression therapy is poor patient compliance. Compliance can be improved by selecting appropriate compression therapy tailored to the needs of the individual patient and by providing adequate patient education.","Although compression therapy was first described over 2,000 years ago, many patients with edema do not receive appropriate compression therapy. Edema is swelling within the body due to excess fluid build up. Instead, most patients with swelling are treated primarily with diuretics (drugs that promote urination). Compression therapy is a vital treatment for venous edema (vein swelling) and lymphatic disorders (disorders of the body's drainage system). Compression therapy decreases the foot and leg volume. Compression therapy also reduces venous reflux and venous hypertension. Venous reflux is abnormal back up of blood in the veins. Venous hypertension is abnormal blood pressure. Compression can be achieved by multiple different methods. These methods include inelastic bandages; multilayered wraps; short, medium, and long stretch bandages; graduated (pressure-varying) compression stockings; and pneumatic (inflatable) compression devices. The major criticism of compression therapy is that patients do not always comply with orders. Compliance with doctor orders can be increased by selecting the right compression therapy for the individual patient. Patients should also be provided with adequate education on the treatment.",1,C
40,14593461,"Chronic venous insufficiency (CVI) has a significant socioeconomic impact. The existent venous hypertension and the subsequent capillary hypertension result in trophic skin damage culminating in an ulcer. Venous ulcers affect 1-3% of the adult population. Compression therapy provides the basis for noninvasive treatment of CVI. It can be applied alone or in combination with invasive strategies. A variety of materials are available for phlebological compression therapy in the form of compression bandages and compression hosiery. Knowledge of the different qualities of the compression materials and their mode of action is important in choosing the correct means of compression with regard to clinical findings and the patient's needs. As far as possible, the compression method applied should be monitored for any loss of effectivity during regular follow-up examinations of the patients. The following article deals with this topic. A new option for compression therapy of crural ulcers is presented and the possibility for checking the effectiveness of the compression stockings during outpatient","Chronic venous insufficiency (CVI), or malfunctioning veins, has a significant impact on a person's social and economic status. Continual pressure in the circulatory system can result in skin damage before forming an ulcer (sore). Venous ulcers (sores from irregular blood flow) affect 1-3% of the adult population. Compression therapy provides the basis for noninvasive (nonsurgical) treatment of CVI. It can be applied alone or in combination with invasive (e.g. surgery) strategies. A variety of materials are available for compression therapy in the form of bandages or hosiery (legwear). Knowledge of the different compression materials and how they work is important in choosing the correct compression treatment to meet health goals and patient needs. The compression method applied should be monitored for any decrease in effectiveness. The following article deals with this topic. This paper presents a new option for compression therapy of crural (leg) ulcers. This paper also checks the effectiveness of the compression stockings during outpatient treatment.",1,C
40,10845240,"Venous (gravitational) leg ulcers are unsightly, sometimes painful and often difficult to heal. They are associated with incompetence of valves in the deep leg veins and venous hypertension. The main approaches in the management of venous leg ulcers have been to reduce the 'back pressure' in the veins by surgical removal of any varicose veins, postural drainage (elevation of the legs when the patient is lying or seated), and use of compression therapy with bandages, hosiery or intermittent pneumatic compression. In this article, we review the efficacy and discuss correct use of compression therapy.","Venous (gravitational) leg ulcers (leg sores) are unappealing to the eye, sometimes painful, and often difficult to heal. They are associated with incompetence of valves in the deep leg veins and venous hypertension (high blood pressure in veins). There are several approaches used in the management of venous leg ulcers. These approaches focus on reducing the 'back pressure' or 'reverse pressure' in the veins. These approaches include surgical removal of any varicose veins (twisted, enlarged veins), postural drainage (elevation of the legs when the patient is lying or seated), and use of compression therapy with bandages, hosiery (legwear) or intermittent (periodic) pneumatic (inflatable) compression. This article reviews the success and correct use of compression therapy.",1,C
40,10845240,"Venous (gravitational) leg ulcers are unsightly, sometimes painful and often difficult to heal. They are associated with incompetence of valves in the deep leg veins and venous hypertension. The main approaches in the management of venous leg ulcers have been to reduce the 'back pressure' in the veins by surgical removal of any varicose veins, postural drainage (elevation of the legs when the patient is lying or seated), and use of compression therapy with bandages, hosiery or intermittent pneumatic compression. In this article, we review the efficacy and discuss correct use of compression therapy.","Venous leg ulcers (leg sores from veins) are ugly, sometimes painful, and often difficult to heal. Venous leg ulcers are linked to high blood pressure in impaired leg veins. Main treatments of venous leg ulcers have been to reduce pressure in veins by surgical removal of enlarged veins, drainage using gravity, and compression from bandages, legwear, or inflatable leg sleeves. In this article, we review the effectiveness and correct use of compression therapy.",2,C
40,11034749,"Background: Up to 1% of adults will suffer from leg ulceration at some time. The majority of leg ulcers are venous in origin and are caused by high pressure in the veins due to blockage or weakness of the valves in the veins of the leg. Prevention and treatment of venous ulcers is aimed at reducing the pressure either by removing / repairing the veins, or by applying compression bandages / stockings to reduce the pressure in the veins. The vast majority of venous ulcers are healed using compression bandages. Once healed they often recur and so it is customary to continue applying compression in the form of bandages, tights, stockings or socks in order to prevent recurrence. Compression bandages or hosiery (tights, stockings, socks) are often applied for ulcer prevention. Objectives: To assess the effects of compression hosiery (socks, stockings, tights) or bandages in preventing the recurrence of venous ulcers. To determine whether there is an optimum pressure/type of compression to prevent recurrence of venous ulcers. Main results: No trials compared recurrence rates with and without compression. One trial (300 patients) compared high (UK Class 3) compression hosiery with moderate (UK Class 2) compression hosiery. A intention to treat analysis found no significant reduction in recurrence at five years follow up associated with high compression hosiery compared with moderate compression hosiery (relative risk of recurrence 0.82, 95% confidence interval 0.61 to 1.12). This analysis would tend to underestimate the effectiveness of the high compression hosiery because a significant proportion of people changed from high compression to medium compression hosiery. Compliance rates were significantly higher with medium compression than with high compression hosiery. One trial (166 patients) found no difference in recurrence between two types of medium (UK Class 2) compression hosiery (relative risk of recurrence with Medi was 0.74, 95% confidence interval 0.45 to 1.2). Both trials reported that not wearing compression hosiery was strongly associated with ulcer recurrence and this is circumstantial evidence that compression reduces ulcer recurrence. No trials were found which evaluated compression bandages for preventing ulcer recurrence. Reviewer's conclusions: No trials compared compression with vs no compression for prevention of ulcer recurrence. Not wearing compression was associated with recurrence in both studies identified in this review. This is circumstantial evidence of the benefit of compression in reducing recurrence. Recurrence rates may be lower in high compression hosiery than in medium compression hosiery and therefore patients should be offered the strongest compression with which they can comply. Further trials are needed to determine the effectiveness of hosiery prescribed in other settings, i.e. in the UK community, in countries other than the UK.","Up to 1% of adults will suffer from leg ulceration (leg sores) at some time. The majority of leg ulcers come from issues within veins. They are caused by high pressure in the veins due to blockage or weakness of the valves in the veins of the leg that prevent backflow or reverse blood flow. Prevention and treatment of venous ulcers is aimed at reducing the pressure. This is achieved by removing or repairing the veins. It can also be achieved by applying compression material to reduce vein pressure. Most venous ulcers are healed using compression bandages. Once healed, they often recur or reappear. Therefore, it is common to continue applying compression in the form of bandages, tights, stockings, or socks to prevent recurrence. Compression bandages or hosiery (tights, stockings, socks) are often applied for ulcer prevention. The aim of this study was to assess (measure) the effects of compression hosiery (socks, stockings, tights) or bandages in preventing the recurrence of venous ulcers. The study also aimed to determine whether there is an optimum pressure/type of compression to prevent recurrence of venous ulcers. None of the reviewed reports compared recurrence rates with and without compression. One trial (300 patients) compared high compression hosiery with moderate compression hosiery. No significant reduction in recurrence at five years follow up was associated with high compression hosiery when compared with moderate compression hosiery. This may underestimate the effectiveness of the high compression hosiery as a large proportion of people changed from high compression to medium compression hosiery. Compliance rates, or the amount of people who used compression consistently, were higher with medium compression than with high compression hosiery. One trial found no difference in recurrence between two types of medium compression hosiery. Both trials reported that not wearing compression hosiery was strongly associated with ulcer recurrence. This is circumstantial evidence that compression reduces ulcer recurrence. No studies that evaluated compression bandages for preventing ulcer recurrence were found. The authors concluded on several points following this evidence review. First, no trials compared compression with no compression for prevention of reoccurring ulcers. Second, not wearing compression was associated with ulcer reoccurrence. This is circumstantial evidence of the benefit of compression in reducing recurrence. Third, recurrence rates may be lower if patients wear high compression hosiery over medium compression hosiery. Because of this, patients should be offered the strongest compression they can tolerate. Lastly, further studies are needed to understand the effectiveness of hosiery utilized in alternative locations.",1,C
40,21944912,"Objectives: This study was conducted to define bandage pressures that are safe and effective in treating leg ulcers of mixed arterial-venous etiology. Methods: In 25 patients with mixed-etiology leg ulcers who received inelastic bandages applied with pressures from 20 to 30, 31 to 40, and 41 to 50 mm Hg, the following measurements were performed before and after bandage application to ensure patient safety throughout the investigation: laser Doppler fluxmetry (LDF) close to the ulcer under the bandage and at the great toe, transcutaneous oxygen pressure (TcPo(2)) on the dorsum of the foot, and toe pressure. Ejection fraction (EF) of the venous pump was performed to assess efficacy on venous hemodynamics. Results: LDF values under the bandages increased by 33% (95% confidence interval [CI], 17-48; P < .01), 28% (95% CI, 12-45; P < .05), and 10% (95% CI, -7 to 28), respectively, under the three pressure ranges applied. At toe level, a significant decrease in flux of -20% (95% CI, -48 to 9; P < .05) was seen when bandage pressure >41 mm Hg. Toe pressure values and TcPo(2) showed a moderate increase, excluding a restriction to arterial perfusion induced by the bandages. Inelastic bandages were highly efficient in improving venous pumping function, increasing the reduced ejection fraction by 72% (95% CI, 50%-95%; P < .001) under pressure of 21 to 30 mm Hg and by 103% (95% CI, 70%-128%; P < .001) at 31 to 40 mm Hg. Conclusions: In patients with mixed ulceration, an ankle-brachial pressure index >0.5 and an absolute ankle pressure of >60 mm Hg, inelastic compression of up to 40 mm Hg does not impede arterial perfusion but may lead to a normalization of the highly reduced venous pumping function. Such bandages are therefore recommended in combination with walking exercises as the basic conservative management for patients with mixed leg ulcers.","This study was conducted to find what bandage pressures are safe and effective in treating leg ulcers (leg sores) caused by various vein disorders. The study evaluated 25 patients with leg ulcers who received inelastic bandages applied with various pressures (20 to 30, 31 to 40, and 41 to 50 mm Hg). Several measurements were performed before and after bandage application to ensure patient safety throughout the investigation. These measurements included laser Doppler fluxmetry (LDF), transcutaneous oxygen pressure (TcPo(2)), and toe pressure. LDF measures blood flow. TcPO2 measures oxygen in the skin. Ejection fraction (EF) of the venous pump, or the blood flowing from the heart, was performed to assess blood flow from the heart every time it contracts. LDF values under the bandages increased at all three pressure ranges. At toe level, a significant decrease in flux (blood buildup) was seen when bandage pressure was >41 mm Hg. Toe pressure values and TcPo(2) moderately increased. Inelastic bandages were highly efficient in improving venous pumping function. This is when a machine compresses veins to force blood flow to the heart. The study concluded that patients with mixed ulceration using inelastic compression of up to 40 mm Hg do not experience impeded (blocked) arterial perfusion. Arterial perfusion is when a patient has their blood drawn, has the blood mixed with medication, and has the blood pumped back into the host. However, this inelastic compression may lead to a regularly decreased venous pumping function. These bandages are recommended in combination with walking exercises as the basic treatment for patients with mixed leg ulcers.",1,C
46,33805605,"Cystic fibrosis is a monogenic, autosomal, recessive disease characterized by an alteration of chloride transport caused by mutations in the CFTR (Cystic Fibrosis Transmembrane Conductance Regulator) gene. The loss of Phe residue in position 508 (ΔF508-CFTR) causes an incorrect folding of the protein causing its degradation and electrolyte imbalance. CF patients are extremely predisposed to the development of a chronic inflammatory process of the bronchopulmonary system. When the cells of a tissue are damaged, the immune cells are activated and trigger the production of free radicals, provoking an inflammatory process. In addition to routine therapies, today drugs called correctors are available for mutations such as ΔF508-CFTR as well as for others less frequent ones. These active molecules are supposed to facilitate the maturation of the mutant CFTR protein, allowing it to reach the apical membrane of the epithelial cell. Matrine induces ΔF508-CFTR release from the endoplasmic reticulum to cell cytosol and its localization on the cell membrane. We now have evidence that Matrine and Lumacaftor not only restore the transport of mutant CFTR protein, but probably also counteract the inflammatory process by improving the course of the disease.","Cystic fibrosis is a deadly disease inherited from both parents characterized by changes in a compound (chloride) movement caused by genetic changes in the Cystic Fibrosis Transmembrane Conductance Regulator (CFTR) gene. One gene change, ΔF508-CFTR, causes the resulting protein the gene creates to fold incorrectly, which leads to its destruction and mineral imbalance. CF patients are at very high risk of developing long-term inflammation (redness and swelling from fighting an infection) of part of the lungs. When tissue cells are damaged, immune cells are stimulated and produce unstable molecules, prompting the immune system to defend the body. In addition to normal treatments, drugs called correctors are available for changes like ΔF508-CFTR and other less common changes. Correctors help the CFTR protein form the right shape to move to the surface of the epithelial cell (cell that covers outer surface of the internal organs). Matrine, a drug, starts ΔF508-CFTR release from the cell's transportation system through the cell liquid to the surface. We now have proof that Matrine and Lumacaftor, another drug, not only allow the changed CFTR to move, but probably neutralize inflammation by improving the course of the disease.",1,B
46,33331662,"Background: Cystic fibrosis (CF) is a common life-shortening genetic condition caused by a variant in the cystic fibrosis transmembrane conductance regulator (CFTR) protein. A class II CFTR variant F508del (found in up to 90% of people with CF (pwCF)) is the commonest CF-causing variant. The faulty protein is degraded before reaching the cell membrane, where it needs to be to effect transepithelial salt transport. The F508del variant lacks meaningful CFTR function and corrective therapy could benefit many pwCF. Therapies in this review include single correctors and any combination of correctors and potentiators.","Cystic fibrosis (CF) is a common life-shortening inherited condition caused by a change in the cystic fibrosis transmembrane conductance regulator (CFTR) protein. The most common change, F508del, is a protein processing change, and up to 90% of people with CF have this change. The faulty protein is destroyed before reaching the cell membrane, where it needs to be to move salt across epithelial cells (cells that cover outer surface of the internal organs). The F508del change doesn't work properly and treatment to fix this could help many people with CF. We review single correctors, which help the CFTR protein form the right shape to move to the cell surface, and any combination of correctors and potentiators, which are CFTR modulators (influencers) that hold the gate to the CFTR channel open so chloride can flow through the cell membrane.",1,B
46,33038767,"Cystic Fibrosis (CF) is an autosomal recessive disease caused by mutations in the CF transmembrane regulator (CFTR) gene, which encodes a chloride channel located at the apical surface of epithelial cells. Unsaturated Fatty Acid (UFA) deficiency has been a persistent observation in tissues from patients with CF. However, the impacts of such deficiencies on the etiology of the disease have been the object of intense debates. The aim of the present review is first to highlight the general consensus on fatty acid dysregulations that emerges from, sometimes apparently contradictory, studies. In a second step, a unifying mechanism for the potential impacts of these fatty acid dysregulations in CF cells, based on alterations of membrane biophysical properties (known as lipointoxication), is proposed. Finally, the contribution of lipointoxication to the progression of the CF disease and how it could affect the efficacy of current treatments is also discussed.","Cystic fibrosis (CF) is a disease inherited from both parents caused by genetic changes in the CF transmembrane conductance Regulator (CFTR) gene, which produces a compound (chloride) channel at the surface of epithelial cells (cells that cover outer surface of the internal organs). Not enough Unsaturated Fatty Acid (UFA) has continually been seen in CF patients. Scientists do not agree on the role of not enough UFA in causing CF. We aim to discuss current beliefs on improper functioning of UFAs that comes from studies that sometime disagree. Next, we suggest a way in which possible improper functioning of UFAs impacts CF cells, based on changes in the biological and physical characteristics of the cell membrane (known as lipointoxication). Finally, we discuss how lipointoxication might play a role in the progression of CF disease and how it could affect how well current treatments work.",1,B
46,33038767,"Cystic Fibrosis (CF) is an autosomal recessive disease caused by mutations in the CF transmembrane regulator (CFTR) gene, which encodes a chloride channel located at the apical surface of epithelial cells. Unsaturated Fatty Acid (UFA) deficiency has been a persistent observation in tissues from patients with CF. However, the impacts of such deficiencies on the etiology of the disease have been the object of intense debates. The aim of the present review is first to highlight the general consensus on fatty acid dysregulations that emerges from, sometimes apparently contradictory, studies. In a second step, a unifying mechanism for the potential impacts of these fatty acid dysregulations in CF cells, based on alterations of membrane biophysical properties (known as lipointoxication), is proposed. Finally, the contribution of lipointoxication to the progression of the CF disease and how it could affect the efficacy of current treatments is also discussed.","Cystic Fibrosis (CF) is an inherited disease, which leads to mucus buildup in many organs, caused by mutations in a specific gene, which encodes a channel located at the surface of boundary cells. Lack of Unsaturated Fatty Acid (UFA), a certain form of fat, has been a common observation for patients with CF. However, the effects of such deficiencies on the causes of the disease are debated. This review aims to highlight the general consensus on fatty acid impairment that comes from, sometimes contradictory, studies. In a second step, how the possible effects of this fatty acid impairment occur in CF cells, based on changes in cell boundary properties, is explored. Finally, how fat toxicity contribtues to progression of the CF disease and affects the success of current treatments is discussed.",2,B
46,32666337,"Human primary bronchial epithelial cells differentiated in vitro represent a valuable tool to study lung diseases such as cystic fibrosis (CF), an inherited disorder caused by mutations in the gene coding for the Cystic Fibrosis Transmembrane Conductance Regulator. In CF, sphingolipids, a ubiquitous class of bioactive lipids mainly associated with the outer layer of the plasma membrane, seem to play a crucial role in the establishment of the severe lung complications. Nevertheless, no information on the involvement of sphingolipids and their metabolism in the differentiation of primary bronchial epithelial cells are available so far. Here we show that ceramide and globotriaosylceramide increased during cell differentiation, whereas glucosylceramide and gangliosides content decreased. In addition, we found that apical plasma membrane of differentiated bronchial cells is characterized by a higher content of sphingolipids in comparison to the other cell membranes and that activity of sphingolipids catabolic enzymes associated with this membrane results altered with respect to the total cell activities. In particular, the apical membrane of CF cells was characterized by high levels of ceramide and glucosylceramide, known to have proinflammatory activity. On this basis, our data further support the role of sphingolipids in the onset of CF lung pathology.","Cells on the surface of the bronchi (two tubes that carry air to your lungs) extracted from humans and grown in the lab are a valuable tool to study lung disease such as cystic fibrosis (CF), an inherited disease caused by a change in the Cystic Fibrosis Transmembrane Conductance Regulator (CFTR) protein which helps transport chloride. In CF, sphingolipids, a common type of lipids found in cell surroundings, seem to play an important role in the onset of serious lung problems. Yet information is lacking on the involvement of sphingolipids and their metabolism (digestion) in the process by which cells on the surface of the bronchi change their type. Two types of sphingolipids increased and two decreased as dividing cells changed type. Cells on the surface of the bronchi have more sphingolipids compared to surfaces of other cells. The surface of CF cells had high levels of two sphingolipids, known to promote inflammation (redness and swelling from fighting an infection). Our results support the role of sphingolipids in the onset of CF lung problems.",1,B
46,32637102,"Cystic fibrosis patients display multi-organ system dysfunction (e.g. pancreas, gastrointestinal tract, and lung) with pathogenesis linked to a failure of Cl- secretion from the epithelial surfaces of these organs. If unmanaged, organ dysfunction starts early and patients experience chronic respiratory infection with reduced lung function and a failure to thrive due to gastrointestinal malabsorption. Early mortality is typically caused by respiratory failure. In the past 40 years of newborn screening and improved disease management have driven the median survival up from the mid-teens to 43-53, with most of that improvement coming from earlier and more aggressive management of the symptoms. In the last decade, promising pharmacotherapies have been developed for the correction of the underlying epithelial dysfunction, namely, Cl- secretion. A new generation of systemic drugs target the mutated Cl- channels in cystic fibrosis patients and allow trafficking of the immature mutated protein to the cell membrane (correctors), restore function to the channel once in situ (potentiators), or increase protein levels in the cells (amplifiers). Restoration of channel function prior to symptom development has the potential to significantly change the trajectory of disease progression and their evidence suggests that a modest restoration of Cl- secretion may delay disease progression by decades. In this article, we review epithelial vectorial ion and fluid transport, its quantification and measurement as a marker for cystic fibrosis ion transport dysfunction, and highlight some of the recent therapies targeted at the dysfunctional ion transport of cystic fibrosis.","People with cystic fibrosis have organs that do not function correctly (e.g., pancreas, digestive system, and lung) linked to not enough CI- (chloride) production and release from epithelial cells (cells that cover outer surface of the internal organs). If not treated, organs quickly stop working and patients have long-term respiratory infection with decreased lung function and a decline in health from poor absorption of food. Early death usually happens when lungs can't get enough oxygen into the blood. The past 40 years of newborn checks and better ways to treat disease have increased the age that half of patients are still alive from the mid-teens to 43-53, with most of the increase due to earlier and more aggressive management of the symptoms. In the last 10 years, drugs have been made to correct underlying problems in epithelial cells, mainly, CI- production and release. New drugs for cystic fibrosis patients that work throughout the body target the changed atom (Cl-; chloride) channels and help the changed protein to move to the cell surface (correctors), hold the gate to the channel open so chloride can flow through the cell membrane (potentiators), or increase protein levels in the cells (amplifiers). Restoring channel function before symptoms develop could change the course of the disease and a small repair of Cl- production and release may delay disease worsening by decades. We look at movement of ions (charged atoms) and fluids in epithelial cells, how to count and measure this movement as a sign of cystic fibrosis transport problems, and discuss recent treatments for cystic fibrosis transport problems.",1,B
46,31401006,"Cystic fibrosis (CF) is an autosomal recessive disease caused by the loss of function of the cystic fibrosis transmembrane conductance regulator (CFTR) protein which primarily acts as a chloride channel. CFTR has mainly been studied in epithelial cells although it is also functional and expressed in other cell types including endothelial cells. The present review summarizes current knowledge on the role of the endothelium in CF. More specifically, this review highlights the role of endothelial cells in CF in acting as a semipermeable barrier, as a key regulator of angiogenesis, coagulation, the vascular tone and the inflammatory responses. It could contribute to different aspects of the disease including cardiovascular symptoms, excessive blood vessel formation, pulmonary and portal hypertension and CF-related diabetes. Despite the important role of vascular endothelium in many biological processes, it has largely been under investigated in CF.","Cystic fibrosis (CF) is a disease inherited from both parents caused when the CF transmembrane conductance Regulator (CFTR) protein no longer functions, which acts as a compound (chloride) channel. Scientists have mostly studied CFTR in epithelial cells (cells that cover outer surface of the internal organs) although it also works and is seen in other cell types including endothelial cells (cells that cover the blood vessel inner surface). This review summarizes what is known on the role of endothelium in CF. We summarize the role of endothelial cells in CF in acting as a barrier that allows only some things to pass through, partly controlling formation of new blood vessels, clotting, blood vessel constriction, and inflammation. Endothelial cells could play a role in different parts of the disease including heart symptoms, formation of too many blood vessels, two kinds of high blood pressure and CF-related diabetes. Vascular endothelium (the inner cellular lining of arteries, veins, and capillaries) is important to many processes in the body, but it has not been studied enough in CF.",1,B
46,31401006,"Cystic fibrosis (CF) is an autosomal recessive disease caused by the loss of function of the cystic fibrosis transmembrane conductance regulator (CFTR) protein which primarily acts as a chloride channel. CFTR has mainly been studied in epithelial cells although it is also functional and expressed in other cell types including endothelial cells. The present review summarizes current knowledge on the role of the endothelium in CF. More specifically, this review highlights the role of endothelial cells in CF in acting as a semipermeable barrier, as a key regulator of angiogenesis, coagulation, the vascular tone and the inflammatory responses. It could contribute to different aspects of the disease including cardiovascular symptoms, excessive blood vessel formation, pulmonary and portal hypertension and CF-related diabetes. Despite the important role of vascular endothelium in many biological processes, it has largely been under investigated in CF.","Cystic fibrosis (CF) is an inherited disease caused by loss of function of a special cell boundary channel and leads to mucus buildup in many organs. This special cell boundary channel has been studied in boundary cells. However, it is also present in other cell types like endothelial cells (cells that line blood vessels and empty areas of the body). This review article summarizes current knowledge on the role of the endothelium in CF. More specifically, this review highlights how endothelial cells in CF act as a barrier which regulates blood vessel creation, blood clotting, vessel activity, and inflammation. It could contribute to different aspects of the disease like heart-related symptoms, excessive blood vessel formation, lung and vein-related high blood pressure and CF-related diabetes. Despite the important role of blood vessel boundary cells in many biological processes, it has largely been under-investigated in CF.",2,B
46,30825185,"Background and purpose: Cystic fibrosis (CF) is a lethal autosomal recessive genetic disease that originates from the defective function of the CF transmembrane conductance regulator (CFTR) protein, a cAMP-dependent anion channel involved in fluid transport across epithelium. Because small synthetic transmembrane anion transporters (anionophores) can replace the biological anion transport mechanisms, independent of genetic mutations in the CFTR, such anionophores are candidates as new potential treatments for CF. Experimental approach: In order to assess their effects on cell physiology, we have analysed the transport properties of five anionophore compounds, three prodigiosines and two tambjamines. Chloride efflux was measured in large uni-lamellar vesicles and in HEK293 cells with chloride-sensitive electrodes. Iodide influx was evaluated in FRT cells transfected with iodide-sensitive YFP. Transport of bicarbonate was assessed by changes of pH after a NH4 + pre-pulse using the BCECF fluorescent probe. Assays were also carried out in FRT cells permanently transfected with wild type and mutant human CFTR. Key results: All studied compounds are capable of transporting halides and bicarbonate across the cell membrane, with a higher transport capacity at acidic pH. Interestingly, the presence of these anionophores did not interfere with the activation of CFTR and did not modify the action of lumacaftor (a CFTR corrector) or ivacaftor (a CFTR potentiator). Conclusion and implications: These anionophores, at low concentrations, transported chloride and bicarbonate across cell membranes, without affecting CFTR function. They therefore provide promising starting points for the development of novel treatments for CF.","Cystic fibrosis (CF) is a deadly disease inherited from both parents that is caused when the CF transmembrane conductance regulator (CFTR) protein does not work correctly, a channel involved with movement of ions (charged atoms) and fluids in epithelial cells (cells that cover the outer surface of the internal organs). Anionophores, which are small human-made substances that move negative ions (anions) across membranes, can replace biological anion movement regardless of gene changes in the CFTR and are possible treatments for CF. We looked at the movement properties of five anionophore substances. We measured the flow of chloride out of the cell. We measured the flow of iodide into the cell. We measured the movement of bicarbonate (compound). We saw that all five compounds are able to move halides and bicarbonate (compounds) across the cell membrane, with more moved in acidic conditions. Anionophores did not change CFTR activation and did not change the effects of a lumacaftor (corrector that helps the changed protein to move to the cell surface) or ivacaftor (potentiator that holds the gate to the channel open so chloride can flow through the cell membrane). We conclude that anionophores, at low concentrations, moved chloride and bicarbonate (compounds) across cell membranes, without changing CFTR function. New CF treatments could be developed starting from anionophores.",1,B
46,30349480,"Cystic fibrosis (CF) is caused by mutations in the cystic fibrosis transmembrane conductance regulator (CFTR) gene and remains the most common life-shortening diseases affecting the exocrine organs. The absence of this channel results in an imbalance of ion concentrations across the cell membrane and results in more abnormal secretion and mucus plugging in the gastrointestinal tract and in the lungs of CF patients. The direct introduction of fully functional CFTR by gene therapy has long been pursued as a therapeutical option to restore CFTR function independent of the specific CFTR mutation, but the different clinical trials failed to propose persuasive evidence of this strategy. The last ten years has led to the development of new pharmacotherapies which can activate CFTR function in a mutation-specific manner. Although approximately 2,000 different disease-associated mutations have been identified, a single codon deletion, F508del, is by far the most common and is present on at least one allele in approximately 70% of the patients in CF populations. This strategy is limited by chemistry, the knowledge on CFTR and the heterogenicity of the patients. New research efforts in CF aim to develop other therapeutical approaches to combine different strategies. Targeting RNA appears as a new and an important opportunity to modulate dysregulated biological processes. Abnormal miRNA activity has been linked to numerous diseases, and over the last decade, the critical role of miRNA in regulating biological processes has fostered interest in how miRNA binds to and interacts explicitly with the target protein. Herein, this review describes the different strategies to identify dysregulated miRNA opens up a new concept and new opportunities to correct CFTR deficiency. This review describes therapeutic applications of antisense techniques currently under investigation in CF.","Changes in the cystic fibrosis transmembrane conductance regulator (CFTR) gene cause cystic fibrosis (CF), one of the most common life-shortening diseases affecting organs that make and release substances into transporting ducts. The absence of this protein from the CFTR gene causes an imbalance of ion (charged atom) concentrations across the cell membrane and causes release and buildup of abnormally high levels of mucus in the digestive tract and lungs of CF patients. Scientists have tried transferring working CFTR into a patient's cells regardless of the specific CFTR gene change, but patient trials were not successful. New drugs that can stimulate CFTR function based on the specific gene change have been made in the last ten years. Of the roughly 2,000 different gene changes linked to CF, the most common change is F508del, a deletion of a sequence of three consecutive gene compounds, and about 70% of people with CF have at least one form of it. Chemistry, CFTR knowledge, and genetic differences of patients limit the ability to stimulate CFTR function based on the specific gene change. New CF research aims to develop other treatments that combine different approaches. Looking at RNA (which changes the genetic information of DNA into proteins) is a new and important chance to fix incorrectly functioning biological processes. Scientists have linked abnormal microRNA (miRNA; short type of non-coding RNA that does not encode a protein) to many diseases, and over the last decade, scientists have developed interest in how miRNA interacts with specific proteins. We describe different ways to find incorrectly functioning miRNA that might help correct lack of CFTR. This review describes RNA treatments currently being studied in CF.",1,B
46,29475947,"The cystic fibrosis transmembrane conductance regulator (CFTR) is a Cl- channel that apparently has evolved from an ancestral active transporter. Key to the CFTR's switch from pump to channel function may have been the appearance of one or more ""lateral portals."" Such portals connect the cytoplasm to the transmembrane channel pore, allowing a continuous pathway for the electrodiffusional movement of Cl- ions. However, these portals remain the least well-characterized part of the Cl- transport pathway; even the number of functional portals is uncertain, and if multiple portals do exist, their relative functional contributions are unknown. Here, we used patch-clamp recording to identify the contributions of positively charged amino acid side chains located in CFTR's cytoplasmic transmembrane extensions to portal function. Mutagenesis-mediated neutralization of several charged side chains reduced single-channel Cl- conductance. However, these same mutations differentially affected channel blockade by cytoplasmic suramin and Pt(NO2)42- anions. We considered and tested several models by which the contribution of these positively charged side chains to one or more independent or non-independent portals to the pore could affect Cl- conductance and interactions with blockers. Overall, our results suggest the existence of a single portal that is lined by several positively charged side chains that interact electrostatically with both Cl- and blocking anions. We further propose that mutations at other sites indirectly alter the function of this single portal. Comparison of our functional results with recent structural information on CFTR completes our picture of the overall molecular architecture of the Cl- permeation pathway.","The cystic fibrosis transmembrane conductance regulator (CFTR) is a chloride (Cl-) channel descended from an active transporter (requiring energy for movement). One or more ""lateral portals"" may have appeared, switching the active pump to a channel. ""Lateral portals"" allow Cl- ions (charged atoms) to move under the influence of an electric field. These portals are the least understood part of Cl- movement. Scientists do not know the number of working portals, and if more than one, which portal does what. We used a cell test to find out how positively charged side chemical groups in CFTR contribute to how the portal works. Neutralization of many charged side chemical groups caused by gene sequence changes reduced movement of Cl- through a single channel. However, these gene sequence changes affected channel blockade by negatively charged atoms differently. We used many models to look at ways these positively charged chemical groups changed one or more portals and its effect on Cl- movement and interaction with blockers. Our results suggest one portal is lined by many positively charged side chemical groups that interact with both CL- and other blocking negatively charged ions. We further suggest that gene sequence changes at other sides change how well the single portal works. We compared our results with recent information on CFTR structure to understand the pathway of Cl- transport at a molecule level.",1,B
46,29475947,"The cystic fibrosis transmembrane conductance regulator (CFTR) is a Cl- channel that apparently has evolved from an ancestral active transporter. Key to the CFTR's switch from pump to channel function may have been the appearance of one or more ""lateral portals."" Such portals connect the cytoplasm to the transmembrane channel pore, allowing a continuous pathway for the electrodiffusional movement of Cl- ions. However, these portals remain the least well-characterized part of the Cl- transport pathway; even the number of functional portals is uncertain, and if multiple portals do exist, their relative functional contributions are unknown. Here, we used patch-clamp recording to identify the contributions of positively charged amino acid side chains located in CFTR's cytoplasmic transmembrane extensions to portal function. Mutagenesis-mediated neutralization of several charged side chains reduced single-channel Cl- conductance. However, these same mutations differentially affected channel blockade by cytoplasmic suramin and Pt(NO2)42- anions. We considered and tested several models by which the contribution of these positively charged side chains to one or more independent or non-independent portals to the pore could affect Cl- conductance and interactions with blockers. Overall, our results suggest the existence of a single portal that is lined by several positively charged side chains that interact electrostatically with both Cl- and blocking anions. We further propose that mutations at other sites indirectly alter the function of this single portal. Comparison of our functional results with recent structural information on CFTR completes our picture of the overall molecular architecture of the Cl- permeation pathway.","The cystic fibrosis transmembrane conductance regulator (CTFR) is a chloride transporter that apparently has evolved from an ancestral active transporter. Key to the CFTR's switch from pump to channel function may have been the appearance of one or more ""lateral portals."" Lateral portals connect the cell's inner fluid to the channel on the cell boundary, allowing a continuous pathway for chloride ion movement. However, these portals are the least studied part of the chloride transport pathway. Even the number of active portals in unknown, and if multiple portals do exists, their relative contributions are unknown. Here, we use electrical recordings to identify the contributions of positively charged protein segments located in CFTR's extensions to portal function. Mutation-controlled silencing of several charged protein segments reduced single-channel chloride transport. However, these same mutations uniquely affected channel blocking by specific inner-cell, negatively-charged particles. We considered and tested many models by which the contribution of these positive protein segments to one or more independent or non-independent portals to the channel could affect chloride transport and interactions with blockers. Overall, our results suggest the existence of a single portal lined with several positively-charged protein segments that interact electrically with both chloride and negative-charged, blocking particles. We propose that mutations at other sites indirectly affect the function of this one portal. Comparing our results with recent structural information on CTFR completes our picture of the overall molecular architecture of the chloride transport pathway.",2,B
46,29402832,"Background/aims: The CFTR-Associated Ligand (CAL), a PDZ domain containing protein with two coiled-coil domains, reduces cell surface WT CFTR through degradation in the lysosome by a well-characterized mechanism. However, CAL's regulatory effect on ΔF508 CFTR has remained almost entirely uninvestigated. Methods: In this study, we describe a previously unknown pathway for CAL by which it regulates the membrane expression of ΔF508 CFTR through arrest of ΔF508 CFTR trafficking in the endoplasmic reticulum (ER) using a combination of cell biology, biochemistry and electrophysiology. Results: We demonstrate that CAL is an ER localized protein that binds to ΔF508 CFTR and is degraded in the 26S proteasome. When CAL is inhibited, ΔF508 CFTR retention in the ER decreases and cell surface expression of mature functional ΔF508 CFTR is observed alongside of enhanced expression of plasma membrane scaffolding protein NHERF1. Chaperone proteins regulate this novel process, and ΔF508 CFTR binding to HSP40, HSP90, HSP70, VCP, and Aha1 changes to improve ΔF508 CFTR cell surface trafficking. Conclusion: Our results reveal a pathway in which CAL regulates the cell surface availability and intracellular retention of ΔF508 CFTR.","The CFTR-Associated Ligand (CAL) reduces the cell surface of normal CFTR through destroying it in a well-understood way. The CFTR protein is a channel involved with movement of ions (charged atoms) and fluids in epithelial cells (cells that cover the outer surface of the internal organs). However, few scientists have studied how CAL controls ΔF508 CFTR, a specific gene sequence change seen in the CFTR gene which slightly alters the CFTR protein. In this study, we describe a previously unknown way CAL controls ΔF508 CFTR through stopping ΔF508 CFTR movement in the cell's transportation system. We show that CAL binds to ΔF508 CFTR in one location of the cell and is destroyed in another location of the cell. When CAL is stopped, the amount of ΔF508 CFTR held within the cell decreases and working ΔF508 CFTR is seen on the cell surface. The binding of ΔF508 CFTR to proteins improves the cell surface movement conducted by ΔF508 CFTR. We conclude that CAL controls the cell surface availability and amount held within the cell of ΔF508 CFTR.",1,B
52,32489015,"47,XXX (triple X) and Turner syndrome (45,X) are sex chromosomal abnormalities with detrimental effects on health with increased mortality and morbidity. In karyotypical normal females, X-chromosome inactivation balances gene expression between sexes and upregulation of the X chromosome in both sexes maintain stoichiometry with the autosomes. In 47,XXX and Turner syndrome a gene dosage imbalance may ensue from increased or decreased expression from the genes that escape X inactivation, as well as from incomplete X chromosome inactivation in 47,XXX. We aim to study genome-wide DNA-methylation and RNA-expression changes can explain phenotypic traits in 47,XXX syndrome. We compare DNA-methylation and RNA-expression data derived from white blood cells of seven women with 47,XXX syndrome, with data from seven female controls, as well as with seven women with Turner syndrome (45,X). To address these questions, we explored genome-wide DNA-methylation and transcriptome data in blood from seven females with 47,XXX syndrome, seven females with Turner syndrome, and seven karyotypically normal females (46,XX). Based on promoter methylation, we describe a demethylation of six X-chromosomal genes (AMOT, HTR2C, IL1RAPL2, STAG2, TCEANC, ZNF673), increased methylation for GEMIN8, and four differentially methylated autosomal regions related to four genes (SPEG, MUC4, SP6, and ZNF492). We illustrate how these changes seem compensated at the transcriptome level although several genes show differential exon usage. In conclusion, our results suggest an impact of the supernumerary X chromosome in 47,XXX syndrome on the methylation status of selected genes despite an overall comparable expression profile.","47,XXX (triple X) and Turner syndrome (45,X) are sex chromosomal (gene material) abnormalities with detrimental effects on health. Both syndromes are associated with increased death and disease suffering. In females with normal chromosomes, X-chromosome (sex chromosome) inactivation balances gene expression between sexes. Upregulation (increase in activity) of the X chromosome in both sexes maintains balance with the autosomes (chromosomes not involved in sex determination). In both syndromes, a gene dosage imbalance may be caused by increased or decreased expression from the genes that escape X inactivation. A gene dosage imbalance may also be caused by incomplete X chromosome inactivation in 47,XXX. This study aimed to evaluate genome-wide DNA-methylation (addition of methyl groups to DNA) and RNA (genetic material)-expression changes. This was done in the hope that these alterations may explain phenotypic traits, or observable character traits, associated with 47,XXX syndrome. This study compared DNA-methylation and RNA-expression data taken from white blood cells of seven women with 47,XXX syndrome. This data was compared with data from seven female controls and seven women with Turner syndrome (45,X). This study evaluated genome-wide DNA-methylation and RNA data in blood from seven females with 47,XXX syndrome, seven females with Turner syndrome, and seven normal females (46,XX). The study identified the loss of methylation of six X-chromosomal genes, increased methylation for one specific gene, and four differentially methylated autosomal regions related to four genes. This data illustrates how these changes seem centered at the RNA level. In conclusion, this study suggests an impact of excess X chromosome in 47,XXX syndrome on the methylation status of selected genes.",1,B
52,30811826,"The pathogenesis of Turner syndrome (TS) and the genotype-phenotype relationship has been thoroughly investigated during the last decade. It has become evident that the phenotype seen in TS does not only depend on simple gene dosage as a result of X chromosome monosomy. The origin of TS specific comorbidities such as infertility, cardiac malformations, bone dysgenesis, and autoimmune diseases may depend on a complex relationship between genes as well as transcriptional and epigenetic factors affecting gene expression across the genome. Furthermore, two individuals with TS with the exact same karyotype may exhibit completely different traits, suggesting that no conventional genotype-phenotype relationship exists. Here, we review the different genetic mechanisms behind differential gene expression, and highlight potential key-genes essential to the comorbidities seen in TS and other X chromosome aneuploidy syndromes. KDM6A, important for germ cell development, has shown to be differentially expressed and methylated in Turner and Klinefelter syndrome across studies. Furthermore, TIMP1/TIMP3 genes seem to affect the prevalence of bicuspid aortic valve. KDM5 C could play a role in the neurocognitive development of Turner and Klinefelter syndrome. However, further research is needed to elucidate the genetic mechanism behind the phenotypic variability and the different phenotypic traits seen in TS.","The disease development of Turner syndrome (TS), and how gene expression effects physical appearance, has been heavily investigated over the last decade. Turner syndrome is a condition where there is an abnormal amount of chromosomes (genetic material). Phenotype (observable characteristics) seen in TS does not only depend on the number of copies of a gene as a result of X chromosome monosomy. Monosomy indicates there is an absence of one member of a chromosome pair; instead of 46 chromosomes in each cell of the body, there is 45. The origin of TS specific comorbidities (presence of two or more diseases) may depend on crosstalk between genes as well as factors affecting gene expression. These comorbidities include infertility, heart-related malformations, defective bone development, and autoimmune diseases (in which immune cells attack healthy cells). Furthermore, two people with TS with the exact same karyotype, number and visual appear of chromosomes, may exhibit completely different traits. This suggests that no conventional genotype-phenotype relationship exists. This study reviews the different genetic mechanisms behind differential (function-unique) gene expression. This study also highlights potential key-genes essential to the comorbidities seen in TS and other X chromosome aneuploidy (abnormal chromosome number) syndromes. KDM6A, a gene important for germ cell development, has shown to be differentially expressed and methylated in Turner and Klinefelter syndrome (male born with extra X chromosome copy) patients. Furthermore, TIMP1/TIMP3 genes seem to affect the prevalence (amount) of bicuspid aortic valve, an abnormality in the aortic valve of the heart. KDM5 C could play a role in the brain- and memory-related development of Turner and Klinefelter syndrome. However, further research is needed to determine the genetic mechanism behind the phenotypic variability and the different phenotypic traits seen in TS.",1,B
52,30779428,"X-chromosome inactivation generally results in dosage equivalence for expression of X-linked genes between 46,XY males and 46,XX females. The 20-30% of genes that escape silencing are thus candidates for having a role in the phenotype of Turner syndrome. Understanding which genes escape from silencing, and how they avoid this chromosome-wide inactivation is therefore an important step toward understanding Turner Syndrome. We have examined the mechanism of escape using a previously reported knock-in of a BAC containing the human escape gene RPS4X in mouse. We now demonstrate that escape from inactivation for RPS4X is already established by embryonic Day 9.5, and that both silencing and escape are faithfully maintained across the lifespan. No overt abnormalities were observed for transgenic mice up to 1 year of age despite robust transcription of the human RPS4X gene with no detectable downregulation of the mouse homolog. However, there was no significant increase in protein levels, suggesting translational compensation in the mouse. Finally, while many of the protein-coding genes have been assessed for their inactivation status, less is known about the X-linked RNA genes, and we propose that for many microRNA genes their inactivation status can be predicted as they are intronic to genes for which the inactivation status is known.","X-chromosome inactivation generally results in gene expression equivalence of X-linked genes between 46,XY males and 46,XX females. The 20-30% of genes that escape silencing may influence the phenotype (physical traits) of Turner syndrome. Turner syndrome is a condition where there is an abnormal amount of chromosomes (genetic material). Understanding which genes escape from silencing, and how they avoid this chromosome-wide inactivation, is an important step toward understanding Turner Syndrome. This study examined the mechanism of escape using a mouse model where a specific human escape gene, RPS4X, was inserted into the genome. The study showed that escape from inactivation for RPS4X is already established by embryonic Day 9.5. Additionally, the study demonstrated that both silencing and escape are maintained across the entire lifespan. No overt (obvious) abnormalities were observed for the mice up to 1 year of age. However, there was no significant increase in protein levels. This suggest translational compensation (altered conversion of RNA to proteins) in the mouse. Finally, while many of the protein-coding genes have been assessed (measured) for their inactivation status, less is known about the X-linked RNA genes. The authors propose that for many microRNA (RNA involved in silencing) genes, inactivation status can be predicted.",1,B
52,25535777,"Turner Syndrome (TS) is an unfavorable genetic condition with a prevalence of 1:2500 in newborn girls. Prompt and effective diagnosis is very important to appropriately monitor the comorbidities. The aim of the present study was to propose a feasible and practical molecular diagnostic tool for newborn screening by quantifying the gene dosage of the SHOX, VAMP7, XIST, UBA1, and SRY genes by quantitative polymerase chain reaction (qPCR) in individuals with a diagnosis of complete X monosomy, as well as those with TS variants, and then compare the results to controls without chromosomal abnormalities. According to our results, the most useful markers for these chromosomal variants were the genes found in the pseudoautosomic regions 1 and 2 (PAR1 and PAR2), because differences in gene dosage (relative quantification) between groups were more evident in SHOX and VAMP7 gene expression. Therefore, we conclude that these markers are useful for early detection in aneuploidies involving sex chromosomes.","Turner Syndrome (TS) is a genetic condition found in 1 out of every 2500 newborn girls. Turner syndrome is a condition where there is an abnormal amount of chromosomes (genetic material). Prompt and effective diagnosis is very important to appropriately monitor the comorbidities (two or more diseases in one patient). The aim of the study was to propose a feasible and practical diagnostic tool for newborn screening. The screening would be completed by quantifying (measuring) the dosage of specific genes in individuals with a diagnosis of complete X monosomy and TS variants. The gene doses would be compared to controls without chromosomal abnormalities. Monosomy indicates there is an absence of one member of a chromosome pair; instead of 46 chromosomes in each cell of the body, there is 45. Study results showed the most useful indicators for these chromosomal variants were the genes found in the pseudoautosomic regions 1 and 2 (PAR1 and PAR2). The authors concluded that these markers are useful for early detection in chromosomal imbalances, specifically those involving sex chromosomes.",1,B
52,24932682,"Turner syndrome is a chromosomal abnormality characterized by the absence of whole or part of the X chromosome in females. This X aneuploidy condition is associated with a diverse set of clinical phenotypes such as gonadal dysfunction, short stature, osteoporosis and Type II diabetes mellitus, among others. These phenotypes differ in their severity and penetrance among the affected individuals. Haploinsufficiency for a few X linked genes has been associated with some of these disease phenotypes. RNA sequencing can provide valuable insights to understand molecular mechanism of disease process. In the current study, we have analysed the transcriptome profiles of human untransformed 45,X and 46,XX fibroblast cells and identified differential expression of genes in these two karyotypes. Functional analysis revealed that these differentially expressing genes are associated with bone differentiation, glucose metabolism and gonadal development pathways. We also report differential expression of lincRNAs in X monosomic cells. Our observations provide a basis for evaluation of cellular and molecular mechanism(s) in the establishment of Turner syndrome phenotypes.","Turner syndrome is a chromosomal abnormality. The disease is characterized by the absence of the whole or part of the X chromosome in females. This X aneuploidy condition, or condition of having abnormal number of chromosomes, is associated with a diverse set of clinical phenotypes. These phenotypes, or outward characteristics, include ovary dysfunction, short stature, brittle bones, and Type II diabetes mellitus. These phenotypes differ in their severity and penetrance (extent) among those with the syndrome. Haploinsufficiency, or when one copy of a gene is deleted, for a few X linked genes has been associated with some of these disease phenotypes. RNA (special genetic material to create proteins) quantification can provide valuable insights to understand how these diseases form. This study analyzed RNA profiles of human 45,X (abnormal) and 46,XX (normal) cells. This study identified differential (unique function) expression of genes in these two chromosomal types. Analysis revealed that these differentially expressing genes are associated with bone differentiation, glucose metabolism, and gonadal development pathways. This study also reported differential expression of non-coding RNAs (RNAs that cannot be transformed into proteins) in X cells with abnormal chromosome count. This study provides a basis for evaluation of cellular and molecular mechanism(s) in the establishment of Turner syndrome phenotypes.",1,B
52,24850140,"Turner syndrome is a sex chromosome aneuploidy with characteristic malformations. Amniotic fluid, a complex biological material, could contribute to the understanding of Turner syndrome pathogenesis. In this pilot study, global gene expression analysis of cell-free RNA in amniotic fluid supernatant was utilized to identify specific genes/organ systems that may play a role in Turner syndrome pathophysiology. Cell-free RNA from amniotic fluid of five mid-trimester Turner syndrome fetuses and five euploid female fetuses matched for gestational age was extracted, amplified, and hybridized onto Affymetrix(®) U133 Plus 2.0 arrays. Significantly differentially regulated genes were identified using paired t tests. Biological interpretation was performed using Ingenuity Pathway Analysis and BioGPS gene expression atlas. There were 470 statistically significantly differentially expressed genes identified. They were widely distributed across the genome. XIST was significantly down-regulated (p < 0.0001); SHOX was not differentially expressed. One of the most highly represented organ systems was the hematologic/immune system, distinguishing the Turner syndrome transcriptome from other aneuploidies we previously studied. Manual curation of the differentially expressed gene list identified genes of possible pathologic significance, including NFATC3, IGFBP5, and LDLR. Transcriptomic differences in the amniotic fluid of Turner syndrome fetuses are due to genome-wide dysregulation. The hematologic/immune system differences may play a role in early-onset autoimmune dysfunction. Other genes identified with possible pathologic significance are associated with cardiac and skeletal systems, which are known to be affected in females with Turner syndrome. The discovery-driven approach described here may be useful in elucidating novel mechanisms of disease in Turner syndrome.","Turner syndrome is a condition where there is an abnormal number of chromosomes (genetic material). Amniotic fluid, the fluid surrounding a fetus, could contribute to the understanding of Turner syndrome development. In this pilot study, gene expression analysis of cell-free RNA (genetic material to create proteins) in amniotic fluid was analyzed. The fluid was evaluated to identify genes/organ systems that may play a role in Turner syndrome development. Cell-free RNA from amniotic fluid of five mid-trimester Turner syndrome fetuses and five control female fetuses was extracted. Significantly differentially (functionally unique) regulated genes were identified. Biological interpretation was performed to determine gene expression. There were 470 statistically significantly differentially expressed genes identified. They were widely distributed across the genome. XIST was significantly down-regulated (decreased in activity). SHOX was not differentially expressed. One of the most highly represented organ systems was the hematologic (blood) and immune system. Organization of the differentially expressed gene list identified genes of possible disease development significance. Transcriptomic (RNA) differences in the amniotic fluid of Turner syndrome fetuses are due to genome-wide dysregulation. The hematologic/immune system differences may play a role in early-onset autoimmune dysfunction (in which infection-fighting cells attack healthy cells). Other genes identified with possible pathologic (harmful) significance were associated with cardiac (heart) and skeletal systems. These systems are known to be affected in females with Turner syndrome. This data may be useful in identifying how Turner syndrome develops.",1,B
52,32210915,"Background: Turner syndrome (TS) is a sex chromosome aneuploidy with a variable spectrum of symptoms including short stature, ovarian failure and skeletal abnormalities. The etiology of TS is complex, and the mechanisms driving its pathogenesis remain unclear. Methods: In our study, we used the online Gene Expression Omnibus (GEO) microarray expression profiling dataset GSE46687 to identify differentially expressed genes (DEGs) between monosomy X TS patients and normal female individuals. The relevant data on 26 subjects with TS (45,XO) and 10 subjects with the normal karyotype (46,XX) was investigated. Then, tissue-specific gene expression, functional enrichment, and protein-protein interaction (PPI) network analyses were performed, and the key modules were identified. Results: In total, 25 upregulated and 60 downregulated genes were identified in the differential expression analysis. The tissue-specific gene expression analysis of the DEGs revealed that the system with the most highly enriched tissue-specific gene expression was the hematologic/immune system, followed by the skin/skeletal muscle and neurologic systems. The PPI network analysis, construction of key modules and manual screening of tissue-specific gene expression resulted in the identification of the following five genes of interest: CD99, CSF2RA, MYL9, MYLPF, and IGFBP2. CD99 and CSF2RA are involved in the hematologic/immune system, MYL9 and MYLPF are related to the circulatory system, and IGFBP2 is related to skeletal abnormalities. In addition, several genes of interest with possible roles in the pathogenesis of TS were identified as being associated with the hematologic/immune system or metabolism. Conclusion: This discovery-driven analysis may be a useful method for elucidating novel mechanisms underlying TS. However, more experiments are needed to further explore the relationships between these genes and TS in the future.","Turner syndrome (TS) is a sex chromosome aneuploidy or condition of having abnormal number of chromosomes. The disease has a broad spectrum of symptoms including short stature, ovarian failure, and skeletal abnormalities. The cause of TS is complex. The mechanisms driving its development are unclear. In this study, an online database was used to identify differentially expressed (functionally unique) genes (DEGs) between monosomy X TS patients (one chromosome lacks its partner) and normal female individuals. Data on 26 subjects with TS (45,XO) and 10 subjects with the normal chromosomal count (46,XX) was investigated. Several genetic analyses were performed. In total, 25 upregulated (increased in activity) and 60 downregulated (decreased in activity) genes were identified. The system with the most highly enriched tissue-specific gene expression was the hematologic (blood) and immune system. This was followed by the skin/skeletal muscle and brain-related systems. Additionally, analysis resulted in the identification of five genes of interest. Two of these genes, CD99 and CSF2RA, are involved in the hematologic (blood)/immune system. Others, MYL9 and MYLPF, are related to the circulatory (heart and blood vessels) system. A fifth gene, IGFBP2, is related to skeletal abnormalities. Additionally, several genes of interest with possible roles in the pathogenesis (disease creation) of TS were identified as being associated with the hematologic/immune system or metabolism. This analysis may be a useful method for identifying novel mechanisms underlying TS. However, more experiments are needed to explore the relationships between these genes and TS.",1,B
52,29973620,"Turner Syndrome (TS) is a condition where several genes are affected but the molecular mechanism remains unknown. Identifying the genes that regulate the TS network is one of the main challenges in understanding its aetiology. Here, we studied the regulatory network from manually curated genes reported in the literature and identified essential proteins involved in TS. The power-law distribution analysis showed that TS network carries scale-free hierarchical fractal attributes. This organization of the network maintained the self-ruled constitution of nodes at various levels without having centrality-lethality control systems. Out of twenty-seven genes culminating into leading hubs in the network, we identified two key regulators (KRs) i.e. KDM6A and BDNF. These KRs serve as the backbone for all the network activities. Removal of KRs does not cause its breakdown, rather a change in the topological properties was observed. Since essential proteins are evolutionarily conserved, the orthologs of selected interacting proteins in C. elegans, cat and macaque monkey (lower to higher level organisms) were identified. We deciphered three important interologs i.e. KDM6A-WDR5, KDM6A-ASH2L and WDR5-ASH2L that form a triangular motif. In conclusion, these KRs and identified interologs are expected to regulate the TS network signifying their biological importance.","Turner Syndrome (TS) is a condition where several genes are affected. However, how this occurs remains unknown. Turner syndrome is a condition where there is an abnormal amount of chromosomes (genetic material). Identifying the genes that regulate the TS network is one of the main challenges in understanding its cause. This study evaluated the regulatory network of genes reported in scientific literature and identified essential proteins involved in TS. A statistical evaluation was completed to model a TS network. Out of twenty-seven genes, the authors identified two key regulators (KRs) i.e. KDM6A and BDNF. These KRs serve as the backbone for all the network activities. Removal of KRs does not cause its breakdown, rather a change in the network properties was observed. Essential (necessary) proteins are evolutionarily conserved (kept). Because of this, genes of selected interacting proteins in C. elegans, cat, and macaque monkey were identified. The authors deciphered three important interologs, interactions between pairs of proteins. In conclusion, these KRs and identified interologs are expected to regulate the TS network. This data demonstrates their biological importance.",1,B
52,28627089,"Turner syndrome (TS) is one of the most common sexual chromosome abnormalities and is clearly associated with an increased risk of autoimmune diseases, particularly thyroid disease and coeliac disease (CD). Single-nucleotide polymorphism analyses have been shown to provide correlative evidence that specific genes are associated with autoimmune disease. Our aim was to study the functional polymorphic variants of PTPN22 and ZFAT in relation to thyroid disease and those of MYO9B in relation to CD. A cross-sectional comparative analysis was performed on Mexican mestizo patients with TS and age-matched healthy females. Our data showed that PTPN22 C1858T (considered a risk variant) is not associated with TS (X2 = 3.50, p = .61, and OR = 0.33 [95% CI = 0.10-1.10]). Also, ZFAT was not associated with TS (X2 = 1.2, p = .28, and OR = 1.22 [95% CI = 0.84-1.79]). However, for the first time, rs2305767 MYO9B was revealed to have a strong association with TS (X2 = 58.6, p = .0001, and OR = 10.44 [95% C = 5.51-19.80]), supporting a high level of predisposition to CD among TS patients. This report addresses additional data regarding the polymorphic variants associated with autoimmune disease, one of the most common complications in TS.","Turner syndrome (TS) is one of the most common sexual chromosome abnormalities. TS is associated with an increased risk of autoimmune diseases (in which immune cells attack healthy cells), particularly metabolism-affecting thyroid disease and coeliac disease (CD)--gluten sensitivity. Genetic analyses have provided evidence that correlates specific genes with autoimmune diseases. This report aimed to study the genetic variants of PTPN22 and ZFAT (protein-coding genes) in relation to thyroid disease. Additionally, this study evaluated the variants (gene types) of MYO9B (another protein-coding gene) to CD. An analysis was performed on Mexican, mixed heritage patients with TS. These patients were age-matched to healthy females. Data showed that PTPN22 C1858T, a PTPN22 variant, is not associated with TS. Also, ZFAT was not associated with TS. However, rs2305767 MYO9B, a MYO9B variant, was revealed to have a strong association with TS. This suggests patients with this variant have increased susceptibility to CD among TS patients. This report addresses additional data regarding the genetic variants associated with autoimmune disease. Autoimmune disease is one of the most common complications found in TS patients.",1,B
58,35085224,"Estimates of COVID-19 mRNA vaccine effectiveness (VE) have declined in recent months because of waning vaccine induced immunity over time, possible increased immune evasion by SARS-CoV-2 variants, or a combination of these and other factors. CDC recommends that all persons aged â‰¥12 years receive a third dose (booster) of an mRNA vaccine â‰¥5 months after receipt of the second mRNA vaccine dose and that immunocompromised individuals receive a third primary dose. A third dose of BNT162b2 (Pfizer-BioNTech) COVID-19 vaccine increases neutralizing antibody levels, and three recent studies from Israel have shown improved effectiveness of a third dose in preventing COVID-19 associated with infections with the SARS-CoV-2 B.1.617.2 (Delta) variant. Yet, data are limited on the real-world effectiveness of third doses of COVID-19 mRNA vaccine in the United States, especially since the SARS-CoV-2 B.1.1.529 (Omicron) variant became predominant in mid-December 2021. The VISION Network examined VE by analyzing 222,772 encounters from 383 emergency departments (EDs) and urgent care (UC) clinics and 87,904 hospitalizations from 259 hospitals among adults aged â‰¥18 years across 10 states from August 26, 2021 to January 5, 2022. Analyses were stratified by the period before and after the Omicron variant became the predominant strain (>50% of sequenced viruses) at each study site. During the period of Delta predominance across study sites in the United States (August-mid-December 2021), VE against laboratory-confirmed COVID-19-associated ED and UC encounters was 86% 14-179 days after dose 2, 76% â‰¥180 days after dose 2, and 94% â‰¥14 days after dose 3. Estimates of VE for the same intervals after vaccination during Omicron variant predominance were 52%, 38%, and 82%, respectively. During the period of Delta variant predominance, VE against laboratory-confirmed COVID-19-associated hospitalizations was 90% 14-179 days after dose 2, 81% â‰¥180 days after dose 2, and 94% â‰¥14 days after dose 3. During Omicron variant predominance, VE estimates for the same intervals after vaccination were 81%, 57%, and 90%, respectively. The highest estimates of VE against COVID-19-associated ED and UC encounters or hospitalizations during both Delta- and Omicron-predominant periods were among adults who received a third dose of mRNA vaccine. All unvaccinated persons should get vaccinated as soon as possible. All adults who have received mRNA vaccines during their primary COVID-19 vaccination series should receive a third dose when eligible, and eligible persons should stay up to date with COVID-19 vaccinations.","It is estimated that the effectiveness of COVID-19 mRNA vaccines has declined in recent months. There are several possible reasons for this. Vaccine-induced immunity decreases over time. New strains of the SARS-CoV-2 virus can become resistant to the vaccine, a process called immune evasion. A combination of these two phenomena or other factors could also cause decreased vaccine effectiveness. The US Centers for Disease Control and Prevention recommends that all people 12 years and older receive a third booster shot of an mRNA vaccine 5 months or later after receiving the second primary shot. Patients with a weakened immune system should receive a third primary shot. A third dose of the Pfizer vaccine (BNT162b2 COVID-19 vaccine) increases the blood level of antibodies that neutralize the virus and prevent infection. Three recent studies from Israel have shown that a third booster dose helps prevent COVID-19 caused by the Delta variant (SARS-CoV-2 B.1.617.2). However, in the United States there is little data to prove the effectiveness of third booster shots to prevent COVID-19, especially since the Omicron variant (SARS-CoV-2 B.1.1.529) became the most common strain in mid-December 2021. From August 26, 2021 to January 5, 2022, the VISION Network examined vaccine effectiveness among adults 18 and older across 10 states in the US by studying over 222,000 patients in 383 emergency departments and urgent care clinics, and over 87,000 hospitalized inpatients from 259 hospitals. The analysis was split apart at each study site by looking at the periods before and after the Omicron strain became the most common strain. During the time when the Delta strain was most common in the US (August to mid-December 2021), in emergency departments and urgent care clinics vaccine effectiveness in preventing infection was 86% effective 14-179 days after dose 2, dropped to 76% more than 180 days after dose 2, but increased up to 94% 14 days or more after dose 3. When the Omicron strain was most common, vaccine effectiveness for the same time intervals was only 52%, 38%, and 82%, respectively. In hospitalized patients, during the Delta strain period vaccine effectiveness was 90% 14-179 days after dose 2, 81% 180 days or longer after dose 2, and 94% 14 days or more after dose 3. During the Omicron period, estimates for the same time intervals after vaccination were 81%, 57%, and 90%, respectively. The highest estimates of vaccine effectiveness in both patient populations during both the Delta and Omicron periods were in adults who had received a third dose of mRNA vaccine. Based on this data, we recommed that all unvaccinated persons should get vaccinated as soon as possible. All adults who have received their first two doses of COVID-19 mRNA vaccines should receive a third dose as soon as they are eligible, and eligible persons should stay up to date with COVID-19 vaccinations and boosters.",2,C
58,35140406,"There is considerable interest in the waning of effectiveness of coronavirus disease 2019 (COVID-19) vaccines and vaccine effectiveness (VE) of booster doses. Using linked national Brazilian databases, we undertook a test-negative design study involving almost 14 million people (~16 million tests) to estimate VE of Corona Vac over time and VE of BNT162b2 booster vaccination against RT-PCR-confirmed severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection and severe COVID-19 outcomes (hospitalization or death). Compared with unvaccinated individuals, CoronaVac VE at 14-30 d after the second dose was 55.0% (95% confidence interval (CI): 54.3-55.7) against confirmed infection and 82.1% (95% CI: 81.4-82.8) against severe outcomes. VE decreased to 34.7% (95% CI: 33.1-36.2) against infection and 72.5% (95% CI: 70.9-74.0) against severe outcomes over 180 d after the second dose. A BNT162b2 booster, 6 months after the second dose of CoronaVac, improved VE against infection to 92.7% (95% CI: 91.0-94.0) and VE against severe outcomes to 97.3% (95% CI: 96.1-98.1) 14-30 d after the booster. Compared with younger age groups, individuals 80 years of age or older had lower protection after the second dose but similar protection after the booster. Our findings support a BNT162b2 booster vaccine dose after two doses of CoronaVac, particularly for the elderly.","We are interested in studying the waning effectiveness of COVID-19 vaccines and the effectiveness of booster doses. Using national Brazilian databases, we studied the trends of negative test results of almost 14 million people (representing about 16 million tests) to estimate the reduced effectiveness of CoronaVac COVID-19 vaccines over time. We also studied the effects of the Pfizer BNT162b2 booster dose on the rates of confirmed COVID-19 infection and severe outcomes (hospitalization or death). Compared with unvaccinated individuals, the effectiveness of the CoronaVac vaccine at 14-30 days after the second dose was 55% against confirmed infection and 82.1% against severe outcomes. Vaccine effectiveness decreased to 34.7% against infection and 72.5% against severe outcomes over 180 days after the second dose. A Pfizer BNT162b2 booster shot given 6 months after the second dose of CoronaVac, improved vaccine effectiveness by 92.7% against infection and 97.3% against severe outcomes 14-30 days after the booster. Compared with younger age groups, individuals 80 years of age or older had lower protection after the second dose but similar protection after the booster. Based on these results, we recommend a BNT162b2 booster vaccine dose after two doses of CoronaVac, particularly for the elderly.",2,C
58,35139036,"Background: Immunization against SARS-CoV-2, the causative agent of coronavirus disease-19 (COVID-19) occurs via natural infection or vaccination. However, it is currently unknown how long infection- or vaccination-induced immunological memory will last. Methods: We performed a longitudinal evaluation of immunological memory to SARS-CoV-2 up to one year post infection and following mRNA vaccination in naÃ¯ve and COVID-19 recovered individuals. Results: We found that memory cells are still detectable 8 months after vaccination, while antibody levels decline significantly especially in naÃ¯ve subjects. We also found that a booster injection is efficacious in reactivating immunological memory to spike protein in naÃ¯ve subjects, while it results ineffective in previously SARS-CoV-2 infected individuals. Finally, we observed a similar kinetics of decay of humoral and cellular immunity to SARS-CoV-2 up to one year following natural infection in a cohort of unvaccinated individuals. Conclusion: Short-term persistence of humoral immunity, together with the reduced neutralization capacity versus the currently prevailing SARS-CoV-2 variants, may account for reinfections and breakthrough infections. Long-lived memory B and CD4+ T cells may protect from severe disease development. A booster dose restores optimal anti-spike immunity in naÃ¯ve subjects, while the need for vaccinated COVID-19 recovered subjects has yet to be defined.","Immunity to SARS-CoV-2, the virus that causes coronavirus disease-19 (COVID-19) occurs from natural infection or vaccination. However, it is currently unknown how long infection- or vaccination-induced immunity will last. We performed a long-term study of immunity to SARS-CoV-2 up to one year post infection and following mRNA vaccination in unexposed people and in people who have recovered from COVID-19 infection. We found that memory cells (immune cells that ""remember"" having encountered an infection before) are still detectable 8 months after vaccination, while viral antibody blood levels decline significantly, especially in previously unexposed people. We also found that a booster shot is effective in reactivating immunity to the virus spike protein in previously unexposed people, while it is ineffective in people who were previously infected with SARS-CoV-2. Finally, we found a similar reduction of immunity to SARS-CoV-2 up to one year following natural infection in a group of unvaccinated individuals. We conclude that short-term immunity, together with the reduced ability of the immune system to block the newer strains of SARS-CoV-2, might account for breakthrough infections in vaccinated people and reinfections in people who were previously infected. A booster dose restores the strongest immunity against the viral spike protein in unexposed people, but it is not clear if people who have recovered from COVID-19 need to have a booster.",2,C
58,35132427,"Recognizing that anti-SARS-CoV-2 antibody levels wane over time following the 2-dose SARS-CoV-2 mRNA series, the FDA approved a booster dose for people greater than 12 years old. Limited data exist on whether a booster dose of the mRNA vaccine results in greater antibody protection than the primary series. We examined total and neutralizing antibodies to the spike protein of SARS-CoV-2, and neutralizing antibodies against Washington-1 (WA-1) and variants of concern (VOC) including Beta, Delta and Omicron in a longitudinal cohort. Healthcare workers (HWs) were included in the analysis if serum was collected 1) within 14-44 days post-dose2 of an mRNA SARS-CoV-2 vaccine (Timepoint 1, TP1), or 2) at least 8 months post-dose2 (Timepoint 2, TP2), or 3) within 14-44 days following mRNA booster (Timepoint 3, TP3). HWs with prior covid-positive PCR were excluded. We found that there is little to no neutralizing capability following a 2-dose mRNA vaccine series against the omicron variant, and neutralizing capacity to any variant strain tested has been lost by 8-months post two-dose vaccination series. However, the mRNA booster series eliminates the immune escape observed by the omicron variant with the two-dose series. Neutralizing titers were significantly higher for all variants post-boost compared to the titers post two-dose series. The longitudinal nature of our cohort facilitated the analysis of paired samples pre and post boost, showing a greater than 15-fold increase in neutralization against omicron post-boost in these paired samples. An mRNA booster dose provides greater quantity and quality of antibodies compared to a two-dose regimen and is critical to provide any protection against the omicron variant.","After vaccination with the 2-dose COVID-19 mRNA vaccine, blood levels of antibodies against the spike protein of SARS-CoV-2 drop over time. To increase blood levels of antibodies again, the FDA approved a booster dose for people greater than 12 years old. However, limited data exist on whether a booster dose of the mRNA vaccine results in greater antibody protection than the primary series. We measured the blood levels of all types of antibodies against the spike protein of SARS-CoV-2, and a specific type of antibodies - called neutralizing antibodies - against Washington-1 and other variants of concern including Beta, Delta and Omicron. We followed the trends in antibody levels at several points in time after vaccination. Healthcare workers were included in the analysis if serum was collected within 14-44 days post-dose 2 of an mRNA SARS-CoV-2 vaccine, or at least 8 months post-dose 2, or within 14-44 days following an mRNA booster shot. Healthcare workers who had previously tested positive to COVID-19 were excluded from this study. We found that there are few to no neutralizing antibodies produced against the omicron variant following a 2-dose mRNA vaccine series. By 8-months post two-dose vaccination series, no neutralizing antibodies remain in blood circulation. However, the mRNA booster reactivates immunity to the omicron variant after the two-dose vaccination. Neutralizing antibody blood levels were significantly higher for all variants post-booster compared to the levels after the first two shots. The long-term nature of our study helped us to analyze the trends of antibody blood levels over time. We saw a 15-fold increase in neutralizing antibodies against the omicron variant following the booster shot. An mRNA booster dose provides greater quantity and quality of antibodies compared to a two-dose vaccine and is critical to provide any protection against the omicron variant.",2,C
58,35131043,"Background: Vaccine effectiveness against COVID-19 beyond 6 months remains incompletely understood. We aimed to investigate the effectiveness of COVID-19 vaccination against the risk of infection, hospitalisation, and death during the first 9 months after vaccination for the total population of Sweden. Methods: This retrospective, total population cohort study was done using data from Swedish nationwide registers. The cohort comprised all individuals vaccinated with two doses of ChAdOx1 nCoV-19, mRNA-1273, or BNT162b2, and matched unvaccinated individuals, with data on vaccinations and infections updated until Oct 4, 2021. Two outcomes were evaluated. The first was SARS-CoV-2 infection of any severity from Jan 12 to Oct 4, 2021. The second was severe COVID-19, defined as hospitalisation for COVID-19 or all-cause 30-day mortality after confirmed infection, from March 15 to Sept 28, 2021. Findings: Between Dec 28, 2020, and Oct 4, 2021, 842 974 individuals were fully vaccinated (two doses), and were matched (1:1) to an equal number of unvaccinated individuals (total study cohort n=1 685 948). For the outcome SARS-CoV-2 infection of any severity, the vaccine effectiveness of BNT162b2 waned progressively over time, from 92% (95% CI 92 to 93; p<0Â·001) at 15-30 days, to 47% (39 to 55; p<0Â·001) at 121-180 days, and to 23% (-2 to 41; p=0Â·07) from day 211 onwards. Waning was slightly slower for mRNA-1273, with a vaccine effectiveness of 96% (94 to 97; p<0Â·001) at 15-30 days and 59% (18 to 79; p=0Â·012) from day 181 onwards. Waning was also slightly slower for heterologous ChAdOx1 nCoV-19 plus an mRNA vaccine, for which vaccine effectiveness was 89% (79 to 94; p<0Â·001) at 15-30 days and 66% (41 to 80; p<0Â·001) from day 121 onwards. By contrast, vaccine effectiveness for homologous ChAdOx1 nCoV-19 vaccine was 68% (52 to 79; p<0Â·001) at 15-30 days, with no detectable effectiveness from day 121 onwards (-19% [-98 to 28]; p=0Â·49). For the outcome of severe COVID-19, vaccine effectiveness waned from 89% (82 to 93; p<0Â·001) at 15-30 days to 64% (44 to 77; p<0Â·001) from day 121 onwards. Overall, there was some evidence for lower vaccine effectiveness in men than in women and in older individuals than in younger individuals. Interpretation: We found progressively waning vaccine effectiveness against SARS-CoV-2 infection of any severity across all subgroups, but the rate of waning differed according to vaccine type. With respect to severe COVID-19, vaccine effectiveness seemed to be better maintained, although some waning became evident after 4 months. The results strengthen the evidence-based rationale for administration of a third vaccine dose as a booster.","The effectiveness of a vaccine against COVID-19 6 months after vaccination is not fully understood. We studied the effectiveness of COVID-19 vaccination against the risk of infection, hospitalisation, and death during the first 9 months after vaccination for the total population of Sweden. We took our data from Swedish nationwide registers. The data were from all individuals vaccinated with two doses of ChAdOx1 nCoV-19 (AstraZeneca), mRNA-1273 (Moderna), or BNT162b2 (Pfizer) vaccines, and matched unvaccinated individuals, with data on vaccinations and infections updated until Oct 4, 2021. Two outcomes were evaluated. The first was SARS-CoV-2 infection of any severity from Jan 12 to Oct 4, 2021. The second was severe COVID-19, defined as hospitalisation for COVID-19 or death from any cause 30 days after confirmed infection, from March 15 to Sept 28, 2021. Between Dec 28, 2020, and Oct 4, 2021, 842 974 individuals were fully vaccinated (two doses). We compared these individuals to an equal number of unvaccinated individuals. The total number of people studied was 1,685,948. In patients who had SARS-CoV-2 infection of any severity, the vaccine effectiveness of BNT162b2 dropped progressively over time, from 92% at 15-30 days after vaccination, to 47% at 121-180 days, and to 23% from day 211 onwards. Reduced vaccine effectiveness was slightly slower for the mRNA-1273 vaccine, with a vaccine effectiveness of 96% at 15-30 days and 59% from day 181 onwards. Reduction of vaccine effectiveness was also slightly slower for the combination of the ChAdOx1 nCoV-19 vaccine plus an mRNA vaccine, for which vaccine effectiveness was 89% at 15-30 days and 66% from day 121 onwards. By contrast, vaccine effectiveness for the ChAdOx1 nCoV-19 vaccine only (not combined with another vaccine) was 68% at 15-30 days, with no detectable effectiveness from day 121 onwards. In patients who had severe COVID-19, vaccine effectiveness dropped from 89% at 15-30 days to 64% from day 121 onwards. Overall, there was some evidence for lower vaccine effectiveness in men than in women and in older individuals than in younger individuals. We found progressively reduced vaccine effectiveness against SARS-CoV-2 infection of any severity across all subgroups of patients, but the rate of reduction differed according to vaccine type. In patients who had severe COVID-19, vaccine effectiveness seemed to be better maintained, although some reduction became evident after 4 months. Our study provides evidence that a third vaccine dose as a booster will improve immunity against COVID-19.",2,C
58,35123028,"Objectives: To determine the status of immune responses after primary and booster immunization for coronavirus disease 2019 (COVID-19) variants and evaluate the differences in disease-resistance based upon titers of neutralizing antibodies (NAbs) against the variants. Methods: Participants aged 18 - 59 y received two doses of inactivated COVID-19 vaccine, 14 days apart, and a booster dose after 12 m. Blood samples were collected before vaccination (baseline), 1 and 6 m after primary immunization, and at multiple instances within 21 d of booster dose. NAbs against the spike protein of Wuhan-Hu-1 and three variants were measured using pseudovirus neutralization assays. Results: Out of 400 enrolled participants, 387 completed visits scheduled within 6 m of the second dose, and 346 participated received the booster dose in the follow-up research. After 1 m of primary immunization, geometric mean titers (GMTs) of NAbs peaked for Wuhan-Hu-1, while GMTs of other variants were < 30. After 6 m of primary immunization, GMTs of NAbs against all strains were < 30. After 3 d of booster immunization, GMTs were unaltered, seroconversion rates reached approximately 50% after 7 d, and GMTs of NAbs against all strains peaked at 14 d. Conclusion: Two-dose of inactivated COVID-19 vaccine induced the formation of NAbs and memory-associated immune responses, and high titers of NAbs against the variants obtained after booster immunization may further improve the effectiveness of the vaccine.","We studied the immune response after primary and booster immunization for coronavirus disease 2019 (COVID-19) variants. We evaluated the differences in disease resistance based upon blood levels of neutralizing antibodies (antibodies that inactivate the virus) against the variants. Participants aged 18 - 59 years old received two doses of inactivated COVID-19 vaccine, 14 days apart, and a booster dose after 12 months. Blood samples were collected before vaccination, 1 and 6 months after primary immunization, and several times within 21 days of booster dose. Neutralizing antibodies against the spike protein of the Wuhan-Hu-1 variant and three other variants were measured. Out of 400 enrolled participants, 387 completed visits scheduled within 6 months of the second dose, and 346 participants received the booster dose in the follow-up research. After 1 month of primary immunization, blood levels of neutralizing antibodies peaked for Wuhan-Hu-1, while blood levels against other variants were lower. After 6 months of primary immunization, blood levels against all strains were reduced. After 3 days of booster immunization, blood levels were unchanged. However, blood levels increased approximately 50% after 7 days, and peaked at 14 days for all strains. Two doses of inactivated COVID-19 vaccine produced neutralizing antibodies and immunity, and high levels of neutralizing antibodies against the variants after booster immunization could further improve the effectiveness of the vaccine.",2,C
58,35144968,"Objectives: To estimate the effectiveness of mRNA vaccines against SARS-CoV-2 infection and severe covid-19 at different time after vaccination. Design: Retrospective cohort study. Setting: Italy, 27 December 2020 to 7 November 2021. Participants: 33 250 344 people aged â‰¥16 years who received a first dose of BNT162b2 (Pfizer-BioNTech) or mRNA-1273 (Moderna) vaccine and did not have a previous diagnosis of SARS-CoV-2 infection. Main outcome measures: SARS-CoV-2 infection and severe covid-19 (admission to hospital or death). Data were divided by weekly time intervals after vaccination. Incidence rate ratios at different time intervals were estimated by multilevel negative binomial models with robust variance estimator. Sex, age group, brand of vaccine, priority risk category, and regional weekly incidence in the general population were included as covariates. Geographic region was included as a random effect. Adjusted vaccine effectiveness was calculated as (1-IRR)Ã—100, where IRR=incidence rate ratio, with the time interval 0-14 days after the first dose of vaccine as the reference. Results: During the epidemic phase when the delta variant was the predominant strain of the SARS-CoV-2 virus, vaccine effectiveness against SARS-CoV-2 infection significantly decreased (P<0.001) from 82% (95% confidence interval 80% to 84%) at 3-4 weeks after the second dose of vaccine to 33% (27% to 39%) at 27-30 weeks after the second dose. In the same time intervals, vaccine effectiveness against severe covid-19 also decreased (P<0.001), although to a lesser extent, from 96% (95% to 97%) to 80% (76% to 83%). High risk people (vaccine effectiveness -6%, -28% to 12%), those aged â‰¥80 years (11%, -15% to 31%), and those aged 60-79 years (2%, -11% to 14%) did not seem to be protected against infection at 27-30 weeks after the second dose of vaccine. Conclusions: The results support the vaccination campaigns targeting high risk people, those aged â‰¥60 years, and healthcare workers to receive a booster dose of vaccine six months after the primary vaccination cycle. The results also suggest that timing the booster dose earlier than six months after the primary vaccination cycle and extending the offer of the booster dose to the wider eligible population might be warranted.","We studied the effectiveness of mRNA vaccines against SARS-CoV-2 infection and severe COVID-19 at different times after vaccination. This study was performed in Italy from December 27, 2020 to November 7, 2021. The participants included people aged 16 years and older who received a first dose of BNT162b2 (Pfizer-BioNTech) or mRNA-1273 (Moderna) vaccine and did not have a previous diagnosis of SARS-CoV-2 infection. We tracked the number of people with SARS-CoV-2 infection and severe COVID-19 (admission to hospital or death). The data was calculated for each week following vaccination. The incidence rate of infection at different time intervals was estimated using statistical models. We recorded the sex, age group, brand of vaccine, and priority risk category of patients, and recorded the regional weekly incidence in the general population. We tracked the number of cases according to geographic region. Using infection rates at 0-14 days after the first dose of vaccine as our starting point, we followed the trend of vaccine effectiveness During the epidemic phase when the delta variant was the most common strain of the SARS-CoV-2 virus, vaccine effectiveness against SARS-CoV-2 infection significantly decreased from 82% at 3-4 weeks after the second dose of vaccine to 33% at 27-30 weeks after the second dose. In the same time range, vaccine effectiveness against severe COVID-19 also decreased, although to a lesser extent, from an average of 96% to an average of 80%. High risk people, those aged more than 80 years old, and those aged 60-79 years did not seem to be protected against infection at 27-30 weeks after the second dose of vaccine. Our results support vaccinating high risk people, those older than 60 years, and healthcare workers with a booster dose of vaccine six months after the primary vaccination cycle. Our results also suggest that giving the booster dose earlier than six months after the primary vaccination cycle and extending the offer of the booster dose to other groups of people might be a good idea.",2,C
58,35087066,"The BNT162b2 vaccine is highly effective against COVID-19 infection and was delivered with a 3-week time interval in registration studies 1. However, many countries extended this interval to accelerate population coverage with a single vaccine. It is not known how immune responses are influenced by delaying the second dose. We provide the assessment of immune responses in the first 14 weeks after standard or extended-interval BNT162b2 vaccination and show that delaying the second dose strongly boosts the peak antibody response by 3.5-fold in older people. This enhanced antibody response may offer a longer period of clinical protection and delay the need for booster vaccination. In contrast, peak cellular-specific responses were the strongest in those vaccinated on a standard 3-week vaccine interval. As such, the timing of the second dose has a marked influence on the kinetics and magnitude of the adaptive immune response after mRNA vaccination in older people.","The BNT162b2 vaccine (Pfizer) is highly effective against COVID-19 infection. In registration studies, the two doses were delivered 3 weeks apart.. However, many countries extended the time between the first and second doses to maximize the number of people vaccinated with one dose. It is not known how antibody responses or cellular immune responses are influenced by delaying the second dose. We studied the immune response in the first 14 weeks after the standard 3-week interval or the extended interval BNT162b2 vaccination. We showed that delaying the second dose strongly boosts the peak antibody response by 3.5-fold in older people. This enhanced antibody response may offer a longer period of protection against infection and delay the need for booster vaccination. In contrast, peak cellular-specific responses were the strongest in those vaccinated on a standard 3-week vaccine interval. The timing of the second dose has a strong influence on the antibody response after BNT162b2 vaccination in older people.",2,C
58,34976499,"Introduction: Coronavirus disease 2019 (COVID-19) vaccines are nothing short of a miracle story halting the pandemic across the globe. Nearly half of the global population has received at least one dose. Nevertheless, antibody levels in vaccinated people have shown waning, and breakthrough infections have occurred. Our study aims to measure antibody kinetics following AZD1222 (ChAdOx1) vaccination six months after the second dose and the factors affecting the kinetics. Materials and methods: We conducted a prospective longitudinal study monitoring for six months after the second of two AZD1222 (ChAdOx1) vaccine doses in healthcare professionals and healthcare facility employees at Veer Surendra Sai Institute of Medical Sciences and Research (included doctors, nurses, paramedical staff, security and sanitary workers, and students). Two 0.5-mL doses of the vaccine were administered intramuscularly, containing 5 x 1010 viral particles 28 to 30 days between doses. We collected blood samples one month after the first dose (Round 1), one month after the second dose (Round 2), and six months after the second dose (Round 3). We tested for immunoglobulin G (IgG) levels against the receptor-binding domain of the spike protein of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) by chemiluminescence microparticle immunoassay. We conducted a linear mixed model analysis to study the antibody kinetics and influencing factors. Results: Our study included 122 participants (mean age, 41.5 years; 66 men, 56 women). The geometric mean IgG titers were 138.01 binding antibody units (BAU)/mL in Round 1, 176.48 BAU/mL in Round 2, and 112.95 BAU/mL in Round 3. Seven participants showed seroreversion, and 11 had breakthrough infections. Eighty-six participants showed a substantial decline in antibody titer from Rounds 2 to 3. Persons aged 45 or older had higher mean titer than people aged younger than 45 years. Overweight and obese (BMI â‰¥ 25 kg/m2) had a higher mean titer than average or underweight persons. The only significant predictor of IgG titers at six months was SARS-CoV-2 infection on mixed model analysis. Conclusion: We found a substantial decline in antibody levels leading to seven cases of seroreversion in healthcare professionals who received the ChAdOx1 vaccine. History of prior COVID-19 was the only significant factor in antibody levels at six months. Seroreversion and breakthrough infection warrant further research into the optimal timing and potential benefits of booster doses of the AZD1222 (ChAdOx1) COVID-19 vaccine.","Coronavirus disease 2019 (COVID-19) vaccines are nothing short of a miracle story halting the pandemic across the globe. Nearly half of the global population has received at least one dose. Nevertheless, antibody blood levels in vaccinated people drop over time, and breakthrough infections have occurred. We studied the trends in antibody blood levels six months after the second dose of the AZD1222 (ChAdOx1) AstraZeneca vaccine. We conducted a 6 month study after the second of two AZD1222 (ChAdOx1) vaccine doses in healthcare professionals and healthcare facility employees at Veer Surendra Sai Institute of Medical Sciences and Research. The study population included doctors, nurses, paramedical staff, security and sanitary workers, and students. Two doses of the vaccine were injected into the upper arm, with 28 to 30 days between doses. We collected blood samples one month after the first dose (Round 1), one month after the second dose (Round 2), and six months after the second dose (Round 3). We measured blood levels of antibodies against the spike protein of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). We used statistical methods to measure the change in antibody blood levels over time. Our study included 122 participants (mean age, 41.5 years; 66 men, 56 women). Antibody blood levels were 138.01 in Round 1, increased to 1,176.48 in Round 2, and dropped to 112.95 in Round 3. Seven participants showed a complete loss of measurable blood antibodies, and 11 had breakthrough infections. Eighty-six participants showed a substantial decline in antibody levels from Rounds 2 to 3. Persons aged 45 or older had higher antibody levels than people aged younger than 45 years. Overweight and obese people (body mass index larger than 25) had higher antibody levels than average or underweight persons. The only people who maintained high levels of antibodies at 6 months were in people who had been infected with SARS-CoV2. We found a substantial decline in antibody levels leading to seven cases of loss of antibodies in healthcare professionals who received the ChAdOx1 vaccine. A history of prior COVID-19 infection was the only significant reason for high antibody levels at six months. More research needs to be conducted on the optimal timing and potential benefits of booster doses of the AZD1222 (ChAdOx1) COVID-19 vaccine to avoid breakthrough infections or loss of immunity.",2,C
58,34962970,"The timing of the development of specific adaptive immunity after natural SARS-CoV-2 infection, and its relevance in clinical outcome, has not been characterized in depth. Description of the long-term maintenance of both cellular and humoral responses elicited by real-world anti-SARS-CoV-2 vaccination is still scarce. Here we aimed to understand the development of optimal protective responses after SARS-CoV-2 infection and vaccination. We performed an early, longitudinal study of S1-, M- and N-specific IFN-Î³ and IL-2 T cell immunity and anti-S total and neutralizing antibodies in 88 mild, moderate or severe acute COVID-19 patients. Moreover, SARS-CoV-2-specific adaptive immunity was also analysed in 234 COVID-19 recovered subjects, 28 uninfected BNT162b2-vaccinees and 30 uninfected healthy controls. Upon natural infection, cellular and humoral responses were early and coordinated in mild patients, while weak and inconsistent in severe patients. The S1-specific cellular response measured at hospital arrival was an independent predictive factor against severity. In COVID-19 recovered patients, four to seven months post-infection, cellular immunity was maintained but antibodies and neutralization capacity declined. Finally, a robust Th1-driven immune response was developed in uninfected BNT162b2-vaccinees. Three months post-vaccination, the cellular response was comparable, while the humoral response was consistently stronger, to that measured in COVID-19 recovered patients. Thus, measurement of both humoral and cellular responses provides information on prognosis and protection from infection, which may add value for individual and public health recommendations.","The timing of the development of immunity after natural SARS-CoV-2 infection has not been studied in depth. Data about long-term immunity following anti-SARS-CoV-2 vaccination is still scarce. We studied the the development of the strongest protective immune responses after SARS-CoV-2 infection and vaccination. We performed a long-term study of the immune responses and antibody blood levels in patients with mild, moderate or severe acute COVID-19 infection. Immune response was also studied in 234 COVID-19 recovered subjects, 28 uninfected BNT162b2 (Pfizer)-vaccinees and 30 uninfected healthy unvaccinated people. During natural infection, immune responses were early and coordinated in patients with mild COVID-19, while the immune responses were weak and inconsistent in patients with severe illness. Immunity in patients was measured at hospital arrival. Patients with a strong cellular immune response against the virus spike protein had a less severe illness. In COVID-19 recovered patients, four to seven months post-infection, cellular immunity was maintained but antibody blood levels and the capacity of the antibodies to block the virus declined. In uninfected BNT162b2-vaccinees, a strong cellular immune response developed. Three months post-vaccination, the cellular immune response was comparable to that measured in patients who recovered from COVID-19. In vaccinated people, the antibody response was consistently stronger to that measured in COVID-19 recovered patients. Our study of the cellular immune response and the antibody response provides information about protection from infection and the likelihood of developing severe infection. This information could be used to influence public health recommendations.",2,C
66,22087198,"Nephrotic syndrome (NS) is one of the most common glomerular diseases that affect children. Renal histology reveals the presence of minimal change nephrotic syndrome (MCNS) in more than 80% of these patients. Most patients with MCNS have favorable outcomes without complications. However, a few of these children have lesions of focal segmental glomerulosclerosis, suffer from severe and prolonged proteinuria, and are at high risk for complications. Complications of NS are divided into two categories: disease-associated and drug-related complications. Disease-associated complications include infections (e.g., peritonitis, sepsis, cellulitis, and chicken pox), thromboembolism (e.g., venous thromboembolism and pulmonary embolism), hypovolemic crisis (e.g., abdominal pain, tachycardia, and hypotension), cardiovascular problems (e.g., hyperlipidemia), acute renal failure, anemia, and others (e.g., hypothyroidism, hypocalcemia, bone disease, and intussusception). The main pathomechanism of disease-associated complications originates from the large loss of plasma proteins in the urine of nephrotic children. The majority of children with MCNS who respond to treatment with corticosteroids or cytotoxic agents have smaller and milder complications than those with steroid-resistant NS. Corticosteroids, alkylating agents, cyclosporin A, and mycophenolate mofetil have often been used to treat NS, and these drugs have treatment-related complications. Early detection and appropriate treatment of these complications will improve outcomes for patients with NS.","Nephrotic Syndrome (NS) is one of the most common kidney diseases in children. Most of the time, looking at kidney tissue under a microscope shows that there is Minimal Change Nephrotic Syndrome (MCNS). Most people with MCNS can be cured without other issues. However, some children with NS have a different form called focal segmental glomerulosclerosis. Children with this form have excess protein in their urine for long periods of time and are at high risk for additional problems. Additional problems caused by NS come in two categories: problems caused by the disease, and problems caused by drugs used to treat the disease. Problems caused by the disease include infections, blood clots, shock, heart disease, kidney failure, anemia, and others. The main way NS causes problems is from the large loss of proteins from the blood through urine. Most children with MCNS that is treatable with drugs have fewer additional problems that children with NS that drugs cannot help. Several drugs have been used to treat NS. These drugs can cause additional problems when used to treat NS. Finding additional problems early will improve results for patients with NS.",2,B
66,25230687,"Background and objectives: There are very little data available regarding nephrotic syndrome (NS) in elderly (aged ≥65 years) Japanese. The aim of this study was to examine the causes and outcomes of NS in elderly patients who underwent renal biopsies between 2007 and 2010. Design, setting, participants, and measurements: From July 2007 to June 2010, all of the elderly (aged ≥65 years) Japanese primary NS patients who underwent native renal biopsies and were registered in the Japan renal biopsy registry (J-RBR; 438 patients including 226 males and 212 females) were identified. From this cohort, 61 patients [28 males and 33 females including 29, 19, 6, 4, and 3 patients with membranous nephropathy (MN), minimal change nephrotic syndrome (MCNS), focal segmental glomerulosclerosis (FSGS), membranoproliferative glomerulonephritis (MPGN), and other conditions, respectively] were registered from the representative multi-centers over all districts of Japan, and analyzed retrospectively. The treatment outcome was assessed using proteinuria-based criteria; i.e., complete remission (CR) was defined as urinary protein level of <0.3 g/day or g/g Cr, and incomplete remission type I (ICR-I) was defined as urinary protein level of <1.0-0.3 g/day or g/g Cr, and renal dysfunction was defined as a serum creatinine (Cr) level of 1.5 times the baseline level. Results: In this elderly primary NS cohort, MN was the most common histological type of NS (54.8 %), followed by MCNS (19.4 %), FSGS (17.4 %), and MPGN (8.4 %). Of the patients with MN, MCNS, or FSGS, immunosuppressive therapy involving oral prednisolone was performed in 25 MN patients (86.2 %), 18 MCNS patients (94.7 %), and all 6 FSGS patients (100 %). CR was achieved in all 19 (100 %) MCNS patients. In addition, CR and ICR-I were achieved in 16 (55.2 %) and 18 (62.1 %) MN patients and 4 (66.7 %) and 5 (83.3 %) FSGS patients, respectively. There were significant differences in the median time to CR among the MCNS, FSGS, and MN patients (median: 26 vs. 271 vs. 461 days, respectively, p < 0.001), and between the elderly (65-74 years, n = 7) and very elderly (aged ≥75 years, n = 12) MCNS patients (7 vs. 22 days, p = 0.037). Relapse occurred in two (6.9 %) of the MN and nine (47.4 %) of the MCNS patients. Renal dysfunction was observed in five (7.2 %) of the MN patients. Serious complications developed in eight (14.8 %) patients, i.e., two (3.7 %) patients died, four (7.4 %, including three MCNS patients) were hospitalized due to infectious disease, and two (3.7 %) developed malignancies. The initiation of diabetic therapy was necessary in 14 of the 61 patients (23.0 %) with much higher initial steroid dosage. Conclusion: Renal biopsy is a valuable diagnostic tool for elderly Japanese NS patients. In this study, most of elderly primary NS patients respond to immunosuppressive therapy with favorable clinical outcomes. On the other hand, infectious disease is a harmful complication among elderly NS patients, especially those with MCNS. In future, modified clinical guidelines for elderly NS patients should be developed.","There’s not a lot of information about Nephrotic Syndrome (NS) in older Japanese people. The goal of this study was to look at the causes and outcomes of NS in older patients that had kidney biopsies (tissue samples) between 2007 and 2010. From a registry in Japan, we found all Japanese NS patients 65 or older who had kidney biopsies (tissue samples) from July 2007 to June 2010. This included 438 patients, 226 men and 212 women. Out of these patients, we looked closer at 61 that had certain types of NS. To know how well treatments worked, we looked at the levels of protein in the patients’ urine. In this group of older people with NS, more than half (58%) had a type called Membranous Nephropathy (MN). Another 19% had Minimal Change Nephrotic Syndrome (MCNS), 17% had Focal Segmental Glomerosclerosis (FSGS), and 8% had membranoproliferative glomerulonephritis (MPGN). A drug called prednisolone, which suppresses the immune system, was given to 86% of the patients with MN, 94% of the patients with MCNS, and all 6 patients with FSGS. The disease was completely cured in all 19 MCNS patients. Fifty-five percent of MN patients were completely cured and 62% were at least partially cured. Sixty-six percent of FSGS patients were completely cured and 83% were partially cured. There were meaningful differences in how long it took for different types of NS to be cured. The differences depended both on the type of NS (MCNS, FSGS, or MN) and whether MCNS patients were 75 or older. The disease came back in 7% of patients with MN and 47% of patients with MCNS. Seven percent of patients with MN had problems with kidney function. Eight patients had serious problems. For example, two of them died, four were hospitalized for infections, and two developed malignant tumors. Treatment for diabetes was needed for 14 of the 61 patients with higher initial steroid dosage. Conclusions we can make include the fact that kidney biopsies are valuable for diagnosing NS in older Japanese patients. Also, in this study, most older NS patients responded well to drugs that suppress the immune system. On the other hand, infections are a harmful related problem among older people with NS, especially if they have MCNS. In the future, doctors should update guidelines for older NS patients.",2,B
66,28089478,"Background: Few studies have examined the treatment and outcome of adult-onset minimal change nephrotic syndrome (MCNS). We retrospectively studied 125 patients who had MCNS with onset in either adulthood or late adolescence. Presenting characteristics, duration of initial treatment and response to treatment, relapse patterns, complications, and long-term outcome were studied. Study design: Case series. Setting & participants: Patients with new-onset nephrotic syndrome 16 years or older and a histologic diagnosis of MCNS in 1985 to 2011 were identified from pathology records of 10 participating centers. Outcomes: Partial and complete remission, treatment resistance, relapse, complications, renal survival. Results: Corticosteroids were given as initial treatment in 105 (84%) patients. After 16 weeks of corticosteroid treatment, 92 (88%) of these patients had reached remission. Median time to remission was 4 (IQR, 2-7) weeks. 7 (6%) patients initially received cyclophosphamide with or without corticosteroids, and all attained remission after a median of 4 (IQR, 3-11) weeks. 13 (10%) patients reached remission without immunosuppressive treatment. One or more relapses were observed in 57 (54%) patients who received initial corticosteroid treatment. Second-line cyclophosphamide resulted in stable remission in 57% of patients with relapsing MCNS. Acute kidney injury was observed in 50 (40%) patients. Recovery of kidney function occurred almost without exception. Arterial or venous thrombosis occurred in 11 (9%) patients. At the last follow-up, 113 (90%) patients were in remission and had preserved kidney function. 3 patients with steroid-resistant MCNS progressed to end-stage renal disease, which was associated with focal segmental glomerulosclerosis lesions on repeat biopsy. Limitations: Retrospective design, variable treatment protocols. Conclusions: The large majority of patients who had MCNS with onset in adulthood or late adolescence were treated with corticosteroids and reached remission, but many had relapses. Cyclophosphamide resulted in stable remission in many patients with relapses. Significant morbidity was observed due to acute kidney injury and other complications. Progression to end-stage renal disease occurred in a few patients and was explained by focal segmental glomerulosclerosis.","Minimal Change Nephrotic Syndrome (MCNS) is a kidney disease that can lead to a group of symptoms called Nephrotic Syndrome (NS). Not many studies have looked at the treatment and results of MCNS that starts in adulthood. We looked at 125 patients from other studies that had MCNS appear as a late adolescent or adult. We studied how the disease appeared, how long it was treated, and how patients responded to the treatment. We also studied further problems caused by the disease and treatment, as well as long-term outcomes. The type of this study is to look at various cases. We looked at pathology reports from 1985 to 2011 from 10 participating health care centers. From these, we found patients 16 years or older with new NS and a diagnosis of MCNS confirmed by looking at tissue under a microscope. The cases had a variety of outcomes: partial and complete reversal of the disease, resistance of the disease to treatment, the disease coming back, further problems caused by the disease, and how long the kidneys functioned. Eighty-four percent of patients were given corticosteroids as an initial treatment. After 16 weeks of corticosteroids, 88% of these patients were cured. On average, time to reversing the disease was around 4 weeks. Six percent of patients at first were given a cancer drug called cyclophosphamide, either with or without corticosteroids. All these patients were cured, in about 4 weeks on average. Ten percent of patients were cured without drugs that suppress the immune system. The disease returned at least once in 54% of patients that initially were given corticosteroids. Cyclophosphamide used as a second-choice alternative drug cured 57% of the patients who had returning MCNS. Forty percent of the patients had serious kidney damage. Kidney function returned in almost every case. Nine percent of patients had blood clots. At the last follow-up, 90% of the patients still had the disease reversed and had functioning kidneys. Three patients with steroid-resistant MCNS continued to kidney failure. The kidney failure came along with damage to the small structures of the kidney that were seen under a microscope. This study is limited because we only looked at past patients and they were not all given the same treatments and dosages. The first conclusion we can make is that most patients with MCNS that appeared as an adult or late adolescent were cured with corticosteroids. However, many had the disease come back. In many of the patients for whom the disease came back, treatment with cyclophosphamide kept it from coming back again. Significant health problems were seen from serious damage to the kidneys and other problems caused by the disease. In a few patients, the disease continued to kidney failure, and this could be explained by damage to the small structures of the kidney.",2,B
66,32146646,"Background: Despite recent advances in immunosuppressive therapy for patients with primary nephrotic syndrome, its effectiveness and safety have not been fully studied in recent nationwide real-world clinical data in Japan. Methods: A 5-year cohort study, the Japan Nephrotic Syndrome Cohort Study, enrolled 374 patients with primary nephrotic syndrome in 55 hospitals in Japan, including 155, 148, 38, and 33 patients with minimal change disease (MCD), membranous nephropathy (MN), focal segmental glomerulosclerosis (FSGS), and other glomerulonephritides, respectively. The incidence rates of remission and relapse of proteinuria, 50% and 100% increases in serum creatinine, end-stage kidney disease (ESKD), all-cause mortality, and other major adverse outcomes were compared among glomerulonephritides using the Log-rank test. Incidence of hospitalization for infection, the most common cause of mortality, was compared using a multivariable-adjusted Cox proportional hazard model. Results: Immunosuppressive therapy was administered in 339 (90.6%) patients. The cumulative probabilities of complete remission within 3 years of the baseline visit was ≥ 0.75 in patients with MCD, MN, and FSGS (0.95, 0.77, and 0.79, respectively). Diabetes was the most common adverse events associated with immunosuppressive therapy (incidence rate, 71.0 per 1000 person-years). All-cause mortality (15.6 per 1000 person-years), mainly infection-related mortality (47.8%), was more common than ESKD (8.9 per 1000 person-years), especially in patients with MCD and MN. MCD was significantly associated with hospitalization for infection than MN. Conclusions: Patients with MCD and MN had a higher mortality, especially infection-related mortality, than ESKD. Nephrologists should pay more attention to infections in patients with primary nephrotic syndrome.","New discoveries in treatments that suppress the immune system have helped patients with Nephrotic Syndrome (NS), a group of symptoms relating to the kidneys. However, the safety and effectiveness of these treatments has not been fully studied for recent cases in Japan. A study 5 years ago followed 374 patients with NS in Japan, across 55 hospitals. It included 155 patients with Minimal Change Disease (MCD), 148 patients with membranous nephropathy (MN), 38 patients with Focal Segmental Glomerulosclerosis (FSGS), and 33 patients with other types of inflammation of the small filtering structures in the kidney. Among these diseases, we compared how often proteinuria (too much protein in the urine) was cured and came back. We also compared how often patients died, had kidney failure, or had other serious problems. The most common cause of death was being hospitalized for an infection. We compared this cause among the different diseases using statistical methods. We found that treatment to suppress the immune system was given to 339 patients. For patients with MCD, MN, FSGS, there was more than a 75% chance the disease was completely cured within 3 years. The most common side effect of treatments suppressing the immune system was diabetes. In each year, a person had a 71 in 1000 chance of this happening. Death from any cause, about half of which caused by infections, was more common than kidney failure, especially in patients with MCD and MN. MCD was more associated with hospitalization for infections than MN. Patients with MCD and MN were more likely to die, especially from infections, than to have kidney failure. We also conclude that doctors should pay more attention to infections in patients with NS.",2,B
66,34362836,"Background: Little population-based data exist about adults with primary nephrotic syndrome. Methods: To evaluate kidney, cardiovascular, and mortality outcomes in adults with primary nephrotic syndrome, we identified adults within an integrated health care delivery system (Kaiser Permanente Northern California) with nephrotic-range proteinuria or diagnosed nephrotic syndrome between 1996 and 2012. Nephrologists reviewed medical records for clinical presentation, laboratory findings, and biopsy results to confirm primary nephrotic syndrome and assigned etiology. We identified a 1:100 time-matched cohort of adults without diabetes, diagnosed nephrotic syndrome, or proteinuria as controls to compare rates of ESKD, cardiovascular outcomes, and death through 2014, using multivariable Cox regression. Results: We confirmed 907 patients with primary nephrotic syndrome (655 definite and 252 presumed patients with FSGS [40%], membranous nephropathy [40%], and minimal change disease [20%]). Mean age was 49 years; 43% were women. Adults with primary nephrotic syndrome had higher adjusted rates of ESKD (adjusted hazard ratio [aHR], 19.63; 95% confidence interval [95% CI], 12.76 to 30.20), acute coronary syndrome (aHR, 2.58; 95% CI, 1.89 to 3.52), heart failure (aHR, 3.01; 95% CI, 2.16 to 4.19), ischemic stroke (aHR, 1.80; 95% CI, 1.06 to 3.05), venous thromboembolism (aHR, 2.56; 95% CI, 1.35 to 4.85), and death (aHR, 1.34; 95% CI, 1.09 to 1.64) versus controls. Excess ESKD risk was significantly higher for FSGS and membranous nephropathy than for presumed minimal change disease. The three etiologies of primary nephrotic syndrome did not differ significantly in terms of cardiovascular outcomes and death. Conclusions: Adults with primary nephrotic syndrome experience higher adjusted rates of ESKD, cardiovascular outcomes, and death, with significant variation by underlying etiology in the risk for developing ESKD.","Nephrotic Syndrome (NS) is a combination of symptoms relating to the kidneys. There is not much information about how NS exists in the population. We looked at kidney health, heart disease, and death in adults within Kaiser Permanente of Northern California that had symptoms of NS between 1996 and 2012. Kidney specialists looked at medical records to confirm NS and determined causes. For comparison, we also found a group of adults with no diabetes, NS, or proteinuria (too much protein in the urine) through 2014. This group was to compare rates of kidney failure, heart disease, and death using statistical methods. We confirmed 907 patients with NS. Of these, 655 definitely had either Focal Segmental Glomerulosclerosis (FSGS), Membranous Nephropathy (MN), or Minimal Change Disease (MCD). Another 252 were presumed to have one of these. The average age of the patients was 49, and 43% were women. Adults with NS had about 20 times the rate of kidney failure compared to people with no NS. They also had about two and a half times the rate of heart-related chest pain, three times the rate of heart failure, almost twice the rate of stroke, two and a half times the rate of blood clots, and 34% higher rate of death. The extra risk for kidney failure was much higher for FSGS and MN than for disease presumed to be MCD. The three causes of NS did not have significant differences in heart disease and death. In conclusion, adults with NS have higher rates of kidney failure, heart disease, and death. The risk of kidney failure depended significantly on which underlying disease caused the NS symptoms.",2,B
66,35102405,"Nephrotic syndrome (NS) encompasses a variety of disease processes leading to heavy proteinuria and edema. Minimal change disease (MCD) remains the most common primary cause of NS, as well as the most responsive to pharmacologic treatment with often minimal to no chronic kidney disease. Other causes of NS include focal segmental glomerulosclerosis, which follows MCD, and secondary causes, including extrarenal or systemic diseases, infections, and drugs. Although initial diagnosis relies on clinical findings as well as urine and blood chemistries, renal biopsy and genetic testing are important diagnostic tools, especially when considering non-MCD NS. Moreover, biomarkers in urine and serum have become important areas for research in this disease. NS progression and prognosis are variable and depend on etiology, with corticosteroids being the mainstay of treatment. Other alternative therapies found to be successful in inducing and maintaining remission include calcineurin inhibitors and rituximab. Disease course can range from recurrent disease relapse with or without acute kidney injury to end-stage renal disease in some cases. Given the complex pathogenesis of NS, which remains incompletely understood, complications are numerous and diverse and include infections, electrolyte abnormalities, acute kidney injury, and thrombosis. Pediatricians must be aware of the presentation, complications, and overall long-term implications of NS and its treatment.","Nephrotic syndrome (NS) includes a variety of underlying diseases that lead to serious proteinuria (too much protein in the blood) and edema (fluid buildup). Minimal change disease (MCD) is still the most common cause of NS, and is the most responsive to treatment with drugs, often with little or no chronic kidney disease. Other causes of NS include focal segmental glomerulosclerosis, which follows MCD, and secondary causes, including diseases outside the kidneys, infections, and drugs. The initial diagnosis relies on signs and symptoms observed by a doctor. However, urine and blood tests, kidney tissue samples, and genetic testing are also important for diagnosis. This is especially true for NS not caused by MCD. Additionally, signs of disease in blood and urine have become important areas for research in this disease. The outcomes of NS and the way it progresses can vary and depend on the underlying disease. Corticosteroids are the main treatment. Alternative treatments that have been found to work include calcineurin inhibitors (a class of drugs that suppress the immune system) and rituximab (Rituxan). The course of the disease can range from continuing to come back, with or without serious kidney damage, to kidney failure in some cases. Since the origin of NS is complicated and not completely understood, there are many associated problems. These include infections, electrolyte problems, serious kidney damage, and blood clots. Pediatricians should know about how NS appears and the associated problems and long-term issues of NS and its treatment.",2,B
66,35017338,"Children with nephrotic syndrome (NS) have a number of potential risk factors for the development of acute kidney injury (AKI) including intravascular volume depletion, infection, exposure to nephrotoxic medication, and renal interstitial edema. This study was aimed to determine the incidence of AKI in children hospitalized with a relapse of NS and its short-term outcome. This prospective observational study was conducted from February 2017 to January 2018 at a tertiary care teaching hospital. A total of 54 children and adolescents (1-18 years) hospitalized with a diagnosis of NS and relapse with/or without other complications were enrolled. Clinical data and examination were recorded. AKI was defined using the Kidney Disease Improving Global Outcomes (KDIGO) serum creatinine criteria and Pediatric Risk, Injury, Failure, Loss, End-Stage Renal Disease (p-RIFLE) classification. Children who developed AKI during the first two weeks of hospitalization were followed up till recovery or six weeks whichever was earlier to determine the outcome and factors predisposing to AKI. The mean age of the study population was 59.5 months and 35 (64.8%) patients were male. Of the 54 patients hospitalized, 42 (77.8%) were admitted with infection-associated relapses while 22.2% of children had relapse alone. Diarrhea and spontaneous bacterial peritonitis were the most common infections (26.1% each) followed by urinary tract infections in 19% and pneumonia in 14.3%. Twenty-three (42.6%) children developed AKI according to the KDIGO definition and 27 (50%) using the pRIFLE classification. Fourteen (60.9%) had stage 2 AKI while 21.7% had stage 3 AKI. Infections [odds ratio (OR) 1.24] and use of angiotensin-converting enzyme inhibitors (ACEI) (OR 2.3) were the most common predisposing factors for AKI. The mean recovery time for AKI was 7.34 days. Development of AKI was associated with prolonged hospital stay (12.57 vs.8.55 days P <0.01) and delayed recovery. At the end of follow-up all children recovered from AKI. The incidence of AKI in children hospitalized with complications of NS is high. While the occurrence of these AKI episodes may appear transient, a recurrence of such episodes may be detrimental to the long-term outcome of children with NS. Infections and the use of ACEI during relapses are risk factor for the occurrence of AKI.","Nephrotic Syndrome (NS) is a combination of kidney-related symptoms. Children with NS are at risk for serious kidney damage for a number of reasons. These include too little blood, infection, medications, and swelling in the kidneys. The aim of this study was to find out how often serious kidney damage happens in children that had NS come back, and the short-term outcome of that damage. This study followed people over time and was conducted from February 2017 to January 2018 at a teaching hospital with highly specialized care. We enrolled a total of 54 children and adolescents who were hospitalized with a diagnosis of NS that came back, with or without associated problems. We recorded health-related information. To figure out what puts children more at risk for serious kidney damage, we followed up with children who had this problem during the first two weeks of hospitalization. We continued following up with them either until they recovered or until six weeks, whichever was earlier. The average age of people in the study was 59.5 months. Thirty-five of the patients (65%) were male. Out of the 54 patients that were hospitalized, 42 had returning NS that was associated with infections. Twenty-two percent of children had returning NS only. The most common infections were diarrhea and bacterial infections of the peritoneum, the lining of the abdominal cavity. The next most common were urinary tract infections and pneumonia. Twenty-three or 27 children developed serious kidney damage, depending on the criteria used. The most common factors that increased risk of serious kidney damage were infections and the use of ACE inhibitors. The average time for recovery from the serious kidney damage was about 7 days. Serious kidney damage was associated with longer hospital stays and delayed recovery. At the end of the followup, all children recovered from the kidney damage. Children hospitalized with problems associated with NS have high rates of serious kidney damage. These instances of serious kidney damage may seem to come and go, but happening repeatedly may cause long-term problems for children with NS. Infections and the use of ACE inhibitors increases the risk of serious kidney damage.",2,B
66,34979093,"Background: Although venous thromboembolism is a well-known complication of nephrotic syndrome, the long-term absolute and relative risks of arterial thromboembolism, venous thromboembolism, and bleeding in adults with nephrotic syndrome remain unclarified. Methods: In this matched cohort study, we identified every adult with first-time recorded nephrotic syndrome from admissions, outpatient clinics, or emergency department visits in Denmark during 1995-2018. Each patient was matched by age and sex with 10 individuals from the general population. We estimated the 10-year cumulative risks of recorded arterial thromboembolism, venous thromboembolism, and bleeding accounting for the competing risk of death. Using Cox models, we computed crude and adjusted hazard ratios (HRs) of the outcomes in patients with nephrotic syndrome versus comparators. Results: Among 3967 adults with first-time nephrotic syndrome, the 1-year risk of arterial thromboembolism was 4.2% (95% confidence interval [CI] 3.6-4.8), of venous thromboembolism was 2.8% (95% CI 2.3-3.3), and of bleeding was 5.2% (95% CI 4.5-5.9). The 10-year risk of arterial thromboembolism was 14.0% (95% CI 12.8-15.2), of venous thromboembolism 7.7% (95% CI 6.8-8.6), and of bleeding 17.0% (95% CI 15.7-18.3), with highest risks of ischemic stroke (8.1%), myocardial infarction (6.0%), and gastrointestinal bleeding (8.2%). During the first year, patients with nephrotic syndrome had increased rates of both arterial thromboembolism (adjusted HR [HRadj] = 3.11 [95% CI 2.60-3.73]), venous thromboembolism (HRadj = 7.11 [5.49-9.19]), and bleeding (HRadj = 4.02 [3.40-4.75]) compared with the general population comparators after adjusting for confounders. Conclusion: Adults with nephrotic syndrome have a high risk of arterial thromboembolism, venous thromboembolism, and bleeding compared with the general population. The mechanisms and consequences of this needs to be clarified.","Blood clots are a well-known problem associated with Nephrotic Syndrome (NS), a group of kidney-related symptoms. However, the risks of blood clots and bleeding in adults with NS is not clear. This study looked at groups of similar people with and without disease. In it, we found every adult that had NS recorded for the first time from 1995 to 2018 at admissions, outpatient clinics, and emergency department visits in Denmark. Each patient was matched by age and sex with 10 individuals from the general population. We estimated the 10-year risk of recorded blood clots and bleeding, accounting for risk of death. Using a statistical model, we calculated how likely the patients were to have certain outcomes compared to the general population. Among 3,967 adults with first-time NS, the risk of having a blood clot in an artery within one year was 4%. The risk of a blood clot in a vein in the same time was 3%, and the risk of bleeding was 5%. Within 10 years, the risk of a blood clot in an artery was 14%, the risk of a blood clot in a vein was 8%, and the risk of bleeding was 17%. The highest bleeding risks were of stroke, heart attack, and gastrointestinal bleeding. During the first year, patients with NS had higher rates of blood clots in arteries, blood clots in veins, and bleeding than the people from the general population. In conclusion, adults with NS have a higher risk of blood clots, both in arteries and veins, and bleeding compared to the general population. We still need to figure out how this happens and what the consequences are.",2,B
66,34839817,"Background: Steroid resistant nephrotic syndrome (SRNS), while uncommon in children, is associated with significant morbidity. Calcineurin inhibitors (CNIs) remain the first line recommended therapy for children with non-genetic forms of SRNS, but some children fail to respond to them. Intravenous (IV) cyclophosphamide (CTX) has been shown to be effective in Asian-Indian children with difficult to treat SRNS (SRNS-DTT). Our study evaluated the outcome of IV CTX treatment in North American children with SRNS-DTT. Methods: Retrospective review of the medical records of children with SRNS-DTT treated with IV CTX from January 2000 to July 2019 at our center. Data abstracted included demographics, histopathology on renal biopsy, prior and concomitant use of other immunosuppressive agents and serial clinical/laboratory data. Primary outcome measure was attainment of complete remission (CR). Results: Eight children with SRNS-DTT received monthly doses (median 6; range 4-6) of IV CTX. Four (50%) went into CR, 1 achieved partial remission and 3 did not respond. Three of the 4 responders had minimal change disease (MCD). Excluding the 1 child who responded after the 4th infusion, the median time to CR was 6.5 (range 0.5-8) months after completion of IV CTX infusions. Three remain in CR at a median of 8.5 years (range: 3.7-10.5 years) after completion of CTX; one child relapsed and became steroid-dependent. No infections or life-threatening complications related to IV CTX were observed. Conclusions: IV CXT can induce long term remission in North-American children with MCD who have SRNS-DTT.","Steroid Resistant Nephrotic Syndrome (SRNS) is a group of kidney-related symptoms that cannot be treated with steroids. Though it is not common in children, it is associated with many serious health problems. Immune system suppressing drugs called Calcineurin Inhibitors (CNIs) are the preferred treatment for children with SRNS that is not inherited. However, some children do not respond to these drugs. Another drug called Cyclophosphamide (CTX), given by IV, has been shown to be effective in Asian-Indian children with SRNS that is difficult to treat (SRNS-DTT). Our study looked at the outcome of IV CTX treatment in North American children with this disease. The study was a review of previous medical records of children with SRNS-DTT treated with IV CTX from January 2000 to July 2019 at our center. From these records, we looked at demographics, kidney tissue samples, and the use of drugs to suppress the immune system. We also looked at notes from office visits and lab results recorded over time. The main outcome that we measured was complete reversal of the disease. We found that 8 children with SRNS-DTT were given monthly doses of IV CTX. Four were completely cured, one was partially cured, and three did not respond to the treatment. Three of the four patients that were cured had a particular type of SRNS that was caused by Minimal Change Disease (MCD). Except for the one child who responded to treatment after the 4th dose, the average time to reverse the disease was about six and a half months after completing all the IV CTX doses. Three of the patients are still cured at an average of 8 and a half years after completing the CTX treatment. One child had the disease come back and had to continue taking steroids indefinitely. No infections or other life-threatening problems relating to IV CTX were seen. In conclusion, in North American children, Cyclophosphamide (CTX) given by IV can completely cure SRNS that is difficult to treat and is caused by Minimal Change Disease (MCD).",2,B
66,34775776,"Chylothorax is an uncommon and serious clinical condition, typically induced by trauma, either postsurgical or accidental injury, but the mechanism of chylothorax caused by nephrotic syndrome is still unclear. Here, we report a case of primary nephrotic syndrome with membranous nephropathy (MN) in a 66-year-old man who presented with severe chylothorax. The chylothorax was managed by intercostal chest tube drainage, subcutaneous injection of enoxaparin, and treatment with anti-inflammatory agents and diuretics. After treatment, the patient's pleural effusion decreased, and the chyle gradually became clear. We discuss the causes of MN with chylothorax. We considered that the hypoproteinemia changed the permeability of mucous membranes and lymphatic vessels, leading to leakage of chylous particles and chylous pleural effusion formation. Chylothorax may also have been caused by severe tissue edema, edema of the lymphatic walls, and increased pressure, resulting in increased permeability or rupture of the lymphatic wall, and leakage of chylous fluid into the thoracic cavity. Because of its rarity, we hope this case report will improve clinicians' understanding of MN complications in primary nephrotic syndrome and provide suitable treatment options for future clinical reference.","Chylothorax is an uncommon and serious medical condition where fluid from the lymphatic system builds up around the lungs. It is typically caused by serious damage to the body, either after surgery or an accident. However, we don’t know how a cluster of kidney-related symptoms called Nephrotic Syndrome (NS) causes chylothorax. In this article we describe a case of NS with a kidney disease called Membranous Nephropathy (MN) in a 66 year old man with severe chylothorax. The chylothorax was kept under control with a drainage tube inserted into the chest, injections of a drug called enoxaparin, and treatment with anti-inflammatory drugs and diuretics, which increase urine production. After treatment, the fluid around the lungs decreased and became clear. We will talk about the causes of MN with chylothorax. We thought that maybe too little protein in the blood allowed fluid from the lymphatic system to leak through tissues and cause the buildup around the lungs. The chylothorax may also have been caused by severe swelling and increased pressure. This could have caused fluid to seep through or rupture the walls of lymph vessels (which are similar to blood vessels) and leak into the chest cavity. Since MN is rare, we hope this case report will help doctors to understand problems associated with MN in patients that have NS, and that it will provide treatment options for doctors in the future.",2,B
